\documentclass{article}

% === Dependencies === %

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage{listings,listings-rust}
\usepackage{xcolor}
\usepackage[ruled, lined, linesnumbered, commentsnumbered, longend]{algorithm2e}
\usepackage{forest}

%\usepackage{algorithm}% http://ctan.org/pkg/algorithms
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
%\MakeRobust{\Call}

\usepackage[backend=biber]{biblatex} % Imports biblatex package
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{parskip} % Make space between subsubsections instead of indent
\usepackage[style=iso]{datetime2} % ISO style dates

\addbibresource{references.bib} % Import the bibliography file

% === Paragraphing === %

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
% display heading, like subsubsection
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\makeatother

% === Theorem Styles === %

\newtheorem{definition}{Definition}[section]

% === Custom Commands === %

\newcommand{\eq}[1]{\begin{alignat*}{20}#1\end{alignat*}}
\newcommand{\eqn}[2]{\begin{equation}\label{#1}\begin{split}#2\end{split}\end{equation}}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\ran}[1]{\mathrm{#1}}
\newcommand{\vecran}[1]{\mathbf{#1}}

\renewcommand{\O}{\mathcal{O}}
\newcommand{\V}{\mathcal{V}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\D}{\mathcal{D}}

% Sets
\newcommand{\F}{\mathbb{F}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand\concat{\mathbin{+\mkern-10mu+}} % concat-symbol
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\opn}[1]{\operatorname{#1}}
\newcommand{\veclo}[1]{\vec{#1_{\opn{lo}}}}
\newcommand{\vechi}[1]{\vec{#1_{\opn{hi}}}}
\newcommand{\vecloh}[2]{\vec{#1^{\textit{#2}}_{\opn{lo}}}}
\newcommand{\vechih}[2]{\vec{#1^{\textit{#2}}_{\opn{hi}}}}
\newcommand{\vecl}[1]{\vec{#1_{\opn{L}}}}
\newcommand{\vecr}[1]{\vec{#1_{\opn{R}}}}
\newcommand{\blind}[1]{\widetilde{#1}}
\newcommand{\vecblind}[1]{\vec{\blind{#1}}}

% Blinded points
\newcommand{\tB}{\widetilde{B}}
\renewcommand{\tt}{\widetilde{t}}
\newcommand{\tv}{\widetilde{v}}
\newcommand{\ba}{\widetilde{a}}
\newcommand{\bB}{\widetilde{B}}


\newcommand{\len}[1]{\Call{len}{#1}}

% Code style

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{rust}{
	language=Rust,
	backgroundcolor=\color{backcolour},
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2
}

\lstset{style=rust}

% === Header === %

\title{Property based testing of Rust Bulletproofs using Hacspec}
\author{ 
Rasmus Kirk Jakobsen -- 201907084\\
Anders Wibrand Larsen -- 201906147\\
\textbf{Advisor:} Bas Spitters
}

\date{\today}

\begin{document}

\maketitle

\begin{center}
    Bachelor report (15 ECTS) in Computer Science\\
Department of Computer Science, Aarhus University\\
\end{center} 

\begin{center}
	\includegraphics[scale=0.4]{img/bulletproof-hacspec-2.png}
\end{center} 

\subsection*{Abstract:}

\tableofcontents

\newpage

% === Body === %

\section{Introduction:} \label{Introduction}

Transmission of data over the Internet can be incredibly risky, especially where transmitting money is concerned. This is the problem that cryptographic protocols seek to eliminate by ensuring that this data is transmitted with as little information as possible. The least possible information to transmit is a so-called Zero-knowledge proof where \textit{no} information can be infered from the outside, and only one piece of information can be discovered by the receiver. One of the newest cryptographic protocols is the Bulletproof Protocol, created by BÃ¼nz et al. (\cite{bulletproofs}). This protocol creates a short and secure way to show that a series of values lie within a range of $[0:2^n)$ for a provided n. Showing this will allow honest provers to encrypt information about the amount of money on their account when making online purchases, among other things, in a secure and compact way. 

Bulletproofs were implemented in Rust about three years ago. However this implementation may have certain vulnerabilities, which could allow for exploitation by malicious users. This may lead to inflation bugs (\cite{cryptonote}, \cite{z-cash}) or the Frozen Heart vulnerability. (\cite{frozen-heart}) One way to minimize the odds of such vulnerabilities is using a Proof Assistant, such as Coq. The greatest issue with using such a Proof Assistant is that it is currently impossible to make an implementation of a cryptographic protocol which can both use a Proof Assistant and also execute effectively. 

For this purpose a specification language, called Hacspec, was created. \cite{hacspec}Hacspec is a subset of the programming language Rust which sets certain restrictions on the code to allow code written in it, to compile into a Proof Assistant. %Rasmus kan nok skrive det her om meget kort

The overall goal of this paper is to implement the Bulletrpoof protocol in Hacspec, while ensuring it functions identically to the existing Rust implementation, thus allowing the use of a Proof Assistant on the existing indirectly by using our Hacspec implementation. Additionally we also wish to write our implementation in a way that allows future devealopers to implement other cryptographic protocols using various modules from our implementation.

In this paper we will briefly go over the prerequisites for understanding the bulletproofs protocol, the protocol itself, as well as relevant material for understanding the implementation and finally everything we implemented. We finalize this paper by discussing possible future work that can be done on the back of our work, concluding the overall project.


%Background: Hacspec.

%Problem: Bulletproofs implementation is not proven to function as the protocol says it should. If it does not there might be double-spend vulnerabilities. 

%Goals: Implement Bulletproofs in Hacspec, in such a way that allows for other Hacspec protocols to be implemented using our modules.

%Map out the paper.


\textbf{Problem:}

Bla bla vulnerabilities.

Sources of vulnerabilities could for example be inflation bugs or Frozen Heart.

One way to minimize the odds of vulnerabilities is using proof assistance. 

However, you cannot implement protocols that can be used by proof assistance programs and execute efficiently.




\section{Prerequisites:}

In this section we will be going over the necessary background knowledge
to understand the results of the work in this project.

\subsection{Finite Field Arithmetic:} \label{Finite Field Arithmetic}

We start with \textit{fields} and the operations defined within
them, as the operations we will later define that work on elliptic
curves are built on fields. The definitions presented here are largely
from \cite{elliptic-curves}.

\begin{definition}[Field]
	A field is a set $\F$, along with the \textit{addition} and
	\textit{multiplication} operations. These two operations must
	uphold the so called \textit{field axioms}:

	\begin{itemize}
		\item Associativity of addition and multiplication:
		$\forall a,b,c \in \F : a + (b + c) = (a + b) + c \land a \cdot (b \cdot c) = (a \cdot b) \cdot c$
		\item Commutativity of addition and multiplication:
		$\forall a,b \in \F : a+b=b+a \land a \cdot b = b \cdot a$
		\item Additive and multiplicative identity:
		$\exists 0,1 \in \F : a + 0 = a \land a \cdot 1 = a$
		\item Additive inverses:
		$\forall a \in \F,\ \exists {-a} \in \F : a + ({-a}) = 0$
		\item Multiplicative inverses:
		$\forall a \neq 0 \in \F,\  \exists a^{-1} \in \F : a \cdot a^{-1} = 1$
		\item Distributivity over addition:
		$\forall a,b,c \in \F : a \cdot (b + c) = (a \cdot b) + (a \cdot c)$
	\end{itemize}
\end{definition}

Note that this also means that subtraction and division is defined,
due to the existance of the additive and multiplicative inverses
respectively:

\eq{
	a-b         &= a + (-b) \\
	\frac{a}{b} &= a \cdot b^{-1}
}

Corrolarily, this leads us to \textit{finite fields}:

\begin{definition}[Finite Field]
	A finite field, is a field that contains a finite number of elements.
\end{definition}

One of the most commonly used types of finite fields, and those used
throughout this report are \textit{Prime Fields:} 

\begin{definition}[Prime Field]
	A prime field $\F_p$ is a finite field with elements $[0,p-1]$
	where each operation is performed over integers modulo $p$
	with $p$ being a prime number.
\end{definition}

Something important to note about this definition is that inverses in
this kind of field are also positive whole numbers. This will come up
again in section \ref{implementing-ristretto} with regards to the code.

\subsection{Elliptic Curve:}\label{elliptic-curves}

To start our explanation of elliptic curves we add a few
definitions. Again, the definitions here are mostly from
\cite{elliptic-curves}:

\begin{definition}[Elliptic Curves]
	An elliptic curve, $E$, is an algebraic curve defined over a
	prime field, $\F_p$, defined by the formula:
	$$y^2 = x^3 + ax + b$$
\end{definition}

\begin{definition}[EC Additive Identity]
	Let $\O$ be the point where $y = \infty$. For every point, $P$,
	on $E$ the following property holds:
	$P + \O = \O + P = P$.
\end{definition}

\begin{definition}[EC Negation]
	Let $P = (x,y)$ be a point on $E$. The negation of $P$, called $-P$
	is defined as the mirror of P over the x-axis, or $-P = (x,-y)$.
\end{definition}

\begin{definition}[EC Addition]
	Let $\ell$ be a line intersecting $E$ at three points, $P$, $Q$ and
	$-R$. The group operation: Addition defined on two points is defined
	as:
	$$P + Q = {-R}$$
\end{definition}

\begin{definition}[EC Doubling]
	let $\ell$ be the tangent to the point $P$ on $E$, which intersects
	points $P$ and $-R$. The group operation: point doubling on P is
	defined as:
	$$2P = P + P = R$$
\end{definition}

An important thing to note about point addition. In the case where
you add points $P$ and $Q$ where $\ell$ is a tangent to $P$ then $P +
Q = -P$. This is due to the fact that the \textit{only} case in which
this can happen is when $Q = -(P + P)$ and thus you get $P + Q = P +
(-(P + P)) = -P$. Another similar case is using point doubling on any
point $P = (x,0)$ on $E$. The tangent of $P$ do not intersect
a second point. However note that the negation of $P$ here is $-P =
(x,-0) = (x,0) = P$. Thus point doubling on these points is equivalent
to the equation $P + (-P) = \O$.

From these definitions we can extrapolate two more definitions:

\begin{definition}[EC Subtraction]
	Let $P$ and $Q$ be elliptic curve points. The group operation:
	Subtraction is defined by:
	$$P-Q = P + (-Q)$$
\end{definition}

\begin{definition}[EC Scalar Multiplication]
	Let $m$ be a scalar and let $P$ be a point on $E$. The group operation:
	Scalar multiplication is defined by adding $P$ to itself $m$ times and
	denoted a $m\cdot P$.
\end{definition}

Due to the absence of scalar division it is impossible to multiply by
anything other than integers as we cannot have something akin to $2.5
\cdot P = \frac{5\cdot P}{2}$. Therefore scalars are always integers
when doing computations for elliptic curves. Multiplying by $0$ will
naturally yield the identity element $\mathcal{O}$ by definition.
Additionally multiplying by a negative integer, ${-m}$, is defined as
$-m\cdot P = m\cdot ({-P})$.

With these definitions give rise to the concept of an elliptic
curve's \textit{generator}.

\begin{definition}[EC Generator]
	A Generator for any given Elliptic curve is a point, for
	which all possible points can be reached using addition or
	scalar multiplication of this point. The set of all possible
	generators for a curve is denoted $\G$.
\end{definition}

\subsection{Pedersen Commitment:}

A Pedersen commitment is a commitment scheme originally defined
by Torben Pryds Pedersen \cite{pedersen}. The essence of
Pedersen commitments is the so called \textit{Additive Homomorphism}
\ref{pedersen-additive-homomorphism} defined later. For our purposes we
redefine the scheme to work with commitments to elliptic curve points
rather than a value. First a single point $B$ on an elliptic curve,
is chosen. This point will be the generator.

\begin{definition}[Pedersen Commitment]
	A Pedersen commitment, to some message, $a$, with a randomly chosen
	integer, $\ba$, is defined as:

	$$aB + \ba\blind{B}$$

	where $B$ is a canonical generator for the curve and $\tB$
	is a curve point where no one knows $q$ such that $\tB = qB$.
\end{definition}

Here $\widetilde{a}$ serves as our perfect hiding. The construction
of $\tB$ ensures perfect hiding as there is a nearly infinite amount
of possible combinations of two points that can add to any given
point. However while that is true, it does not provide perfect binding
by definition of binding and hiding. From here on we will refer to
a commitment to a message, $a$, with random value, $\widetilde{a}$,
as $C(a,\widetilde{a})$.

The most important property of Pedersen commitments however is
that pedersen commitments have the property of Additive Homomorphism:

\begin{definition}[Additive Homomorphism] \label{pedersen-additive-homomorphism}
	For any two Pedersen commitments $C(a_1,\ba_1), C(a_2,\ba_2)$ we have:
	\eq{
		C(a_1,\ba_1) + C(a_2,\ba_2) = C(a_1 + a_2, \ba_1 + \ba_2)
	}
\end{definition}

This is simple to show by applying the definition of Pedersen
commitments: 

\eq{
	C(a_1,\ba_1) + C(a_2,\ba_2) &= \ba_1\tB + a_1B  + \ba_2\tB + a_2B \\
	                            &= (\ba_1 + \ba_2)\tB + (a_1 + a_2)B \\
	                            &= C(a_1 + a_2, \ba_1 + \ba_2)
}

Which ensures a homomorphism between the sum of commitments and the
commitment to the sum.

\subsection{Zero Knowledge Proofs:}\label{zero-knowledge}

A zero-knowledge proof is a cryptographical concept in which a prover,
$\P$, will convince a verifier, $\V$, that a certain property holds,
usually in the form of a mathematical equation, and $\P$ will do so
without $\V$ gaining any additional information. To exemplify this we
will show the simplest example of a Zero-knowledge proof, using the
so-called Schnorr Identity Protocol.

This crux of the Schnorr Identity Protocol is for $\P$ to show that
they know the secret value $x$. This secret value has the following
relation to the public elliptic curve points $X$ and $G$:

\eq{
	X = xG
}

$\V$ knows both $X$ and $G$ but not $x$. $\P$ wishes to show $\V$
without simply revealing $x$ directly.

The first step $\P$ takes is choosing a random value $k$ and commiting
to this value by sending $K$, such that $K = kG$, to $\V$. $\V$ then
chooses a random value $e$ and sends it to $\P$. Finally $\P$ sends $s$
such that $s = k + ex$. Written out as a transcript this would be:

\eq{
	\P \rightarrow \V &: K \\
	\V \rightarrow \P &: e \\
	\P \rightarrow \V &: s \\
}

From here $\V$ can do a simple check to see if $\P$ truly knows $x$:

\eq{
	sG \stackrel{?}{=} K + eX
}


\textbf{Completeness:}

For a proof to be 'complete' it means that an honest $\P$ will convince
an honest $\V$ that the statement they wish to prove holds. This proof
is complete because of the following:

\eq{
	sG &= K + eX \\
	   &= kG + e(xG) \\
	   &= (k + ex)G \\
	   &= sG
}

\textbf{Soundness:}

For a proof to be sound it means that a \textit{dishonest} $\P$
cannot convince an honest $\V$ that their statement is true. In this
case it means that if $\P$ does not know $x$ but wishes to convince
$\V$ that it does, this is not possible. For this to be the case $\P$
must create a $K$ and $s$ such that $sG = K + eX$ without knowing $x$,
which is used in the creation of $s$. This \textit{could} be done if
$\P$ had knowledge of $e$ by constructing $K$ as follows:

\eq{
	K = s(G - eX)
}

and then choose a random value $s$. If the verifier then did the check
they would find that it holds. However this is NOT a possibility
because $\P$ must commit to a $K$ before they are given $e$. There
is a small probability that $\P$ could randomly guess $e$ correctly,
but this is miniscule.

\textbf{Zero-knowledge:}

For a proof to be zero-knowledge it must hold that $\V$ did not gain
any additional information. To see this we look at the transcript
and see that $K$ does not include $x$ in its calculations, but $s$
does. However $s$ is blinded by $k$ meaning that $\V$ cannot infer $x$
without knowing $k$. And $\V$ cannot infer $k$ from $K$ any easier
than it can infer $x$ from $X$, thus this proof is also zero-knowledge.

\section{The Bulletproofs protocol} \label{Bulletproofs}

The ultimate goal of this project has been to implement bulletproofs
in Hacspec, using property-based testing to ensure it is equivalent to
the implementation done in Rust, using Curve25519. A bulletproof is a
non-interactive aggregation of rangeproofs using Pedersen commitments.
The following section will describe each of these terms in greater
detail on a theoretic level. The following subsections have been
created based on the original bulletproofs paper \cite{bulletproofs}
as well as the implementation notes from the Dalek-Bulletproofs project
\cite{dalek}.

\subsection{Inner Product Proof:}
A prover, $\P$, wants to prove to a verifier, $\V$, that he has
knowledge of two vectors $\vec{a}$, $\vec{b}$ and know their inner
product $\dotp{\vec{a}}{\vec{b}} = c$. The prover could simply send
the verifier $\vec{a}, \vec{b}, c$ but this would take up $O(n)$
bandwidth and time to verify. Instead we will use a compression
technique included in the bulletproofs paper which allows for just
$O(\log_2(n))$ bandwidth and time to verify.

\subsubsection{Provers Algorithm:}
$\P$ is given values with the following definitions:

\eqn{ipp-def1}{
	\vec{a}, \vec{b} &\in \Z^n_p \\
	\vec{G}, \vec{H} &\in \G^n \\
}
\eqn{ipp-def2}{
	P &= \dotp{\vec{a}}{\vec{G}} + \dotp{\vec{b}}{\vec{H}} \\
	c &= \dotp{\vec{a}}{\vec{b}} \\
}

Here $P$ is similar to a Pedersen Commitment with the crucial difference
that \textit{there is no blinding}. The blinding will be introduced
later on in the range proofs section. (\ref{range-proofs}) We will
introduce a combined form of $P$ and $c$ in the form of $P^{(k)}$,
The parenthesised superscript, denotes that it's a new variable,
it does \textit{not} refer to exponentiation:

\eq{
	P^{(k)} &= \dotp{\vec{a}}{\vec{G}} +
	           \dotp{\vec{b}}{\vec{H}} +
	           \dotp{\vec{a}}{\vec{b}}Q \\
} 

This redefinition will be useful, because it will allow us redefine
$\vec{a}, \vec{b}, \vec{G}, \vec{H}$ ($\vec{a}^{(k)}, \vec{b}^{(k)},
\vec{G}^{(k)}, \vec{H}^{(k)}$) into a shorter format, while keeping the
properties in equation \ref{ipp-def2} as invariants.

\eq{
	\vec{a}^{(k-1)} &= \veclo{a^{\text{(k)}}} &&\cdot \ran{u}_k      &&+ \ran{u}^{-1}_k &&\cdot \vechi{a^{\text{(k)}}} \\
	\vec{b}^{(k-1)} &= \veclo{b^{\text{(k)}}} &&\cdot \ran{u}^{-1}_k &&+ \ran{u}_k      &&\cdot \vechi{b^{\text{(k)}}} \\
	\vec{G}^{(k-1)} &= \veclo{G^{\text{(k)}}} &&\cdot \ran{u}^{-1}_k &&+ \ran{u}_k      &&\cdot \vechi{G^{\text{(k)}}} \\
	\vec{H}^{(k-1)} &= \veclo{H^{\text{(k)}}} &&\cdot \ran{u}_k      &&+ \ran{u}^{-1}_k &&\cdot \vechi{H^{\text{(k)}}} \\
}

Notice, that these redefined vectors will have a length half of that
of the original vectors, satisfying our requirement for more compact
vectors. The random variable, $\ran{u_k}$, will be provided by $\V$
to ensure that it is indeed random. From these new vectors we will
define a new compressed $P^{(k-1)}$ and isolate the variable $\ran{u}$:

\eq{
	P^{(k-1)} =
	\dotp{\vec{a}^{(k-1)}}{\vec{G}^{(k-1)}} +
	\dotp{\vec{b}^{(k-1)}}{\vec{H}^{(k-1)}} +
	\dotp{\vec{a}^{(k-1)}}{\vec{b}^{(k-1)}} \cdot Q
}

Notice that $P^{(k-1)}$ is still defined the same way
that we defined $P^{(k)}$, and that the property $c^{(k-1)} =
\dotp{\vec{a}^{(k-1)}}{\vec{b}^{(k-1)}}$ still holds. If we substitute
our defined variables:

\eq{
	P^{(k-1)} = \: \:
	&\dotp
		{        \veclo{a} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{a}}
		{&&\quad \veclo{G} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{G}}
	+ \\
	&\dotp
		{        \veclo{b} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{b}}
		{&&\quad \veclo{H} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{H}}
	+ \\
	&\dotp
		{        \veclo{a} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{a}}
		{&&\quad \veclo{b} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{b}}
	\cdot Q \\
}

And group the terms:

\eq{
	P^{(k-1)} = \: \:
	        &\dotp{\veclo{a}}{\veclo{G}}            +
	         \dotp{\vechi{a}}{\vechi{G}}          &&+
	u^2_k    \dotp{\veclo{a}}{\vechi{G}}          &&+
	u^{-2}_k \dotp{\veclo{a}}{\vechi{G}}            +\\
	        &\dotp{\veclo{b}}{\veclo{H}}            +
	         \dotp{\vechi{b}}{\vechi{H}}          &&+
	u^2_k    \dotp{\veclo{b}}{\vechi{H}}          &&+
	u^{-2}_k \dotp{\veclo{b}}{\vechi{H}}            +\\
	       &(\dotp{\veclo{a}}{\veclo{b}}            +
		       \dotp{\vechi{a}}{\vechi{b}}) \cdot Q &&+
	(u^2_k   \dotp{\veclo{a}}{\veclo{b}}          &&+
	u^{-2}_k \dotp{\vechi{a}}{\vechi{b}}) \cdot Q
}

We can see that this compressed $P^{(k-1)}$ contains $c =
(\dotp{\veclo{a}}{\veclo{b}} + \dotp{\vechi{a}}{\vechi{b}})$. We can
simplify further by defining variables $L_k$ and $R_k$:

\eq{
	&P^{(k-1)} &&= P_k + u^2_k \cdot L_k + u^{-2}_k \cdot R_k \\
	&\textbf{where:} \\
	&L_k     &&= \dotp{\veclo{a^{\text{(k)}}}}{\vechi{G^{\text{(k)}}}} +
	             \dotp{\vechi{b^{\text{(k)}}}}{\veclo{H^{\text{(k)}}}} + 
	             \dotp{\veclo{a^{\text{(k)}}}}{\vechi{b^{\text{(k)}}}} \cdot Q \\
	&R_k     &&= \dotp{\vechi{a^{\text{(k)}}}}{\veclo{G^{\text{(k)}}}} +
	             \dotp{\veclo{b^{\text{(k)}}}}{\vechi{H^{\text{(k)}}}} +
	             \dotp{\vechi{a^{\text{(k)}}}}{\veclo{b^{\text{(k)}}}} \cdot Q \\
}

So, $\P$ calculates $L_k, R_k$ and sends it to $\V$. Afterwards,
$\V$ responds with $u_k$. This is repeated $k = \log_2(n)$ times,
until we get the final $P$, $P^{(0)}$:

\eq{
	P^{(0)} &= a^{(0)}_0 G^{(0)}_0 + b^{(0)}_0 H^{(0)}_0 + a^{(0)}_0 b^{(0)}_0 Q \\
	P^{(0)} &= P^{(k)} + \sum^k_{i=1}(L_i u^2_i + u^{-2}_i R_i)
}

Finally $\P$ sends $(a^{(0)}, b^{(0)})$ to $\V$.

The entirety of the dialogue between $\V$ and $\P$,
the so called \textit{transcript}, can be summarized as the following:

\eq{
	\P \rightarrow \V &: L_k, R_k \\
	\V \rightarrow \P &: u_k \\[5pt]
	\P \rightarrow \V &: L_{k-1}, R_{k-1} \\
	\V \rightarrow \P &: u_{k-1} \\[-5pt]
	                  &\vdots \\
	\P \rightarrow \V &: L_{1}, R_{1} \\
	\V \rightarrow \P &: u_{1} \\[5pt]
	\P \rightarrow \V &: a^{(0)}, b^{(0)} \\
}

To make the inner product proof \textit{non-interactive}, the
Fiat-Shamir Heuristic (\ref{fiat-shamir-heuristic}) can be used.

\subsubsection{Verifiers Algorithm}
$\V$ has knowledge of the following values:
\eqn{def1-ver}{
	a^{(0)}, b^{(0)} &\in \Z_p \\
	\vec{L}, \vec{R} &\in \G_p^{k} \\
	\vec{G}, \vec{H} &\in \G^n \\
	P^{(k)}, Q &\in \G \\
	\vec{u} &\in \Z^{k} \\
}

We need a way to get the final $G^{(0)}$ and $H^{(0)}$, from $\vec{G}$
and $\vec{H}$, let's start with the former. We recall our reduced
version of $\vec{G}$:

\eqn{G0}{
	\vec{G}^{(j-1)} = \veclo{G^\textit{(j)}} u^{-1}_j + \vechi{G^\textit{(j)}} u^{1}_j
}

We want to define a vector $\vec{s}$ such that $\dotp{\vec{s}}{\vec{G}}
= G^{(0)}$. From equation \ref{G0} we conclude that each $s_i$ is
defined as:

\eq{
	&s_i = u^{b(i,k)}_k \cdots u^{b(i,1)}_1 \\
	&\textbf{where:} \\
	&b(i,j) = 
	\begin{cases}
		\text{-1} &\quad  g(i,j) = \top \\
		\text{1}  &\quad  g(i,j) = \bot \\
	\end{cases} \\
	&g(i,j) = 
	\begin{cases}
		\top &\quad  \text{if $(i$ mod $2^j) <    2^{j-1}$} \\
		\bot &\quad  \text{if $(i$ mod $2^j) \geq 2^{j-1}$} \\
	\end{cases}
}

We define $b(i,j)$\footnote{Note that in the code we zero-index
so $j_{\text{code}} = j+1$.} to be -1 if $G_i$ appears in the left
half of $\vec{G}^{(j)}$, and 1 if $G_i$ appears in the right half
of $\vec{G}^{(j)}$. Therefore $G_i$ is in the right side if
$(i$ mod $2^j) < 2^{j-1}$.

We make a similar argument for $H^{(0)}$. We want to define a vector
$\vec{s'}$ such that $\dotp{\vec{s'}}{\vec{H}} = H^{(0)}$:
\eqn{H0}{
	\vec{H}^{(k-1)} = \veclo{H^\textit{(k)}} u^{1}_j + \vechi{H^\textit{(j)}} u^{-1}_j
}

To construct $\vec{s'}$:

\eq{
	&s_i' = u^{b'(i,k)}_k \cdots u^{b'(i,1)}_1 \\
	&\textbf{where:} \\
	&b'(i,j) = 
	\begin{cases}
		\text{-1} &\quad  \lnot g(i,j) = \top \\
		\text{1}  &\quad  \lnot g(i,j) = \bot \\
	\end{cases} \\
}

But this is indeed just:

\eq{
	\vec{s'} = \frac{1}{s_1}, \frac{1}{s_2}, \cdots \frac{1}{s_n}
}

Leading us to our desired $G^{(0)}, H^{(0)}$:

\eq{
	G^{(0)} &= \dotp{\vec{s}}{\vec{G}} \\
	H^{(0)} &= \dotp{\vec{s'}}{\vec{H}}
}

Now $\V$ can simply check if $P^{(k)} \stackrel{?}{=} P^{\V}$,
if the former statement is true, then the $\V$ concludes that
$\dotp{\vec{a}}{\vec{b}} = c$:

\eq{
	P^{(k)} &\stackrel{?}{=} P^{\V} \\
	        &\stackrel{?}{=} a^{(0)}G^{(0)} + b^{(0)}H^{(0)} + a^{(0)}b^{(0)}Q - \sum^k_{i=1} (L_i u^2_i + u^{-2}_i R_i) \\
	        &\stackrel{?}{=} \dotp{a\vec{s}}{\vec{G}} + \dotp{b\vec{s'}}{\vec{H}} + abQ - \sum^k_{i=1} (L_i u^2_i + u^{-2}_i R_i)
}

\subsection{Range Proofs:}\label{range-proofs}

A range-proof is a zero-knowledge proof which seeks to prove that for
some value $v$, $v$ lies within the range of $[0,2^n)$ without revealing
$v$ or any additional information about $v$, thus zero-knowledge. We
wish to express this property of $v$ in a single inner product to be
able to use an inner product proof as described above.

\subsubsection{Prover's Algorithm}\label{prover-range-proofs}

To start, the prover, $\P$, is given the following values:

\eqn{def1}{
	v &\in \Z_p \\
	n &\in \Z\\
	B, \tB &\in \G\\
	\vec{G}, \vec{H} &\in \G^n \\
}

$\P$ wishes to convince the verifer, $\V$ that the value, $v$ lies
within the range $[0,2^n)$.

The first step towards this is for $\P$ to commit to $v$ using
a Pedersen commitment $V$, and sending this to the $\V$, as well
commiting to the desired range $n$, which is not blinded. With the
Pedersen-commitment having base-point $B$ and blinding point $\tB$:

\eq{
	V = vB + \tv \tB
}

If we let $\vec{a}$ be $v$ expressed in bits then we know the following:

\eq{
	\dotp{\vec{a}}{\vec{2^n}} = v
}

Additionally, as we also need a guarantee that $\vec{a}$ consists
entirely of bits i.e. $\vec{a} \in \{0,1\}^n$. This can be
proven by the following property:

\eq{\vec{a}\circ (\vec{a} - \vec{1}) = \vec{0}}

This property will only hold if $\vec{a}$ has entries that are either
0 or 1. Due to needing to commit to both $\vec{a}$ and $\vec{a}
- \vec{1}$, they will henceforth be referred to as $\vecl{a}$
and $\vecr{a}$ respectively and add another property to show the
relationship between these two which gives us the following final
properties:

\eq{
	\dotp{\vecl{a}}{\vec{2^n}} = v \\
	\vecl{a}\circ \vecr{a} = \vec{0} \\
	(\vecl{a} - \vec{1}) - \vecr{a} = \vec{0}
}

We wish to combine these properties into a single inner product which
$\P$ will then prove to $\V$. To do this we observe that $\vec{a}
= \vec{0} \iff \forall y\in\mathbb{Z}: \dotp{\vec{a}}{\vec{y^n}} =
\vec{0}$. So $\P$ lets $\V$ chose a random scalar $\ran{y}$ and using
$\ran{y}$ we get a new set of properties:

\eq{
	\dotp{\vecl{a}}{\vec{2^n}} = v \\
	\dotp{\vecl{a} - \vec{1} - \vecr{a}}{\vecran{y^n}} = 0 \\
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} = 0
}

Next $\P$ lets $\V$ chose another random scalar $\ran{z}$, and using
this we can combine the three properties above to the following single
statement:

\eq{
	\ran{z^2}v = 
	\ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a} - \vec{1} - \vecr{a}}{\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}}
}

With this we have condensed the properties we wish to describe into
a single statement. We wish to rewrite this as a single inner product
where $\vecl{a}$ appears only in the first argument of the inner product
and $\vecr{a}$ appears only in the second argument and all non-secret
terms are factored out.

First we can rewrite the middle term:

\eq{	
	&\ran{z^2}v &&= 
	\ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a}}{\vecran{y^n}} -
	\ran{z}\dotp{\vec{1}}{\vecran{y^n}} -
	\ran{z}\dotp{\vecr{a}}{\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a}}{\vecran{y^n}} -
	\ran{z}\dotp{\vec{1}}{\vecr{a}\circ\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n}} +
	\dotp{\vecl{a}}{\ran{z}\vecran{y^n}} +
	\dotp{-\ran{z}\vec{1}}{\vecr{a}\circ\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} +
	\dotp{-\ran{z}\vec{1}}{\vecr{a}\circ\vecran{y^n}}
}

For the final step towards combining all the conditions into a single
inner product we add $\dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} +
\ran{z}\vecran{y^n}}$ to both sides and simplify again:

\eq{
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n}}
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} +
	\dotp{-z\vec{1}}{\vecr{a}\circ\vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n}} \\ 
	&\ran{z^2}v + (\ran{z} - \ran{z^2})\dotp{\vec{1}}{\vecran{y^n}} - \ran{z^3}\dotp{\vec{1}}{\vec{2^n}} &&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n}+\ran{z}\vecran{y^n} + \vecr{a}\circ\vecran{y^n}}
	\\
	&\ran{z^2}v + (\ran{z} - \ran{z^2})\dotp{\vec{1}}{\vecran{y^n}} - \ran{z^3}\dotp{\vec{1}}{\vec{2^n}} &&= \dotp{\vecl{a}- \ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}}
}

We define a function $\delta(y,z) = (z - z^2)\dotp{\vec{1}}{\vec{y^n}}
- z^3\dotp{\vec{1}}{\vec{2^n}}$ and finish simplifying:

\eq{
	\ran{z^2}v + \delta(\ran{y},\ran{z}) = \dotp{\vecl{a} - \ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + (\vecr{a} + \ran{z}\vec{1})\circ\vecran{y^n}}
}

From here on we will refer to the first argument of the inner product
as $\vec{l_0}$ and the second argument as $\vec{r_0}$:

\eq{
	\ran{z^2}v + \delta(\ran{y},\ran{z}) = \dotp{\vec{l_0}}{\vec{r_0}}
}

If the goal was simply to construct a single inner product which
would prove that $v$ lies in $[0,2^n)$, then $\P$ could simply send
these values to $\V$ and be done. However, we wish for the proof to be
zero-knowledge and therefore this inner product needs to be blinded. To
do this, $\P$ constructs a polynomial $t(x)$. This polynomial will
be constructed using $\vec{l_0}$ and $\vec{r_0}$ in a way that blinds
them, while still allowing $\P$ to prove to $\V$ that $v$ lies in
$[0,2^n)$.

First $\P$ chooses random blinding factors $\vecran{s_L}$ and
$\vecran{s_R}$ such that $\vecl{s},\vecr{s}\in \Z^n_p$. Using these
$\P$ can costruct two vector-function $\vec{l}(x)$ and $\vec{r}(x)$
in the following way:

\eq{
	\vec{l}(x) &= \vec{l_0} + \vec{l_1}x \\ &= (\vecl{a} - \ran{z}\vec{1}) + \vecl{s}x \\ &= (\vecl{a} + \vecl{s}x) - \ran{z}\vec{1}\\&\\
	\vec{r}(x) &= \vec{r_0} + \vec{r_1}x \\ &= (\ran{z^2}\vec{2^n} + (\vecr{a} + \ran{z}\vec{1})\circ\vecran{y^n}) + \vecr{s}x\circ\vecran{y^n} \\ &= ((\vecr{a} + \vecr{s}x) + \ran{z}\vec{1}) \circ \vecran{y^n} + \ran{z}^2\vec{2^n}
}

Now $\P$ can construct $t(x)$ as follows:

\eq{
	t(x) = \dotp{\vec{l}(x)}{\vec{r}(x)}
}

We group $x$ to get:

\eq{
	&t(x) = t_0 + t_1x + t_2x^2 \\
	&\textbf{where:} \\
	&t_0 = \dotp{\vec{l_0}}{\vec{r_0}}\\
	&t_2 = \dotp{\vec{l_1}}{\vec{r_1}}\\
	&t_1 = \dotp{\vec{l_0}+\vec{l_1}}{\vec{r_0} + \vec{r_1}} - t_0 - t_2
}

Now $\P$ wishes to prove to $\V$ that $t_0$ and $t(x)$ are correctly
constructed. So, $t_0 = \ran{z^2}v + \delta(\ran{y},\ran{z})$, and $t(x)
= \dotp{\vec{l}(x)}{\vec{r}(x)}$ such that, $\vec{l}(x) = \vec{l_0}
+ \vec{l_1}x$, and, $\vec{r}(x) = \vec{r_0} + \vec{r_1}x$.

First $\P$ wants to show that $t_0$ is correctly constructed. $\P$
starts by making commitments to each of the coefficients of $t(x)$. Here
we make note of the fact that $\P$ has already commited $t_0$. Namely,
commitment, $V$, made at the very start, is also a commitment to $t_0$
as per the construction of $t_0$. $\P$ makes commitments to $T_1 =
C(t_1, \tt_1)$ and $T_2 = C(t_2, \tt_2)$. After commiting these values
to $\V$, $\V$ sends back a challenge scalar $\ran{x}$. We know from
the definitions of $V, T_1, T_2$ that these commitments relate in the
following manner:

\eq{
	t(\ran{x})B                   &= \ran{z^2}vB      &&+ \delta(\ran{y},\ran{z})B &&+ \ran{x}t_1B       &&+ \ran{x^2}t_2B \\
	\tt(\ran{x})\tB               &= \ran{z^2}\tv \tB &&+ 0\tB                     &&+ \ran{x} \tt_1 \tB &&+ \ran{x^2} \tt_2 \tB\\
	t(\ran{x})B + \tt(\ran{x})\tB &= \ran{z^2}V       &&+ \delta(\ran{y},\ran{z})B &&+ \ran{x}T_1        &&+ \ran{x^2}T_2
}

The sum of the two top-most coefficients in each 'column' is also
equal to the bottom coefficient of that column. To convince $\V$, $\P$
sends $t(\ran{x})B$ and $\tt(\ran{x})\tB$, evaluated at $\ran{x}$
to $\V$. This is enough to convince $\V$ that $t_0 = \ran{z^2}v +
\delta(\ran{y},\ran{z})$, which is seen in The Verifier Algorithm.
(\ref{verifier-range-proof})

Next $\P$ wishes to convince $\V$ that $\vec{l}(x)$ and
$\vec{r}(x)$ are both constructed correctly and that $t(x) =
\dotp{\vec{l}(x)}{\vec{r}(x)}$. To achieve this $\P$ will use a similar
construction as seen above. We want $\P$ to make commitments that allows
$\V$ to relate $\vec{l}(x)$ and $\vec{r}(x)$ to $\vecl{a}, \vecl{s},
\vecr{a}$ and $\vecr{s}$. It is not that simple however. As seen here:

\eq{
	\vec{r}(x) = \ran{z^2}\vec{2^n} + ((\vecr{a} + \vecr{s}x) + \ran{z}\vec{1})\circ\vecran{y^n}
}

This means we want $\P$ to make commitments to $\vec{y^n}\circ
\vecr{a}$ and $\vec{y^n}\circ\vecr{s}$. This is a problem as these
commitments need to be made and sent to $\V$ BEFORE $\P$ gets the
challenge $\ran{y}$ from $\V$. To fix this issue $\P$ will make a
special commitment to $\vecl{a}$ and $\vecr{a}$ as follows:

\eq{
	C(\vecl{a}, \vecr{a}, \widetilde{a}) = \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecr{a}}{\vec{H}} + \widetilde{a}\tB
}

This same construction is used for the commitment to $C(\vecl{s},
\vecr{s}, \widetilde{s})$. Here we notice that:

\eq{
	C(\vecl{a}, \vecr{a}, \widetilde{a}) &= \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecr{a}}{\vec{H}} + \widetilde{a}\tB \\
	&= \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecran{y^n}\circ \vecr{a}}{\vecran{y^{-n}}\circ \vec{H}} + \widetilde{a}\tB
}

We define $\vec{H'} = \vecran{y^{-n}}\circ\vec{H}$, $A =
C(\vecl{a},\vecr{a}, \widetilde{a})$ and $S = C(\vecl{s}, \vecr{s},
\widetilde{s})$ and can now construct our argument similarly to what
was done in the previous segment:

\eq{
	\dotp{\vec{l}(\ran{x})}{\vec{G}} &= \dotp{\vecl{a}}{\vec{G}} &&+ \ran{x}\dotp{\vecl{s}}{\vec{G}} &&+ \dotp{-\ran{z}\vec{1}}{\vec{G}} \\
	\dotp{\vec{r}(\ran{x})}{\vec{H'}} &= \dotp{\vecr{a}}{\vec{H}} &&+ \ran{x}\dotp{\vecr{s}}{\vec{H}} &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}}\\
	\widetilde{e}\tB &= \widetilde{a}\tB &&+ \ran{x}\widetilde{s}\tB &&+ 0 \tB \\
	\dotp{\vec{l}(\ran{x})}{\vec{G}} + \dotp{\vec{r}(\ran{x})}{\vec{H'}} + \widetilde{e}\tB &= A &&+ \ran{x}S &&+ (\dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}} - \ran{z}\dotp{\vec{1}}{\vec{G}})
}

From here, using the same argument as above $\P$ sends $\widetilde{e}$
to $\V$. This will be enough to convince $\V$ that $\vec{l}(x)$
and $\vec{r}(x)$ are correctly constructed and that $t(x) =
\dotp{\vec{l}(x)}{\vec{r}(x)}$ and together with the previous
commitments to $\V$, $\P$ will be able to successfully convince $\V$
that $v$ lies in the range $[0, 2^n)$.

In summary the transcript between $\V$ and $\P$, will end up being
the following:

\eq{
	\P \rightarrow \V &: V, n, A, S \\
	\V \rightarrow \P &: \ran{y}, \ran{z} \\
	\P \rightarrow \V &: T_1, T_2 \\
	\V \rightarrow \P &: \ran{x} \\
	\P \rightarrow \V &: t(\ran{x})B, \tt(\ran{x})\tB, \widetilde{e} \hspace*{1cm}\text{($t(x)$ evaluated with challenge $\ran{x}$)}\\
}

Just as with the inner product the Fiat-Shamir Heuristic
(\ref{fiat-shamir-heuristic}) can be used to make this process
non-interactive.

\subsubsection{Verifier's Algorithm:} \label{verifier-range-proof}

$\V$ has access to the following values:

\eqn{range-def1-ver}{
	\ran{y}, \ran{z}, \ran{x}, \widetilde{e} &\in \Z_p \\
	\vec{L}, \vec{R} &\in \G_p^{k} \\
	\vec{G}, \vec{H}, \vec{H'} &\in \G^n \\
	V, A, B, T_0, T_1, B, \tB &\in \G \\
	t(\ran{x})B, \tt(\ran{x})\tB &\in \G \hspace*{1cm} \text{(evaluated at x)}
}

$\V$ needs to make a check for the two properties that $\P$ want to
prove to it.

To check the first property $\V$ simply checks if the following
equation holds:

\eq{
	t(\ran{x})B + \tt(\ran{x})\tB \stackrel{?}{=} \ran{z^2} + \delta(\ran{y},\ran{z})B + \ran{x}T_1 + \ran{x^2}T_2
}

And if it does then that will convince $\V$ that $t(x) = \ran{z^2}v +
\delta(\ran{y},\ran{z}) + t_1x + t_2\ran{x^2}$ due to the column-sum
argument made in the Prover's Algorithm. (\ref{prover-range-proofs})

Similarly to become convinced of the second property $\V$ checks if
this equation holds:

\eq{
	P &= -\widetilde{e}\tB &&+ A &&+ \ran{x}S &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}} &&- \ran{z}\dotp{\vec{1}}{\vec{\vec{G}}} \\
	&= -\widetilde{e}\tB &&+ A &&+ \ran{x}S &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}\circ\vecran{y^{-n}}}{\vec{H}} &&- \ran{z}\dotp{\vec{1}}{\vec{G}}
}

From the column-sum argument we can deduce that $P =
\dotp{\vec{l}(x)}{\vec{G}} + \dotp{\vec{r}(x)}{\vec{H'}}$
assuming an honest $\P$. Thus $\V$ can use $t(x)$ and $P$ as
inputs into the inner product protocol to prove that $ t(x) =
\dotp{\vec{l}(x)}{\vec{r}(x)}$. And if this proof is verified then
$\V$ is now convinced that $v\in [0,2^n)$ without knowing any other
properties of $v$, thus the range-proof is complete.

Additionally by looking at the trancript one can deduce that that the
only way for $\P$ to construct $A$ and $V$ such that they can convince
$\V$ that v lies in the range, would be if they can correctly guess
\textit{all} three challenge scalars, $\ran{y}, \ran{z}$ and $\ran{x}$
in order to 'cheat'. This happens with a probability so small it is
entirely neglegable and thus the proof is also sound.

But the proof being sound does not necessarily mean it is
zero-knowledge. The proof also needs to be Zero-knowledge. If we look
at all the values availible to $\V$ we can see that it has only two
of these values have any direct relation to $v$, these being $V$ and
$t(\ran{x})B$. It is trivial to see that $\V$ cannot learn anything
about $v$ from $V$ without knowledge of $\tv$ which $\V$ does not
know. If we look at the definition of $t(\ran{x})B$ we can see that
it DOES use $v$ in the calculation directly. However the definition
of $t(x)$ uses $t_2$, which is blinded using $\vecran{s_L}$ and
$\vecran{s_R}$ neither of which $\V$ has any way to access. Additionally
$t_1$ also uses the definition of $t_2$ and thus is \textit{also} cannot
be infered. Thus there is no amount of calculations that $\V$ can do to
figure out anything about $v$ and thus the proof is also zero-knowledge.

\subsection{Aggregation of rangeproofs:}

An aggregated range-proof is a rangeproof which proves that a series
of values, $\vec{v} \in \Z^m$, lie within a certain range:

\eqn{agg-range}{
	[0,2^n)
}

for some integer $n$. This is done by aggregating $m$ range-proofs for
each individual value $v_j$ into a single proof which, if verified,
proves that each $v_j$ given lies within the range $[0,2^n)$, while
giving away no more information about any $v_j$. This is done by using
a multi-party protocol (MPC) where each party $\P_{(j)}$, takes on the
role of an independant prover $\P$, meaning it has a value $v_j$ it
needs to prove lies within the range (\ref{agg-range}). Each $\P_{(j)}$
will converse with the so-called dealer $\D$, who takes on the role
of $\V$, thus being responsible for collecting the various commitments
from each $\P_{(j)}$ and generating challenges which will be used by all
$\P_{(j)}$'s. At the very end, $\D$ will then collect all the individual
rangeproofs for each value and aggregate them. It is important to note
that all parties are in agreement over a specific $n$.

The steps used to do this are the same steps used in the construction
of a rangeproof with a few additional steps which aggregate the proofs
into a single rangeproof.

Each $P_{(j)}$ is
also assigned a 'position' $j$. Each party will individually commit
the needed values to the dealer in order to convince them that their
value lie within the range, with one key difference. Each party uses
an offset to ensure the order matters to the dealer, preventing the
parties from working together to 'cheat'. This offset is defined as
such for the jth party:

\eq{
	z_{(j)}   &= z^j \cdot \vec{y^n_{(j)}} \\
	y^n_{(j)} &= y^{n \cdot m}[j \cdot n:(j+1) \cdot n]
}

The evaluation point $\ran{x}$ is the same for all parties. The choice
of $\ran{y}, \ran{z}$ and $\ran{x}$ is done where it would ordinarily
be done, but only once $\D$ has received the needed values from ALL
$\P_{(j)}$. Once again, this can also be done using the Fiat-Shamir
Heuristic. (\ref{fiat-shamir-heuristic})

After each party has created $t_{(j)}(x) =
\dotp{\vec{l}_{(j)}(x)}{\vec{r}_{(j)}(x)}$ it would be sufficient
to perform the proving steps from \ref{range-proofs} for each
party. However this will require $\D$ to make $m$ checks to see if each
$v_j$ lies within the range (\ref{agg-range}). However $\D$ can do a
few more computations to create a singular proof which requires a single
check to see that ALL $v_j$ lie within the range. (\ref{agg-range})

For $\P_{(j)}$ to prove that its polynomial $t_{(j)}(x)$ is correct
means proving that $t_{0,(j)}$ is correct as well as proving that
$\vec{l}_{(j)}(x)$ and $\vec{r}_{(j)}(x)$ are created correctly and
that $t_{(j)} = \dotp{\vec{l}_{(j)}(x)}{\vec{r}_{(j)}(x)}$.

Each $\P_{(j)}$ sends their $T_{1,(j)}$ and $T_{2,(j)}$, to the dealer,
which can then compute:

\eq{
	T_1 &= \sum^{m-1}_{j = 0} T_{1,(j)}\\
	T_2 &= \sum^{m-1}_{j = 0} T_{2,(j)}\\
	\delta(y,z) &= \sum^{m-1}_{j = 0} \delta_{(j)}(y,z)
}

and from here $\D$ sends back the challenge
$\ran{x}$ and $\P_{(j)}$ will return $t_{(j)}(\ran{x})B$ and
$\tt_{(j)}(\ran{x})\tB$. $\D$ can then sum these
together as well:

\eq{
t(x)B &= \sum^{m-1}_{j = 0} t_{j}(x)B\\
\tt(x)\tB &= \sum^{m-1}_{j = 0} \tt_{j}(x)\tB
}

And finally convince themselves that for ALL $t_{0,(j)}$ are correct
by performing the following check:

\eq{
	t(\ran{x})B + \widetilde{t}(\ran{x})\widetilde{B} \stackrel{?}{=} \ran{z^2}\sum^{m-1}_{j = 0} \ran{z_{(j)}} \cdot V_{(j)} + \delta(\ran{y},\ran{z})B + \ran{x}T_1 + \ran{x^2}T_2
}

we know that $z_{(j)} = z^j$ we can make a small rewrite:

\eq{
	t(\ran{x})B + \widetilde{t}(\ran{x})\widetilde{B} \stackrel{?}{=} \sum^{m-1}_{j = 0} \ran{z^{j+2}} \cdot V_{(j)} + \delta(\ran{y},\ran{z})B + \ran{x}T_1 + \ran{x^2}T_2
}


And this check will then convince $\D$ that all $t_{0,(j)}$
are correct.

Just as with a singular rangeproof each $\P_{(j)}$ also wishes to prove
that each $\vec{l_{(j)}(x)}$ and $\vec{r_{(j)}(x)}$ are constructed
correctly. This is done in a similar manner as proving $t_{0,(j)}$
are all correct. Proving this property for the jth party would be
proving the following:

\eq{
	\dotp{\vec{l}_{(j)}(\ran{x})}{\vec{G}_{(j)}} + \dotp{\vec{r}_{(j)}(\ran{x})}{\vec{H'}_{(j)}} \stackrel{?}{=} -\widetilde{e}_{(j)}\widetilde{B} + A_{(j)} + \ran{x}S_{(j)} &&+ (\dotp{\ran{z}\vecran{y^n_{(j)}} + \ran{z^2}\ran{z_{(j)}}\vec{2^n}}{H'}_{(j)} - \ran{z}\dotp{\vec{1}}{\vec{G}_{(j)}})
}

$\P_{(j)}$ will send their $\dotp{\vec{l}_{(j)}(\ran{x})}{\vec{G}_{(j)}},
\dotp{\vec{r}_{(j)}(\ran{x})}{\vec{H'}_{(j)}}$ and $\widetilde{e}_{(j)}$
which $\D$ can then combine in the following manner:

\eq{
	\vec{l}(x) &= \vec{l_0}(x) &&\concat \vec{l_1}(x) &&\concat \dots &&\concat \vec{l_{m-1}}(x)\\
	\vec{r}(x) &= \vec{r_0}(x) &&\concat \vec{r_1}(x) &&\concat \dots &&\concat \vec{r_{m-1}}(x)\\
	\vec{G} &= \vec{G_0} &&\concat \vec{G_1} &&\concat \dots &&\concat \vec{G_{m-1}}\\
	\vec{H'} &= \vec{H'_0} &&\concat \vec{H'_1} &&\concat \dots &&\concat \vec{H'_{m-1}}
}

Additionally $\D$ can construct the following:

\eq{
	\vecran{y^n_{(j)}} &= \ran{y^{n \cdot m}}[j \cdot n : (j+1) \cdot n]\\
	\ran{z_{(j)}} &= \ran{z^j}\\
	\widetilde{e} &= \sum^{m-1}_{j = 0} \widetilde{e}_{(j)}\\
	A &= \sum^{m-1}_{j = 0} A_{(j)}\\
	S &= \sum^{m-1}_{j = 0} S_{(j)}
}

Note that $\D$ has already recieved $A_{(j)}$ and $S_{(j)}$ from the
first step of the transcript. $\D$ can then use the following check
to convince itself that all $\vec{l_{(j)}}(x)$ and $\vec{r_{(j)}}(x)$
are correctly constructed:

\eq{
	\vec{l_{(j)}}(\ran{x}) + \vec{r_{(j)}}(\ran{x}) \stackrel{?}{=} -\widetilde{e}\widetilde{B} + A + \ran{x}S - \ran{z}\dotp{\vec{1}}{\vec{G}} + \ran{z}\dotp{\vecran{y^{n \cdot m}}}{\vec{H'}} + \sum^{m-1}_{j = 0}\dotp{\ran{z^{j+2}} \cdot \vec{2^n}}{\vec{H'}[j \cdot n: (j+1) \cdot n]}
}

And if this holds then $\D$ is convinced that $\vec{l_{(j)}}(x)$ and
$\vec{r_{(j)}}(x)$ are correctly constructed and that $l_{(j)}(x)
= \dotp{\vec{l}_{(j)}(x)}{\vec{r}_{(j)}(x)}$. This, along with
the previous part on top $\D$ is now convinced that all parties'
$v_{(j)}$ are within the range (\ref{agg-range}) and thus the
proof is complete. Additionally, using the same arguments as in
\ref{verifier-range-proof} for each $\P_{(j)}$, this is also sound
and zero-knowledge.

This protocol in transcript form would be the following: 

\eq{
	&\P_{(j)} &&\rightarrow \D &&: V_{(j)}, n, A_{(j)}, S_{(j)} \\
	&\D &&\rightarrow \P_{(j)} &&: \ran{y}, \ran{z}, j \\
	&\P_{(j)} &&\rightarrow \V &&: T_{1, (j)}, T_{2,(j)} \\
	&\D &&\rightarrow \P_{(j)} &&: \ran{x} \\
	&\P_{(j)} &&\rightarrow \V &&: t_{(j)}(\ran{x})B, \tt_{(j)}(\ran{x})\tB, \widetilde{e} \hspace*{1cm}\text{($t_{(j)}(x)$ and $\tt_{(j)}(\ran{x})\tB$ both evaluated with challenge $\ran{x}$)}\\
}

This can be visualized using a graph:

\includegraphics[scale=0.505]{img/multi-party protocol.png}

\subsection{Fiat-Shamir Heuristic}\label{fiat-shamir-heuristic}

The Fiat-Shamir Heuristic is a cryptographic concept, which allows a
prover to compute random values without the need to consult the verifier
first. This is done by defining a hash function $\textbf{H}()$ which
can hash any number of values into a single pseudo-random value. When
the verifier would be needed to provide a challenge, instead, all
public values that $\V$ would have access to are hashed using this
hash function and the resulting value is used as the challenge. This
way the prover can send all values it must generate at once, and the
verifier can simply make the Hashes which an honest prover would have
done to find the challenges and assert correctness.

To show this we will once more look at the Schnorr Identity Protocol
if it used the Fiat Shamir Heuristic. We will use the same notation
we used in \ref{zero-knowledge}.

$\P$ wishes to show $\V$ that they know the secret value $x$ such that
$X = xG$ for the public curve-points $X$ and $G$. Ordinarily $\P$
would send a commitment $K$ to a randomly chosen k and \textit{send
this to $\V$}, who would then send back a challenge. However with the
Fiat-Shamir Heuristic, $\P$ will instead compute:

\eq{
	e = \textbf{H}(X,P,K)
}

From here $\P$ can compute $s = k + ex$ and send $K$ and $s$ to $\V$.

From there $\V$ can compute $e = \textbf{H}(X,P,K)$ and at this
point the rest of the protocol functions the same as it does in
\ref{zero-knowledge}. The transcript for this proof would thus go from:

\eq{
	\P \rightarrow \V &: K \\
	\V \rightarrow \P &: e \\
	\P \rightarrow \V &: s \\
}

To the much shorter:

\eq{
	\P \rightarrow \V: K, s
}

Crutically allowing $\P$ to complete the entire proof without ever
needed to hear from $\V$. The proof is still sound as $\P$ cannot get
$e$ without commiting to a $K$ first.

Our Fiat-Shamir for the bulletproofs algorithm requires a
'overseer' for the parties who can gather all the needed values
to create the challenges. In essence when a challenge is needed
all parties will send their values to this local overseer
who can then generate the challenges using the Fiat-Shamir
Heuristic. Specifically $y = \textbf{H}(\vec{V},\vec{A},\vec{S}),
z = \textbf{H}(\vec{V},\vec{A},\vec{S}, y)$ and $x =
\textbf{H}(\vec{V},\vec{A},\vec{S}, y, \vec{T_1},\vec{T_2})$. This
will result in the following final transcript:

\eq{
	&\P_{(j)} &&\rightarrow \D &&: V_{(j)}, n, A_{(j)}, S_{(j)}, j, T_{1, (j)}, T_{2,(j)}, t_{(j)}(\ran{x})B, \tt_{(j)}(\ran{x})\tB, \widetilde{e}
}

Finally, recently there was a vulnerability within the Fiat-Shamir
Heuristic suggested in \cite{bulletproofs}, in which a malicious
prover could cheat successfully every time. This was because in the
original paper the Fiat-Shamir Heuristic did not include $\vec{V}$
in their hash function which allowed a malicious $\P$ to get a proof
verified even if one or more $vs$ did not lie within the range. This
vulnerability has since been fixed, and is thus not in effect anymore.

\section{Our Contributions:}

\subsection{Hacspec:} \label{Hacspec}

Hacspec is a subset of the programming language of Rust. It's purpose is
to be a specification language for cryptographic algorithms, with the
special properties that it is executable (since it is valid rust code),
but also that it is defined such that it can easily be transpiled
into theorem solvers, namely Coq and F*. The cost for having this
property is certain syntactic enhancements that are available in Rust
are not defined in Hacspec. This makes the language simpler, but it
also makes it harder to simply express some concepts due to the lack
of syntactic sugar. The language however is still in development,
balancing and adding features.

All code written for this project was written to be Hacspec
compliant. Our implementation is a specification meant to be simply
understood compared to a more obfuscated, but highly optimized
implementation. We will use property based testing with QuickCheck
\cite{quickcheck} where possible and convienient and test our
Bulletproofs implementation against the Dalek-Cryptography group's
Bulletproofs Rust implementation \cite{dalek}.  We have also created
a minimal linear algebra library, in hacspec, as it was thought that
it could help with the implementation while also creating a library
to be used with other cryptographic algorithms.

\subsection{Linear Algebra Library:}
It was decided that the specification should of course consist of what
we need, but also of some general linear algebra functions that could
be used by other specifications. The implemented operations included
matrix instantiation, slicing, transposition, scalar multiplication,
addition, subtraction, hadamard product and matrix multiplication.

Since we were not able to find any general linear algebra
specification, we simply implemented the desired functionality,
consulting \cite{linear-algebra} when necessary. All functions were
only defined over matrices, because any vector operation can also
be performed if modelling the vector as a matrix. This simplified
our library somewhat. These matrices consisted of a pair of number
representing dimension and a list of scalars:

\begin{lstlisting}
pub type DimType = usize;
pub type Scalar = i128;
pub type Dims = (DimType, DimType);
pub type Matrix = (Dims, Seq<Scalar>);
pub type MatRes = Result<Matrix, u8>;
pub type ScalRes = Result<Scalar, u8>;
\end{lstlisting}

We model matrices as a \texttt{Seq<Scalar>} instead of a
\texttt{Seq<Seq<Scalar>>} since we had problems with the hacspec type
checker when double indexing (\texttt{list[i][j]}), and it is more
efficient. Since this part was intended to be a library we wanted to
generalize over the \texttt{Scalar} type. We wanted to at least have
it be generalizable over any type that can be defined using hacspec
macros, for example the following Scalar:

\begin{lstlisting}
public_nat_mod!(
    type_name: Scalar,
    type_of_canvas: ScalarCanvas,
    bit_size_of_field: 256,
    modulo_value: "1000000000000000000000000000000014def9dea2f79cd65812631a5cf5d3ed"
);
\end{lstlisting}

This turned out to be a problem though, as Hacspec does not support
generics. Luckily, generics is on the roadmap and after discussion
with the Hacspec Team, it was decided that we would generalize our
library with generics such that the Hacspec developers can use it
as an example of how generics might be needed. Take following matrix
initialization function:

\begin{lstlisting}
pub fn new(rows: DimType, cols: DimType, seq: Seq<Scalar>) -> MatRes {
...
\end{lstlisting}

It would be rewritten as:

\begin{lstlisting}
pub fn new<T>(rows: DimType, cols: DimType, seq: Seq<T>) -> MatRes<T>
where
    T: hacspec_lib::Integer, {
...
\end{lstlisting}

With the following type aliases used instead:

\begin{lstlisting}
type DimType = usize;
type Dims = (DimType, DimType);
type MatRes<T> = Result<Matrix<T>, u8>;
type ScalRes<T> = Result<T, u8>;
\end{lstlisting}

Which would allow anyone using the library to use matrices with their
own defined hacspec types. Notice the \texttt{hacspec\_lib::Integer},
this trait is implemented by all integers.

\textbf{Testing:}

We utilized quickcheck to do property based testing in order to ensure
that our specification had the same functionality as Nalgebra. All
the these tests followed the same standard pattern, where we create a
helper function that does the following: Construct two random matrices
of random dimensions, make sure no dimension is set to 0, run equivelent
operations on them and finally assert that the operations had the same
effect. A simple example is the \texttt{zeros} function:

\begin{lstlisting}
#[test] fn test_nalg_zeros() {
    fn helper(n: u8, m: u8) -> TestResult {
        let n = n as usize; let m = m as usize;

        if n * m == 0 {
            return TestResult::discard();
        }

        let hac = zeros(n, m); let ext = DMatrix::zeros(n, m);

        TestResult::from_bool(assert_matrices(hac, ext))
    } quickcheck(helper as fn(u8, u8) -> TestResult);
}
\end{lstlisting}

Here the only operation that is performed is the construction of the
vectors. The \texttt{assert\_matrices} function is simply a function
that checks each element in the two vectors and assert them all to
be equal.

\subsection{Implementing Ristretto:} \label{implementing-ristretto}

For the Ristretto implementation on Curve25519 we used an IETF-standard
specification and the paper of exactly this, which was used as a guide
for our implementation \cite{ristretto-ietf}.  This specification
was very helpful when implemting, as it clearly defined the types and
functionalities needed to implement Ristretto securely. However there
was one operation that needed to be implemented, which did \textit{not}
have a standard forumla for its implementation. This method was scalar
multiplication. For this we used the specification from (Insert
Reference to literature here). This proved to work perfectly, with
the given tests and the IETF-standard methods.

Ristretto is, in essence, a prime-order subgroup of a non-prime-order
Edwards curve which has a co-factor of 4 or 8. The subgroup formed
under Ristretto is constructed in such a way that it elimates this
co-factor, which is useful to avoid additional checks and also avoids
the risk of leaving in exploitable vulnerabilities which utilize the
co-factor. One such vulnerability was found that affects all CryptoNote
based cryptocurrencies \cite{cryptonote}.  Ristretto elimates all of
these checks and potential vulnerabilities by automatically mapping
all points on the curve directly into the subset of the curve in the
operations themselves. Additionally Ristretto defines equality such that
equivalent representations of the same point are considered equal. In
the same vein the encoding function for points, encodes equivalent
points to the same encoding and by proxy the decoding function decodes
those points to the same point.

For our implementation of bulletproofs, seeing as we needed to test it
against an existing implementation, we made use of the same elliptic
curve, as well as gained an understanding of the basics of elliptic
curve cryptography. The elliptic curve in question is Curve25519,
which is widely used in encryption as is the case here. Curve25519,
is defined by the following formula:

$$y^2 = x^3 + 48662x^2 + x$$

This curve is defined over the prime field $p = 2^{255} - 19$ and base
point defined at $x = 9$ with the positive $y$ value. This results
in a sub-group of order $2^{252} + 2774231777737235353585$, which has
a co-factor of $8$, thus meeting the criteria for using Ristretto to
eliminate this co-factor.

Each internal point representation is composed of four field elements
$(X : Y : Z : T)$. Field-elements, as defined by the standard are
values modulo $p$, with $p$ being the prime field for Curve25519,
$2^{255} - 19$. This was achieved using the \texttt{public\_nat\_mod!}
macro which defines finite fields over a given modulo. Conveniently it
accepts hex values as modulo values, which allowed us to easily make
the modulo value $2^{255} - 19$. Most of the standard integer operators
like addition, subtraction and multiplication are also implemented for
these field elements. During testing we discovered that the internal
calculations for division is done using integer arithmetic rather than
finite field arithmetic, discussion with the Hacspec team confirmed
that this was an oversight and we raised an issue. This was fortunately
not a problem for us as the few times where division is used directly
it is ensured that the result in finite field arithmetic is equal to
the integer arithmetic solution.

Additionally the standard, specified a series of constants. These
constants were too large to be implemented as integers and hacspec
did not allow for the use \texttt{from\_hex()} method. As such, after
converting each of these constants into their corresponding hex-values,
we built them as byte sequences for which we had an equivalent
\texttt{from\_byte\_seq\_be} which created the correct constants. This
worked as intended but invevitably reduced readability of the code.

While it is impossible for a legal point to be encoded and then have the
decoding on its encoding fail, the decoding method has several checks
that ensure that the input given is legal. The reason for this
is that decoding is a method available to the client and as such there
is no guarantee that the byte-strings they want to decode are legal
points that they first encoded properly, but could be artificially
constructed byte-strings. We want to avoid this and thus the standard
has a list of properties the byte-string input must satisfy for the
decoding to be canonical and thus give a proper point as a result.

The IETF-standard was used for the implementation of nearly every
operation implemented, however, scalar multiplication was not specified,
however we used the specification from \cite{elliptic-curves}. This
proved to work perfectly, with the given tests and the IETF-standard
methods.

\subsubsection{Hacspec related problems:}

Due to the need for comparing points scalars had to be implemented using \texttt{public\_nat\_mod!} rather than the more secure \texttt{nat\_mod!} which has nearly identical functionality except over secret integers. However secret integers cannot be compared, something that was required by the IETF-standard.

Additionally we ran into two issues with using \texttt{public\_nat\_mod!}. The first was one of the checks in decode:

\begin{lstlisting}
	pub fn decode(u: RistrettoPointEncoded) -> DecodeResult {
    let mut ret = DecodeResult::Err(DECODING_ERROR);

    let s = FieldElement::from_byte_seq_le(u);

    if !geq_p(u.to_le_bytes()) && !is_negative(s)

	[...]

	fn geq_p(x: Seq<U8>) -> bool {
    let p_seq = byte_seq!([....]);
    let mut res = true;

    for index in 0..p_seq.len() {
        let x_index = x[index].declassify();
        let p_index = p_seq[index].declassify();
        if x_index != p_index {
            res = x_index > p_index;
        }
    }
    res
}
\end{lstlisting}

This check involves checking to see if the inputted byte sequence, if interpreted as a field element is larger than $p$. This is naturally not possible to do directly given that all field elements in our implementation are integers mod $p$. As such it is done in this manner to circumvent this issue.

Another more significant issue we encountered was in the implementation of the one-way map, in which, we must mod the 255 least significant bits of both halves of a ByteString object. However the two halves of the ByteString are each 256 bits. This means that the most significant bit must be set to 0. in the code this is as follows:

\begin{lstlisting}
	pub fn one_way_map(b: ByteString) -> RistrettoPoint {
    let r0_bytes = b.slice(0, 32);
    let r1_bytes = b.slice(32, 32);

    let mut r0_bytes = r0_bytes.declassify();
    let mut r1_bytes = r1_bytes.declassify();

    // The specification states:
    // Set r0 to the low 255 bits of b[ 0..32], taken mod p
    // Set r1 to the low 255 bits of b[32..64], taken mod p
    // Note the low 255 bits. NOT 256 bits! This is why we mod the most significant byte
    r0_bytes[31] = r0_bytes[31] % 128u8;
    r1_bytes[31] = r1_bytes[31] % 128u8;

    let r0 = FieldElement::from_public_byte_seq_le(r0_bytes);
    let r1 = FieldElement::from_public_byte_seq_le(r1_bytes);

	[...]
\end{lstlisting}

This interaction was tested thoroughly to ensure it functions as we intended it to. 

\subsection{Implementing Merlin:} \label{implementing-merlin}

Merlin \cite{merlin} is for the most part a wrapper over the Strobe protocol
\cite{strobe}. It performs the Fiat-Shamir Heuristic by keeping
a running hash of the transcript. Importantly, it does this in a way so algorithms implemented using it will appear to interact with a verifier, while remaining non-interactive. It achieves this by only allowing he algorithm to commit or receive challenge scalars from the transcript. This allows for simpler code that is less error prone than if the Fiat-Shamir Heuristic were done without it.

Most of the work was implementing strobe. There were no great
difficulties in translating most functions, except that the
\texttt{Strobe} type could not be mutably modified. Therefore, in our
implementation we return a strobe object in each function instead
of taking one as argument and modifying it. Since all operations needed
were supported by the Secret Integer hacspec type, we naturally used
this over the less secure Public Integers.

Merlin uses Keccakf1600 for its hashing, fortunately this cryptographic
function was already defined in another hacspec library. We ran into issues however, since Keccakf1600 is defined for 64
bit integers and strobe operates on bytes. Therefore we had to transmute
the \texttt{state} object, from a list of bytes into a shorter list
of 64 bit integers. In the Dalek Rust Merlin implementation this was simple:

\begin{lstlisting}
fn transmute_state(st: &mut AlignedKeccakState) -> &mut [u64; 25] {
    unsafe { &mut *(st as *mut AlignedKeccakState as *mut [u64; 25]) }
}
\end{lstlisting}

But in our implementation we instead do the following: 

\begin{lstlisting}
// Turns a stateU8 into a StateU64
fn transmute_state_to_u64(state: StateU8) -> StateU64 {
    let mut new_state = StateU64::new();

    for i in 0..new_state.len() {
        let mut word = U64Word::new();
        for j in 0..word.len() {
            word[j] = state[i * 8 + j];
        }
        new_state[i] = U64_from_le_bytes(word);
    }

    new_state
}

// Turns a stateU64 into a StateU8
fn transmute_state_to_u8(state: StateU64) -> StateU8 {
    let mut new_state = StateU8::new();

    for i in 0..state.len() {
        let bytes = state[i].to_le_bytes();
        for j in 0..bytes.len() {
            new_state[i * 8 + j] = bytes[j]
        }
    }

    new_state
}
\end{lstlisting}

This implementation is significantly slower, as expected, but functions
correctly.

\textbf{Testing:}

\subsection{Implementing Inner Product Proof:}

We'll start by presenting the prover and verifier's algorithms as
implemented in pseudocode. Note that we index at 1:

\begin{algorithm}[H]
	\caption{IPP: The Prover's Algorithm}

	\SetKwFunction{len}{.len()}
	\SetKwFunction{assertIsPowerOfTwo}{assert($n$.is\_power\_of\_two())}
	\SetKwFunction{assertLengths}{assert($n = \vec{a}\len = \vec{b}\len = \vec{G}\len = \vec{H}\len$)}
	\SetKwFunction{appendN}{transcript.append($n$)}
	\SetKwFunction{appendL}{transcript.append($L$)}
	\SetKwFunction{appendR}{transcript.append($R$)}
	\SetKwFunction{challenge}{transcript.challenge\_scalar()}

	\KwIn{$\vec{a}\in \Z^n_p, \vec{b} \in \Z^n_p, Q \in \G, \vec{G} \in \G^n, \vec{H} \in\G^n$}

	$n := \vec{a}\len$ \\
	$\assertIsPowerOfTwo$ \\
	$\assertLengths$ \\
	$\appendN$ \\
	\For{$i \in [0 : \log_2(n)]$}{
		$L = \dotp{\veclo{a}}{\vechi{G}} + \dotp{\vechi{b}}{\veclo{H}} + \dotp{\veclo{a}}{\vechi{b}}$ \\
		$R = \dotp{\vechi{a}}{\veclo{G}} + \dotp{\veclo{b}}{\vechi{H}} + \dotp{\vechi{a}}{\veclo{b}}$ \\
		\ \\
		$\appendL$ \\
		$\appendR$ \\
		$u := \challenge$ \\
		\ \\
		$\vec{a} := \veclo{a} \cdot u      + u^{-1} \cdot \vechi{a}$ \\
		$\vec{b} := \veclo{b} \cdot u^{-1} + u      \cdot \vechi{b}$ \\
		$\vec{G} := \veclo{G} \cdot u      + u^{-1} \cdot \vechi{G}$ \\
		$\vec{H} := \veclo{H} \cdot u^{-1} + u      \cdot \vechi{H}$ \\
	}

	\Return{$(a_1, b_1, \vec{L}, \vec{R})$}

\end{algorithm}

\begin{algorithm}[H]
	\caption{IPP: The Verifiers's Algorithm}

	\SetKwFunction{len}{.len()}
	\SetKwFunction{assertIsPowerOfTwo}{assert($n$.is\_power\_of\_two())}
	\SetKwFunction{assertLengths}{assert($n = \vec{a}\len = \vec{b}\len = \vec{G}\len = \vec{H}\len$)}
	\SetKwFunction{appendN}{transcript.append($n$)}
	\SetKwFunction{appendL}{transcript.append($L_i$)}
	\SetKwFunction{appendR}{transcript.append($R_i$)}
	\SetKwFunction{challenge}{transcript.challenge\_scalar()}

	\KwIn{$a \in \Z_p, b \in \Z_p, n \in \Z, P' \in \G, Q \in \G, \vec{G} \in \G^n, \vec{H} \in\G^n$}

	$k := \log_2(n)$ \\
	\For{$i \in [1 : k+1]$}{
		$\appendL$ \\
		$\appendR$ \\
		$u_i := \challenge$ \\
	}
	\For{$i \in [1 : n+1]$}{
		$s_i  := u^{b(i,k)}_k \cdots u^{b(i,1)}_1$ \\
		$s_i' := s_i^{-1}$
	}

	$\opn{Result} := P' \stackrel{?}{=} \dotp{a\vec{s}}{\vec{G}} + \dotp{b\vec{s'}}{\vec{H}} + abQ - \sum^k_{i=1} (L_i u^2_i + u^{-2}_i R_i)$

	\Return{$\opn{Result}$}
\end{algorithm}

We tried to find a way to incorperate the linear algebra library into
our implementation but it turned out to be bothersome. Lines 13-16
are run in a for-loop, but although less efficient, it would be more
clear to have $\vec{a}, \vec{b}, \vec{G}, \vec{H},$ calculated using
built-in linear algebra functions. If we take the actual implementation:

\begin{lstlisting}
for i in 0..n {
    a_L[i] = a_L[i] * u + u_inv * a_R[i];
    b_L[i] = b_L[i] * u_inv + u * b_R[i];
    G_L[i] = add(mul(u_inv, G_L[i]), mul(u, G_R[i]));
    H_L[i] = add(mul(u, H_L[i]), mul(u_inv, H_R[i]));
}
\end{lstlisting}

We could rewrite this with vector operations, but this would not
necessarily mean it's more clear. Take for example

\begin{lstlisting}
a_L = v_add(v_scale(u, a_L), v_scale(u_inv, a_R));
b_L = v_add(v_scale(u_inv, b_L), v_scale(u, b_R));
G_L = v_p_add(v_p_mul(u_inv, G_L), v_p_mul(u, G_R));
H_L = v_p_add(v_p_mul(u, H_L), v_p_mul(u_inv, H_R));
\end{lstlisting}

Where \texttt{v\_} indicates a vector operation and \texttt{p\_}
indicates a point operation. Having this many nested functions impacts
readability, but lets suppose that we could define the multiplicative
and additive operators for types like it would be possible in Rust:

\begin{lstlisting}
a_L = a_L * u     + u_inv * a_R;
b_L = b_L * u_inv + u     * b_R;
G_L = G_L * u_inv + u     * G_R;
H_L = H_L * u     + u_inv * H_R;
\end{lstlisting}

This is indeed more intuitive and closer to the math, but it is not
possible in hacspec due to the absence of structs. Even if this were
solved by introducing limited structs we would still have to deal with
the fact that linear algebra operations are defined over two vectors
of the \textit{same} type.

In our implementation we also have the inputs \texttt{G\_factors}
and \texttt{H\_factors}, which is simply factors applied to $\vec{G}$
and $\vec{H}$ respectively. We have these as the Dalek implementation
also have this input because they make optimizations that makes the
code faster than multiplying them on before running the function. We
chose for our implementation to take them as an input, so that the
two functions take the same arguments, but to apply the factors before
running the loop as it simplifies and shortens the code. Furthermore,
the performance increase, while noticable was not significant compared
to other bottlenecks. The file structure here is notable:

\begin{forest}
  for tree={
    font=\rmfamily,
    grow'=0,
    child anchor=west,
    parent anchor=south,
    anchor=west,
    calign=first,
    edge path={
      \noexpand\path [draw, \forestoption{edge}]
      (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
    },
    before typesetting nodes={
      if n=1
        {insert before={[,phantom]}}
        {}
    },
    fit=band,
    before computing xy={l=15pt},
  }
[\textbf{inner-product-proof}
  [\textbf{src}
    [\textbf{transcript}
      [\texttt{\textit{errors.rs}}]
    ]
    [\texttt{errors.rs}]
    [\texttt{ipp.rs}]
    [\texttt{transcript.rs}]
  ]
  [\textbf{tests}
    [\texttt{test\_prop\_ipp.rs}]
  [\texttt{Cargo.toml}]
  ]
]
\end{forest}

We created a \texttt{transcript.rs} file that implemented wrapper
functions over the Fiat-Shamir mechanism introduced by the Merlin
library. This file needed access to defined errors needed access to
the same errors as the \texttt{transcript.rs} file. Normally in Rust
the solution would be:

\begin{lstlisting}
use crate::errors::*;
\end{lstlisting}

But this is not valid hacspec code. We resorted to doing:

\begin{lstlisting}
mod errors;
use errors::*;
\end{lstlisting}

And creating the \textbf{transcript} folder with a symbolic link
\texttt{\textit{errors.rs}} $\rightarrow$ \texttt{errors.rs}. This lets
us run the tests and avoids code duplication.



\textbf{Testing:}

Given that we 

\subsection{Implementing Bulletproofs:}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetAlgoLined
	\KwIn{$n \in \Z, \vec{v}\in \Z^n_p, \vec{\tv} \in \Z^n_p, B\in\G, \tB\in\G, \vec{G}\in\G^n, \vec{H} \in\G^n$}
	\SetKwFunction{len}{.len()}
	\SetKwFunction{performChecks}{perform\_checks()}
	\SetKwFunction{createDealer}{create\_dealer($B,\tB,\vec{G},\vec{H},n,m$)}
	\SetKwFunction{createParty}{create\_Party($B,\tB,\vec{G},\vec{H},v,\tv,n$)}
	\SetKwFunction{computeAS}{compute\_AS($party\_step1, j$)}
	\SetKwFunction{firstaggregate}{aggregate\_and\_get\_YZ($dealer\_step1, \vec{V}, \vec{A},\vec{S}$)}
	\SetKwFunction{computeTs}{compute\_T1T2($party_j\_step1, \ran{y},\ran{z}$)}
	\SetKwFunction{secondaggregate}{aggregate\_and\_get\_X($dealer\_step2, \vec{T_1}, \vec{T_2}$)}
	\SetKwFunction{computetxB}{compute\_txB\_and\_txB\_tilde($party_j\_step2,\ran{x}$)}
	\SetKwFunction{thirdaggregate}{create\_aggregated\_rangeproof($dealer\_step3,\vec{t(\ran{x})B}, \vec{\vec{\tt(\ran{x})\tB}}$)}
	$\performChecks$\\
	$m := \vec{v}\len$\\
	$dealer\_step1 := \createDealer$ \\
	\For{$j \in [1 : m+1]$}{
		$party_j\_step1, V_j := \createParty$\\
		$A_j, S_j := \computeAS$
	}

	$(dealer\_step2, \ran{y},\ran{z}) := \firstaggregate$\\

	\For{$j \in [1 : m+1]$}{
		$(party_j\_step2, T_{1,j}, T_{2,j}) := \computeTs$
	}

	$(dealer\_step3, \ran{x}) := \secondaggregate$\\

	\For{$j\in [1 : m+1]$}{
		$t(\ran{x})_jB, \tt (\ran{x})_j\tB := \computetxB$ 
	}

	$aggregated\_rangeproof := \thirdaggregate$

	\Return{$(\vec{V}, aggregated\_rangeproof)$}

	\caption{Bulletproof algorithm}
\end{algorithm}

The final part of the project was the implementation of the
Bulletproofs protocol. The code was heavily based on the code we
were testing against as well as the mathematical formulas explained
in \ref{Bulletproofs}. The implementation followed the pseudo-code
format as seen above just like the implementation we test against
did. The implementation ran into a few roadblocks during devealopment,
some relating specifically to Hacspec.

The first big roadblock was the immense length it would have. This
meant that writing any sort of test for the code would require it to
be finished in nearly its entirety. This resulted in many tests being
performed before the code was finished through the simple method of
printing out every value on the screen and checking if they match what
we are testing against, in order to ensure that our code was working
correctly. But this in and of itself was an issue as the library we
test against, naturally does not print all values on the screen. For
this reason we forked their library and inserted the needed print
statements. However yet another issue with this was the fact that their
library has built-in randomness. This meant that our code had no way
to utlize the exact same random values, something that is vital when
testing to see if it works in the same way. Therefore we refactored all
random values into inputs in their library. This in no way changes the
functionality of their code, but instead it allows us to control the
random values it uses, which we can then match to our own, and thus
allowing us to test against it.

To keep our implementation as simple as possible we only implemented
what would be equivalent to the \texttt{prove\_multiple\_with\_rng}
method from the library we test against. We felt this was sufficient
as the \texttt{prove\_multiple}, \texttt{prove\_single\_with\_rng}
and \texttt{prove\_single} methods would all call
\texttt{prove\_multiple\_with\_rng} in some way.

\subsubsection{Hacspec-related problems:}

Outside of a few indexing errors and wrong calculation occationally, the implementation of the methods in a way that worked correctly was not much of an issue outside of how long it took to fully implement. The biggest issue with our implementation itself was making it hacspec compliant. Specifically there were many problems involving \texttt{Seq} objects. Most cumbersome of which were the fact that a \texttt{Seq} could not be a part of a tuple, if said tuple was going to be unwrapped. This resulted in \texttt{prove()} being somewhat bloated with sequences, whenever a \texttt{party} method was going to be performed for each party. Ordinarily we would simply make a single \texttt{Seq} and store the tuple in this sequence, however this was not viable as the tuple would need to be unwrapped later, something Hacspec did not allow, due to some of the elements in these tuples having type \texttt{Seq}. Below is a coding example of how this is handled instead:

\begin{lstlisting}
	let mut bp_genss_party_capacity = Seq::<usize>::new(number_of_parties);
	let mut bp_genss_gens_capacity = Seq::<usize>::new(number_of_parties);
	let mut bp_genss_g_vec = Seq::<Seq<Seq<RistrettoPoint>>>::new(number_of_parties);
	let mut bp_genss_h_vec = Seq::<Seq<Seq<RistrettoPoint>>>::new(number_of_parties);
	let mut pc_genss = Seq::<PedersenGens>::new(number_of_parties);
	let mut Vs = Seq::<RistrettoPointEncoded>::new(number_of_parties);

	for i in 0..number_of_parties {
		let ((new_party_capacity, new_gens_capacity, new_g_vec, new_h_vec),new_pc_gens,new_V) = create_party((party_capacity, gens_capacity, g_vec.clone(), h_vec.clone()),pc_gens, values[i], v_blindings[i],n)?;
		
		bp_genss_party_capacity[i] = new_party_capacity;
		bp_genss_gens_capacity[i] = new_gens_capacity;
		bp_genss_g_vec[i] = new_g_vec;
		bp_genss_h_vec[i] = new_h_vec;
		pc_genss[i] = new_pc_gens;
		Vs[i] = new_V;

	}
\end{lstlisting}

Rather than the much more simple:

\begin{lstlisting}
	let mut parties = Seq::<Party_Awaiting_Position>::new(number_of_parties);

	for i in 0..number_of_parties {
		let new_party = create_party((party_capacity, gens_capacity, g_vec.clone(), h_vec.clone()),pc_gens, values[i], v_blindings[i],n)?;
		parties[i] = new_party;
	}
	\end{lstlisting}

Additionally, while it is not actually much of an issue, Hacspec did not allow us to use iterators, and so we instead have an abundance of for-loops in our code. This results in our code being much slower than the library which we test against, which \textit{did} turn out to be somewhat of a problem when it came to testing correctness. While the code we test against runs in mere seconds, ours takes several minutes to finish the same proof which made the checks bothersome. The approximate difference in run-time for our code, compared to the code we test against is 550-600 times slower depending on the inputs. There are multiple reasons for this, not least of which is that we avoid most of the optimizations that are done in the code we test against to make our code more readable. This speed made testing take quite a lot longer than we would have liked, as running all tests, at the current moment, takes approximately 22 hours to run to completion on our PC with the fastest of the 18 tests taking 5 minutes.

Finally the code we test against uses structs to keep hold of the values for each party and the dealer. This ensures that a potential client cannot accidentally perform the steps out of order, due to the next step being integrated in the struct itself. Hacspec does not allow for the use of structs. As such we instead use simple type-aliases which \textit{could} be forged and done out of order. This, in combination with how slow it is, practically disqulifies it from being put into use for creating bulletproofs properly. However this code is not designed for actual use by clients as a cryptographic algorithm, but rather it is supposed to be used for the purposes of creating proofs about the original code we test against.


\section{Future work:}

In this section we will briefly describe the parts of the project that are currently still being worked on as well as what would be interesting to further work on.

Currently our implementation functions as intended, however it has yet to be properly merged into Hacspec. This is currently still being worked on in collaboration with the Hacspec team. This process is lengthy and can involve refactoring parts of the code to allign with the rest of the Hacspec examples.

After this is completed the work which we intended is finished and thus the opportunity for others to pick up our work arises. The most obvious next step is to use our implementation of the bulletproofs protocol with a Proof Assistant such as Coq to show various aspects about the original Rust implementation, which works identically to our own, only much more effectively. This would in turn improve the chances that the bulletproof protocol has no vulnerabilities. 

Another possible path to pursue is the implementation of various other cryptographic protocols in Hacspec, which can use our implementations of Ristretto and Merlin without the need to implement them once more.


\section{Conclusions on our work:}

\section{Acknowledgements:}

%% TAK TIL FRANZISKUS
%% Lasse?

\section{References}
\printbibliography

\section{Appendix A: Notation:} \label{notation}

A table of notation used throughout this report:

\begin{center}
\begin{tabular}{ c l }
	$a$                         & A scalar \\
	$\vec{a}$                   & A vector \\
	$A$                         & A curve point \\
	$\vec{A}$                   & A vector of curve points \\
	$\ran{a}$                   & A scalar random variable \\
	$\vecran{a}$                & A vector of scalar random variables \\
	$\vecran{A}$                & A vector of random curve points \\
	$\veclo{a}$                 & The first half of vector $\vec{a}$, ($\veclo{a} = [a_{1}, \cdots, a_{n/2}]$) \\
	$\vechi{a}$                 & The second half of vector $\vec{a}$, ($\vechi{a} = [a_{n/2+1}, \cdots, a_{n}]$) \\
	$\vec{a^n}$                 & A vector of scalars which are made up of powers of $a$ i.e. $[1,a,a^2... a^{n-1}]$\\
	$\vec{a} \concat \vec{b}$   & A vector, $\vec{a}$, concatinated with another vector, $\vec{b}$\\
	$\mathbb{A}$                & A set \\
	$\mathbb{A}^n$              & A vector space of dimension $n$ \\ 
	$\mathbb{A}_n$              & A set whose elements are mod $n$ \\ 
	$a(x)$                      & A function mapping integers mod p to integers mod p: $\Z_p \rightarrow \Z_p$ \\
	$\vec{a}(x)$                & A function mapping integers mod p to vectors containing integers mod p: $\Z_p \rightarrow \Z^n_p$ \\
	$\dotp{\vec{a}}{\vec{b}}$   & Dot product of $\vec{a}$ and $\vec{b}$ \\
	$\dotp{\vec{a}}{\vec{A}}$   & The sum of scalar-point products of $\vec{a}$ and $\vec{A}$ ($\dotp{\vec{a}}{\vec{A}} = a_1 A_1 + a_2 A_2 + \cdots + a_n A_n$) \\
\end{tabular}
\end{center}

\end{document}
