\documentclass{article}

% === Dependencies === %

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[backend=biber]{biblatex} % Imports biblatex package
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{parskip} % Make space between subsubsections instead of indent
\usepackage[style=iso]{datetime2} % ISO style dates
\usepackage[utf8]{inputenc}

\addbibresource{sample.bib} % Import the bibliography file

% === Paragraphing === %

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
% display heading, like subsubsection
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\makeatother

% === Theorem Styles === %

\newtheorem{definition}{Definition}[section]

% === Custom Commands === %

\newcommand{\eq}[1]{\begin{alignat*}{20}#1\end{alignat*}}
\newcommand{\eqn}[2]{\begin{equation}\label{#1}\begin{split}#2\end{split}\end{equation}}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\ran}[1]{\mathrm{#1}}
\newcommand{\vecran}[1]{\mathbf{#1}}

\renewcommand{\O}{\mathcal{O}}
\newcommand{\V}{\mathcal{V}}
\renewcommand{\P}{\mathcal{P}}

% Sets
\newcommand{\F}{\mathbb{F}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\Z}{\mathbb{Z}}

% Basepoint
\newcommand{\tB}{\widetilde{B}}
\renewcommand{\tt}{\widetilde{t}}
\newcommand{\tv}{\widetilde{v}}

\newcommand\concat{\mathbin{+\mkern-10mu+}} % concat-symbol
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\opn}[1]{\operatorname{#1}}
\newcommand{\veclo}[1]{\vec{#1_{\opn{lo}}}}
\newcommand{\vechi}[1]{\vec{#1_{\opn{hi}}}}
\newcommand{\vecloh}[2]{\vec{#1^{\textit{#2}}_{\opn{lo}}}}
\newcommand{\vechih}[2]{\vec{#1^{\textit{#2}}_{\opn{hi}}}}
\newcommand{\vecl}[1]{\vec{#1_{\opn{L}}}}
\newcommand{\vecr}[1]{\vec{#1_{\opn{R}}}}

% === Header === %

\title{Property based testing of Rust Bulletproofs using Hacspec}
\author{ 
Rasmus Kirk Jakobsen -- 201907084\\
Anders Wibrand Larsen -- 201906147\\
\textbf{Advisor:} Bas Spitters
}

\date{\today}

\begin{document}

\maketitle

\begin{center}
    Bachelor report (15 ECTS) in Computer Science\\
Department of Computer Science, Aarhus University\\
\end{center} 

\begin{center}
	\includegraphics[scale=0.4]{img/bulletproof-hacspec-2.png}
\end{center} 

\subsection*{Abstract:}

\tableofcontents

\newpage

% === Body === %

\section{Introduction:} \label{Introduction}

\section{Prerequisites:}

In this section we will be going over the necessary background knowledge
to understand the results of the work in this project.

\subsection{Finite Field Arithmetic:} \label{Finite Field Arithmetic}

We start with \textit{fields} and the operations defined within
them, as the operations we will later define that work on elliptic
curves are built on fields.

\begin{definition}[Field]
	A field is a set $\F$, along with the \textit{addition} and
	\textit{multiplication} operations. These two operations must
	uphold the so called \textit{field axioms}:

	\begin{itemize}
		\item Associativity of addition and multiplication:
		$\forall a,b,c \in \F : a + (b + c) = (a + b) + c \land a \cdot (b \cdot c) = (a \cdot b) \cdot c$
		\item Commutativity of addition and multiplication:
		$\forall a,b \in \F : a+b=b+a \land a \cdot b = b \cdot a$
		\item Additive and multiplicative identity:
		$\exists 0,1 \in \F : a + 0 = a \land a \cdot 1 = a$
		\item Additive inverses:
		$\forall a \in \F,\ \exists {-a} \in \F : a + ({-a}) = 0$
		\item Multiplicative inverses:
		$\forall a \neq 0 \in \F,\  \exists a^{-1} \in \F : a \cdot a^{-1} = 1$
		\item Distributivity over addition:
		$\forall a,b,c \in \F : a \cdot (b + c) = (a \cdot b) + (a \cdot c)$
	\end{itemize}
\end{definition}

Note that this also means that subtraction and division is defined,
due to the existance of the additive and multiplicative inverses
respectively:

\eq{
	a-b         &= a + (-b) \\
	\frac{a}{b} &= a \cdot b^{-1}
}

Corrolarily, this leads us to \textit{finite fields}:

\begin{definition}[Finite Field]
	A finite field, is a field that contains a finite number of elements.
\end{definition}

One of the most commonly used types of finite fields, and those used
throughout this report are \textit{Prime Fields:} 

\begin{definition}[Prime Field]
	A prime field $\F_p$ is a finite field with elements $[0,p-1]$
	where each operation is performed over integers modulo $p$ with $p$ being a prime number.
\end{definition}

Something important to note about this definition is that inverses in
this kind of field are also positive whole numbers. This will come up
again in section \ref{implementing-ristretto} with regards to the code.

\subsection{Elliptic Curve:}\label{elliptic-curves}

To start our explanation of elliptic curves we add a few definitions:

\begin{definition}[Elliptic Curves]
	An elliptic curve, $E$, is an algebraic curve defined over a prime field, $\F_p$, defined by the formula:
	$$y^2 = x^3 + ax + b$$
\end{definition}

\begin{definition}[EC Additive Identity]
	Let $\O$ be the point where $y = \infty$. For every point, $P$,
	on $E$ the following property holds:
	$P + \O = \O + P = P$.
\end{definition}

\begin{definition}[EC Negation]
	Let $P = (x,y)$ be a point on $E$. The negation of $P$, called $-P$
	is defined as the mirror of P over the x-axis, or $-P = (x,-y)$.
\end{definition}

\begin{definition}[EC Addition]
	Let $\ell$ be a line intersecting $E$ at three points, $P$, $Q$ and
	$-R$. The group operation: Addition defined on two points is defined
	as:
	$$P + Q = {-R}$$
\end{definition}

\begin{definition}[EC Doubling]
	let $\ell$ be the tangent to the point $P$ on $E$, which intersects
	points $P$ and $-R$. The group operation: point doubling on P is
	defined as:
	$$2P = P + P = R$$
\end{definition}

An important thing to note about point addition. In the case where
you add points $P$ and $Q$ where $\ell$ is a tangent to $P$ then $P +
Q = -P$. This is due to the fact that the \textit{only} case in which
this can happen is when $Q = -(P + P)$ and thus you get $P + Q = P +
(-(P + P)) = -P$. Another similar case is using point doubling on any
point $P = (x,0)$ on $E$. The tangent of $P$ do not intersect
a second point. However note that the negation of $P$ here is $-P =
(x,-0) = (x,0) = P$. Thus point doubling on these points is equivalent
to the equation $P + (-P) = \O$.

From these definitions we can extrapolate two more definitions:

\begin{definition}[EC Subtraction]
	Let $P$ and $Q$ be elliptic curve points. The group operation:
	Subtraction is defined by:
	$$P-Q = P + (-Q)$$
\end{definition}

\begin{definition}[EC Scalar Multiplication]
	Let $m$ be a scalar and let $P$ be a point on $E$. The group operation:
	Scalar multiplication is defined by adding $P$ to itself $m$ times and
	denoted a $m\cdot P$.
\end{definition}

Due to the absence of scalar division it is impossible to multiply by
anything other than integers as we cannot have something akin to $2.5
\cdot P = \frac{5\cdot P}{2}$. Therefore scalars are always integers
when doing computations for elliptic curves. Multiplying by $0$ will
naturally yield the identity element $\mathcal{O}$ by definition.
Additionally multiplying by a negative integer, ${-m}$, is defined as
$-m\cdot P = m\cdot ({-P})$.

The particular curve used for our implementation, as well as the
implementation of the Rust bulletproofs, is a special curve from a subset
of curves known as Montgomery curves. These curves are defined by the
formula: $By^2 = x^3 + Ax^2 + x$. Curves of this form have a birational
equivalence with a different set of curves, known as Twisted Edwards
curves. Our implementation, as well as the implementation we test against
utilize this aspect in their internal representations of points on the
curve.

For our implementation of bulletproofs, seeing as we needed to test it
against an existing implementation we made use of the same elliptic
curve, as well as gained an understanding of the basics of elliptic
curve cryptography. The elliptic curve in question is Curve25519, which
is widely used in encryption as is the case here. Curve25519, is defined by the following formula:

$$y^2 = x^3 + 48662x^2 + x$$

This curve is defined over the prime field $K = 2^{255} - 19$ and base
point defined at $x = 9$ with the positive $y$ value. This results in a
sub-group of order $2^{252} + 2774231777737235353585$, which has a
co-factor of $8$. This co-factor would make cryptocurrencies that use
Curve25519 elliptic curves vulnerable to the so-called 'Double Spending
Vulnerability'. However the implementation known as Ristretto circumvents
this by eliminating the co-factor. For more detail see \ref{ristretto}

\subsection{Pedersen Commitment:}

A Pedersen commitment is a commitment scheme defined by a certain
property we will go into a little later. For our purposes we redefine
the scheme to work with commitments to elliptic curve points rather than
a value. First a single point $B$ on an elliptic curve, is chosen. This point will be the so-called generator. For
Curve25519 we use the point we previously defined as the base point with x =
9 and positive $y$.

\begin{definition}[Pedersen Commitment]
	A Pedersen commitment, to some message, $a$, with a randomly chosen
	integer, $\widetilde{a}$, is defined as:

	$$aB + \widetilde{a}\tB$$

	where $B$ is a canonical generator for the curve and $\tB$
	is a curve point where no one knows $q$ such that $\tB = qB$.
\end{definition}

Here $\widetilde{a}$ serves as our perfect hiding. The construction of $\tB$ ensures perfect hiding as there is a nearly infinite
amount of possible combinations of two points that can add to any given
point. However while that is true, it does not provide perfect binding by
definition of binding and hiding. From here on we will refer to a
commitment to a message, $a$, with random value, $\widetilde{a}$, as $C(a,\widetilde{a})$.

The most important property of Pedersen commitments however is
that pedersen commitments have the property of \textit{Additive
Homomorphism}:

\begin{definition}[Additive Homomorphism]
	For any two Pedersen commitments $C(a_1,\widetilde{a}_1), C(a_2,\widetilde{a}_2)$ we have:
	$$ C(a_1,\widetilde{a}_1) + C(a_2,\widetilde{a}_2) = C(a_1 + a_2, \widetilde{a}_1 + \widetilde{a}_2)$$
\end{definition}

This is simple to show by applying the definition of Pedersen
commitments: 

\eq{
	C(a_1,\widetilde{a}_1) + C(a_2,\widetilde{a}_2) &= \widetilde{a}_1\tB + a_1B  + \widetilde{a}_2\tB + a_2B \\
	                        &= (\widetilde{a}_1 + \widetilde{a}_2)\tB + (a_1 + a_2)B \\
	                        &= C(a_1 + a_2, \widetilde{a}_1 + \widetilde{a}_2)
}

Which ensures a homomorphism between the sum of commitments and the
commitment to the sum.

\subsection{Zero Knowledge Proofs:}\label{zero-knowledge}

A zero-knowledge proof is a cryptographical concept in which a prover,
$\P$, will convince a verifier, $\V$, that a certain property holds,
usually in the form of a mathematical equation, and $\P$ will do so
without $\V$ gaining any additional information. To exemplify this we
will show the simplest example of a Zero-knowledge proof, using the
so-called Schnorr Identity Protocol.

This crux of the Schnorr Identity Protocol is for $\P$ to show that
they know the secret value x. This secret value has the following
relation to the public elliptic curve points $X$ and $G$:

\eq{
	X = xG
}

$\V$ knows both $X$ and $G$ but not $x$. $\P$ wishes to show $\V$
without simply revealing $x$ directly.

The first step $\P$ takes is choosing a random value $k$ and commiting
to this value by sending $K$, such that $K = kG$, to $\V$. $\V$ then
chooses a random value $e$ and sends it to $\P$. Finally $\P$ sends $s$
such that $s = k + ex$. Written out as a transcript this would be:

\eq{
	\P \rightarrow \V &: K \\
	\V \rightarrow \P &: e \\
	\P \rightarrow \V &: s \\
}

From here $\V$ can do a simple check to see if $\P$ truly knows $x$:

\eq{
	sG \stackrel{?}{=} K + eX
}


\textbf{Completeness:}

For a proof to be 'complete' it means that an honest $\P$ will convince
an honest $\V$ that the statement they wish to prove holds. This proof
is complete because of the following:

\eq{
	sG &= K + eX \\
	   &= kG + e(xG) \\
	   &= (k + ex)G \\
	   &= sG
}

\textbf{Soundness:}

For a proof to be sound it means that a \textit{dishonest} $\P$
cannot convince an honest $\V$ that their statement is true. In this
case it means that if $\P$ does not know $x$ but wishes to convince
$\V$ that it does, this is not possible. For this to be the case $\P$
must create a $K$ and $s$ such that $sG = K + eX$ without knowing $x$,
which is used in the creation of $s$. This \textit{could} be done if
$\P$ had knowledge of $e$ by constructing $K$ as follows:

\eq{
	K = s(G - eX)
}

and then choose a random value $s$. If the verifier then did the check
they would find that it holds. However this is NOT a possibility
because $\P$ must commit to a $K$ before they are given $e$. There
is a small probability that $\P$ could randomly guess $e$ correctly,
but this is miniscule.

\textbf{Zero-knowledge:}

For a proof to be zero-knowledge it must hold that $\V$ did not gain any additional information. To see this we look at the transcript and see that $K$ does not include $x$ in its calculations, but $s$ does. However $s$ is blinded by $k$ meaning that $\V$ cannot infer $x$ without knowing $k$. And $\V$ cannot infer $k$ from $K$ any easier than it can infer $x$ from $X$, thus this proof is also zero-knowledge. 

\section{The Bulletproofs protocol} \label{Bulletproofs}

The ultimate goal of this project has been to implement bulletproofs in
Hacspec, using property-based testing to ensure it is equivalent to the
implementation done in Rust, using Curve25519. A bulletproof is a
non-interactive aggregation of rangeproofs using Pedersen commitments.
The following section will describe each of these terms in greater detail
on a theoretic level.

\subsection{Inner Product Proof:}
A prover, $\P$, wants to prove to a verifier, $\V$, that he has knowledge
of two vectors $\vec{a}$, $\vec{b}$ and know their inner product
$\dotp{\vec{a}}{\vec{b}} = c$. The prover could simply send the verifier
$\vec{a}, \vec{b}, c$ but this would take up $O(n)$ bandwidth and
time to verify. Instead we will use a compression technique included
in the bulletproofs paper which allows for just $O(log_2(n))$ bandwidth
and time to verify.

\subsubsection{Provers Algorithm:}
$\P$ is given values with the following definitions:

\eqn{def1}{
	\vec{a}, \vec{b} &\in \Z^n_p \\
	\vec{G}, \vec{H} &\in \G^n \\
}
\eqn{def2}{
	P &= \dotp{\vec{a}}{\vec{G}} + \dotp{\vec{b}}{\vec{H}} \\
	c &= \dotp{\vec{a}}{\vec{b}} \\
}

Here $P$ is similar to a Pedersen Commitment with the crucial difference
that \textit{there is no blinding}. The blinding will be introduced
later on in the range proofs section. (\ref{range-proofs}) We will
introduce a combined form of $P$ and $c$ in the form of $P^{(k)}$,
The parenthesised superscript, denotes that it's a new variable,
it does \textit{not} refer to exponentiation:

\eq{
	P^{(k)} &= \dotp{\vec{a}}{\vec{G}} +
	           \dotp{\vec{b}}{\vec{H}} +
	           \dotp{\vec{a}}{\vec{b}}Q \\
} 

This redefinition will be useful, because it will allow us redefine
$\vec{a}, \vec{b}, \vec{G}, \vec{H}$ ($\vec{a}^{(k)}, \vec{b}^{(k)},
\vec{G}^{(k)}, \vec{H}^{(k)}$) into a shorter format, while keeping the
properties in equation \ref{def2} as invariants.

\eq{
	\vec{a}^{(k-1)} &= \veclo{a^{\text{(k)}}} &&\cdot \ran{u}_k      &&+ \ran{u}^{-1}_k &&\cdot \vechi{a^{\text{(k)}}} \\
	\vec{b}^{(k-1)} &= \veclo{b^{\text{(k)}}} &&\cdot \ran{u}^{-1}_k &&+ \ran{u}_k      &&\cdot \vechi{b^{\text{(k)}}} \\
	\vec{G}^{(k-1)} &= \veclo{G^{\text{(k)}}} &&\cdot \ran{u}^{-1}_k &&+ \ran{u}_k      &&\cdot \vechi{G^{\text{(k)}}} \\
	\vec{H}^{(k-1)} &= \veclo{H^{\text{(k)}}} &&\cdot \ran{u}_k      &&+ \ran{u}^{-1}_k &&\cdot \vechi{H^{\text{(k)}}} \\
}

Notice, that these redefined vectors will have a length half of that
of the original vectors, satisfying our requirement for more compact
vectors. The random variable, $\ran{u_k}$, will be provided by $\V$
to ensure that it is indeed random. From these new vectors we will
define a new compressed $P^{(k-1)}$ and isolate the variable $\ran{u}$:

\eq{
	P^{(k-1)} =
	\dotp{\vec{a}^{(k-1)}}{\vec{G}^{(k-1)}} +
	\dotp{\vec{b}^{(k-1)}}{\vec{H}^{(k-1)}} +
	\dotp{\vec{a}^{(k-1)}}{\vec{b}^{(k-1)}} \cdot Q
}

Notice that $P^{(k-1)}$ is still defined the same way
that we defined $P^{(k)}$, and that the property $c^{(k-1)} =
\dotp{\vec{a}^{(k-1)}}{\vec{b}^{(k-1)}}$ still holds. If we substitute
our defined variables:

\eq{
	P^{(k-1)} = \: \:
	&\dotp
		{        \veclo{a} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{a}}
		{&&\quad \veclo{G} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{G}}
	+ \\
	&\dotp
		{        \veclo{b} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{b}}
		{&&\quad \veclo{H} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{H}}
	+ \\
	&\dotp
		{        \veclo{a} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{a}}
		{&&\quad \veclo{b} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{b}}
	\cdot Q \\
}

And group the terms:

\eq{
	P^{(k-1)} = \: \:
	        &\dotp{\veclo{a}}{\veclo{G}}            +
	         \dotp{\vechi{a}}{\vechi{G}}          &&+
	u^2_k    \dotp{\veclo{a}}{\vechi{G}}          &&+
	u^{-2}_k \dotp{\veclo{a}}{\vechi{G}}            +\\
	        &\dotp{\veclo{b}}{\veclo{H}}            +
	         \dotp{\vechi{b}}{\vechi{H}}          &&+
	u^2_k    \dotp{\veclo{b}}{\vechi{H}}          &&+
	u^{-2}_k \dotp{\veclo{b}}{\vechi{H}}            +\\
	       &(\dotp{\veclo{a}}{\veclo{b}}            +
		       \dotp{\vechi{a}}{\vechi{b}}) \cdot Q &&+
	(u^2_k   \dotp{\veclo{a}}{\veclo{b}}          &&+
	u^{-2}_k \dotp{\vechi{a}}{\vechi{b}}) \cdot Q
}

We can see that this compressed $P^{(k-1)}$ contains $c =
(\dotp{\veclo{a}}{\veclo{b}} + \dotp{\vechi{a}}{\vechi{b}})$. We can
simplify further by defining variables $L_k$ and $R_k$:

\eq{
	&P^{(k-1)} &&= P_k + u^2_k \cdot L_k + u^{-2}_k \cdot R_k \\
	&\textbf{where:} \\
	&L_k     &&= \dotp{\veclo{a^{\text{(k)}}}}{\vechi{G^{\text{(k)}}}} +
	             \dotp{\vechi{b^{\text{(k)}}}}{\veclo{H^{\text{(k)}}}} + 
	             \dotp{\veclo{a^{\text{(k)}}}}{\vechi{b^{\text{(k)}}}} \cdot Q \\
	&R_k     &&= \dotp{\vechi{a^{\text{(k)}}}}{\veclo{G^{\text{(k)}}}} +
	             \dotp{\veclo{b^{\text{(k)}}}}{\vechi{H^{\text{(k)}}}} +
	             \dotp{\vechi{a^{\text{(k)}}}}{\veclo{b^{\text{(k)}}}} \cdot Q \\
}

So, $\P$ calculates $L_k, R_k$ and sends it to $\V$. Afterwards,
$\V$ responds with $u_k$. This is repeated $k = log_2(n)$ times,
until we get the final $P$, $P^{(0)}$:

\eq{
	P^{(0)} &= a^{(0)}_0 G^{(0)}_0 + b^{(0)}_0 H^{(0)}_0 + a^{(0)}_0 b^{(0)}_0 Q \\
	P^{(0)} &= P^{(k)} + \sum^k_{i=1}(L_i u^2_i + u^{-2}_i R_i)
}

Finally $\P$ sends $(a^{(0)}, b^{(0)})$ to $\V$.

The entirety of the dialogue between $\V$ and $\P$,
the so called \textit{transcript}, can be summarized as the following:

\eq{
	\P \rightarrow \V &: L_k, R_k \\
	\V \rightarrow \P &: u_k \\[5pt]
	\P \rightarrow \V &: L_{k-1}, R_{k-1} \\
	\V \rightarrow \P &: u_{k-1} \\[-5pt]
	                  &\vdots \\
	\P \rightarrow \V &: L_{1}, R_{1} \\
	\V \rightarrow \P &: u_{1} \\[5pt]
	\P \rightarrow \V &: a^{(0)}, b^{(0)} \\
}

To make the inner product proof \textit{non-interactive}, the
Fiat-Shamir Heuristic (\ref{fiat-shamir-heuristic}) can be used.

\subsubsection{Verifiers Algorithm}
$\V$ has knowledge of the following values:
\eqn{def1-ver}{
	a^{(0)}, b^{(0)} &\in \Z_p \\
	\vec{L}, \vec{R} &\in \G_p^{k} \\
	\vec{G}, \vec{H} &\in \G^n \\
	P^{(k)}, Q &\in \G \\
	\vec{u} &\in \Z^{k} \\
}

We need a way to get the final $G^{(0)}$ and $H^{(0)}$, from $\vec{G}$
and $\vec{H}$, let's start with the former. We recall our reduced
version of $\vec{G}$:

\eqn{G0}{
	\vec{G}^{(j-1)} = \veclo{G^\textit{(j)}} u^{-1}_j + \vechi{G^\textit{(j)}} u^{1}_j
}

We want to define a vector $\vec{s}$ such that $\dotp{\vec{s}}{\vec{G}}
= G^{(0)}$. From equation \ref{G0} we conclude that each $s_i$ is
defined as:

\eq{
	&s_i = u^{b(i,k)}_k \cdots u^{b(i,1)}_1 \\
	&\textbf{where:} \\
	&b(i,j) = 
	\begin{cases}
		\text{-1} &\quad  g(i,j) = \top \\
		\text{1}  &\quad  g(i,j) = \bot \\
	\end{cases} \\
	&g(i,j) = 
	\begin{cases}
		\top &\quad  \text{if $(i$ mod $2^j) <    2^{j-1}$} \\
		\bot &\quad  \text{if $(i$ mod $2^j) \geq 2^{j-1}$} \\
	\end{cases}
}

We define $b(i,j)$\footnote{Note that in the code we zero-index
so $j_{\text{code}} = j+1$.} to be -1 if $G_i$ appears in the left
half of $\vec{G}^{(j)}$, and 1 if $G_i$ appears in the right half
of $\vec{G}^{(j)}$. Therefore $G_i$ is in the right side if
$(i$ mod $2^j) < 2^{j-1}$.

We make a similar argument for $H^{(0)}$. We want to define a vector
$\vec{s'}$ such that $\dotp{\vec{s'}}{\vec{H}} = H^{(0)}$:
\eqn{H0}{
	\vec{H}^{(k-1)} = \veclo{H^\textit{(k)}} u^{1}_j + \vechi{H^\textit{(j)}} u^{-1}_j
}

To construct $\vec{s'}$:

\eq{
	&s_i' = u^{b'(i,k)}_k \cdots u^{b'(i,1)}_1 \\
	&\textbf{where:} \\
	&b'(i,j) = 
	\begin{cases}
		\text{-1} &\quad  \lnot g(i,j) = \top \\
		\text{1}  &\quad  \lnot g(i,j) = \bot \\
	\end{cases} \\
}

But this is indeed just:

\eq{
	\vec{s'} = \frac{1}{s_1}, \frac{1}{s_2}, \cdots \frac{1}{s_n}
}

Leading us to our desired $G^{(0)}, H^{(0)}$:

\eq{
	G^{(0)} &= \dotp{\vec{s}}{\vec{G}} \\
	H^{(0)} &= \dotp{\vec{s'}}{\vec{H}}
}

Now $\V$ can simply check if $P^{(k)} \stackrel{?}{=} P^{\V}$,
if the former statement is true, then the $\V$ concludes that
$\dotp{\vec{a}}{\vec{b}} = c$:

\eq{
	P^{(k)} &\stackrel{?}{=} P^{\V} \\
	        &\stackrel{?}{=} a^{(0)}G^{(0)} + b^{(0)}H^{(0)} + a^{(0)}b^{(0)}Q - \sum^k_{i=1} (L_i u^2_i + u^{-2}_i R_i) \\
	        &\stackrel{?}{=} \dotp{a\vec{s}}{\vec{G}} + \dotp{b\vec{s'}}{\vec{H}} + abQ - \sum^k_{i=1} (L_i u^2_i + u^{-2}_i R_i)
}

\subsubsection{The Algorithm}

\subsection{Range Proofs:}\label{range-proofs}

A range-proof is a zero-knowledge proof which seeks to prove that for some value $v$, $v$ lies within the range of $[0,2^n)$ without revealing $v$ or any additional information about $v$, thus zero-knowledge. We wish to express this property of $v$ in a single inner product to be able to use an inner product proof as described above. 

\subsubsection{Prover's Algorithm}\label{prover-range-proofs}

To start, the prover, $\P$, is given the following values:

\eqn{def1}{
	v &\in \Z_p \\
	n &\in \Z\\
	B, \tB &\in \G\\
	\vec{G}, \vec{H} &\in \G^n \\
}

$\P$ wishes to convince the verifer, $\V$ that the value, $v$ lies
within the range $[0,2^n)$.

The first step towards this is for $\P$ to commit to $v$ using
a Pedersen commitment $V$, and sending this to the $\V$, as well
commiting to the desired range $n$, which is not blinded. With the
Pedersen-commitment having base-point $B$ and blinding point $\tB$:

\eq{
	V = vB + \tv \tB
}

If we let $\vec{a}$ be $v$ expressed in bits then we know the following:

\eq{
	\dotp{\vec{a}}{\vec{2^n}} = v
}

Additionally, as we also need a guarantee that $\vec{a}$ consists
entirely of bits i.e. $\vec{a} \in \{0,1\}^n$. This can be
proven by the following property:

\eq{\vec{a}\circ (\vec{a} - \vec{1}) = \vec{0}}

This property will only hold if $\vec{a}$ has entries that are either
0 or 1. Due to needing to commit to both $\vec{a}$ and $\vec{a}
- \vec{1}$, they will henceforth be referred to as $\vecl{a}$
and $\vecr{a}$ respectively and add another property to show the
relationship between these two which gives us the following final
properties:

\eq{
	\dotp{\vecl{a}}{\vec{2^n}} = v \\
	\vecl{a}\circ \vecr{a} = \vec{0} \\
	(\vecl{a} - \vec{1}) - \vecr{a} = \vec{0}
}

We wish to combine these properties into a single inner product which
$\P$ will then prove to $\V$. To do this we observe that $\vec{a}
= \vec{0} \iff \forall y\in\mathbb{Z}: \dotp{\vec{a}}{\vec{y^n}} =
\vec{0}$. So $\P$ lets $\V$ chose a random scalar $\ran{y}$ and using
$\ran{y}$ we get a new set of properties:

\eq{
	\dotp{\vecl{a}}{\vec{2^n}} = v \\
	\dotp{\vecl{a} - \vec{1} - \vecr{a}}{\vecran{y^n}} = 0 \\
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} = 0
}

Next $\P$ lets $\V$ chose another random scalar $\ran{z}$, and using
this we can combine the three properties above to the following single
statement:

\eq{
	\ran{z^2}v = 
	\ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a} - \vec{1} - \vecr{a}}{\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}}
}

With this we have condensed the properties we wish to describe into
a single statement. We wish to rewrite this as a single inner product
where $\vecl{a}$ appears only in the first argument of the inner product
and $\vecr{a}$ appears only in the second argument and all non-secret
terms are factored out.

\subsubsection{The Algorithm}First we 

\eq{	
	\ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a}}{\vecran{y^n}} -
	\ran{z}\dotp{\vec{1}}{\vecran{y^n}} -
	\ran{z}\dotp{\vecr{a}}{\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a}}{\vecran{y^n}} -
	\ran{z}\dotp{\vec{1}}{\vecr{a}\circ\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n}} +
	\dotp{\vecl{a}}{\ran{z}\vecran{y^n}} +
	\dotp{-\ran{z}\vec{1}}{\vecr{a}\circ\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} +
	\dotp{-\ran{z}\vec{1}}{\vecr{a}\circ\vecran{y^n}}
}

For the final step towards combining all the conditions into a single
inner product we add $\dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} +
\ran{z}\vecran{y^n}}$ to both sides and simplify again:

\eq{
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n}}
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} +
	\dotp{-z\vec{1}}{\vecr{a}\circ\vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n}} \\ 
	&\ran{z^2}v + (\ran{z} - \ran{z^2})\dotp{\vec{1}}{\vecran{y^n}} - \ran{z^3}\dotp{\vec{1}}{\vec{2^n}} &&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n}+\ran{z}\vecran{y^n} + \vecr{a}\circ\vecran{y^n}}
	\\
	&\ran{z^2}v + (\ran{z} - \ran{z^2})\dotp{\vec{1}}{\vecran{y^n}} - \ran{z^3}\dotp{\vec{1}}{\vec{2^n}} &&= \dotp{\vecl{a}- \ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}}
}

We define a function $\delta(y,z) = (z - z^2)\dotp{\vec{1}}{\vec{y^n}}
- z^3\dotp{\vec{1}}{\vec{2^n}}$ and finish simplifying:

\eq{
	\ran{z^2}v + \delta(\ran{y},\ran{z}) = \dotp{\vecl{a} - \ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + (\vecr{a} + \ran{z}\vec{1})\circ\vecran{y^n}}
}

From here on we will refer to the first argument of the inner product as $\vec{l_0}$ and the second argument as $\vec{r_0}$:

\eq{
	\ran{z^2}v + \delta(\ran{y},\ran{z}) = \dotp{\vec{l_0}}{\vec{r_0}}
}

If the goal was simply to construct a single inner product which
would prove that $v$ lies in $[0,2^n)$, then $\P$ could simply send
these values to $\V$ and be done. However, we wish for the proof to be
zero-knowledge and therefore this inner product needs to be blinded. To
do this, $\P$ constructs a polynomial $t(x)$. This polynomial will
be constructed using $\vec{l_0}$ and $\vec{r_0}$ in a way that blinds
them, while still allowing $\P$ to prove to $\V$ that $v$ lies in
$[0,2^n)$.

First $\P$ chooses random blinding factors $\vecran{s_L}$ and
$\vecran{s_R}$ such that $\vecl{s},\vecr{s}\in \Z^n_p$. Using these
$\P$ can costruct two vector-function $\vec{l}(x)$ and $\vec{r}(x)$
in the following way:

\eq{
	\vec{l}(x) &= \vec{l_0} + \vec{l_1}x \\ &= (\vecl{a} - \ran{z}\vec{1}) + \vecl{s}x \\ &= (\vecl{a} + \vecl{s}x) - \ran{z}\vec{1}\\&\\
	\vec{r}(x) &= \vec{r_0} + \vec{r_1}x \\ &= (\ran{z^2}\vec{2^n} + (\vecr{a} + \ran{z}\vec{1})\circ\vecran{y^n}) + \vecr{s}x\circ\vecran{y^n} \\ &= ((\vecr{a} + \vecr{s}x) + \ran{z}\vec{1}) \circ \vecran{y^n} + \ran{z}^2\vec{2^n}
}

Now $\P$ can construct $t(x)$ as follows:

\eq{
	t(x) = \dotp{\vec{l}(x)}{\vec{r}(x)}
}

We want to group $x$, so we can rewrite $t(x)$ into a quadratic equation:

\eq{
	&t(x) = t_0 + t_1x + t_2x^2 \\
	&\textbf{where:} \\
	&t_0 = \dotp{\vec{l_0}}{\vec{r_0}}\\
	&t_2 = \dotp{\vec{l_1}}{\vec{r_1}}\\
	&t_1 = \dotp{\vec{l_0}+\vec{l_1}}{\vec{r_0} + \vec{r_1}} - t_0 - t_2
}

Now $\P$ wishes to prove to $\V$ that $t_0$ and $t(x)$ are correctly
constructed. So, $t_0 = \ran{z^2}v + \delta(\ran{y},\ran{z})$, and $t(x)
= \dotp{\vec{l}(x)}{\vec{r}(x)}$ such that, $\vec{l}(x) = \vec{l_0}
+ \vec{l_1}x$, and, $\vec{r}(x) = \vec{r_0} + \vec{r_1}x$.

First $\P$ wants to show that $t_0$ is correctly constructed. $\P$
starts by making commitments to each of the coefficients of $t(x)$. Here
we make note of the fact that $\P$ has already commited $t_0$. Namely,
commitment, $V$, made at the very start, is also a commitment to $t_0$
as per the construction of $t_0$. $\P$ makes commitments to $T_1 =
C(t_1, \tt_1)$ and $T_2 = C(t_2, \tt_2)$. After commiting these values
to $\V$, $\V$ sends back a challenge scalar $\ran{x}$. We know from
the definitions of $V, T_1, T_2$ that these commitments relate in the
following manner:

\eq{
	t(\ran{x})B                   &= \ran{z^2}vB      &&+ \delta(\ran{y},\ran{z})B &&+ \ran{x}t_1B       &&+ \ran{x^2}t_2B \\
	\tt(\ran{x})\tB               &= \ran{z^2}\tv \tB &&+ 0\tB                     &&+ \ran{x} \tt_1 \tB &&+ \ran{x^2} \tt_2 \tB\\
	t(\ran{x})B + \tt(\ran{x})\tB &= \ran{z^2}V       &&+ \delta(\ran{y},\ran{z})B &&+ \ran{x}T_1        &&+ \ran{x^2}T_2
}

The sum of the two top-most coefficients in each 'column' is also
equal to the bottom coefficient of that column. To convince $\V$, $\P$
sends $t(\ran{x})B$ and $\tt(\ran{x})\tB$, evaluated at $\ran{x}$
to $\V$. This is enough to convince $\V$ that $t_0 = \ran{z^2}v +
\delta(\ran{y},\ran{z})$, which is seen in The Verifier Algorithm.
(\ref{verifier-range-proof})

Next $\P$ wishes to convince $\V$ that $\vec{l}(x)$ and
$\vec{r}(x)$ are both constructed correctly and that $t(x) =
\dotp{\vec{l}(x)}{\vec{r}(x)}$. To achieve this $\P$ will use a similar
construction as seen above. We want $\P$ to make commitments that allows
$\V$ to relate $\vec{l}(x)$ and $\vec{r}(x)$ to $\vecl{a}, \vecl{s},
\vecr{a}$ and $\vecr{s}$. It is not that simple however. As seen here:

\eq{
	\vec{r}(x) = \ran{z^2}\vec{2^n} + ((\vecr{a} + \vecr{s}x) + \ran{z}\vec{1})\circ\vecran{y^n}
}

This means we want $\P$ to make commitments to $\vec{y^n}\circ \vecr{a}$ and $\vec{y^n}\circ\vecr{s}$. This is a problem as these commitments need to be made and sent to $\V$ BEFORE $\P$ gets the challenge $\ran{y}$ from $\V$. To fix this issue $\P$ will make a special commitment to $\vecl{a}$ and $\vecr{a}$ as follows:

\eq{
	C(\vecl{a}, \vecr{a}, \widetilde{a}) = \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecr{a}}{\vec{H}} + \widetilde{a}\tB
}

This same construction is used for the commitment to $C(\vecl{s},
\vecr{s}, \widetilde{s})$. Here we notice that:

\eq{
	C(\vecl{a}, \vecr{a}, \widetilde{a}) &= \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecr{a}}{\vec{H}} + \widetilde{a}\tB \\
	&= \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecran{y^n}\circ \vecr{a}}{\vecran{y^{-n}}\circ \vec{H}} + \widetilde{a}\tB
}

We define $\vec{H'} = \vecran{y^{-n}}\circ\vec{H}$, $A = C(\vecl{a},\vecr{a}, \widetilde{a})$ and $S = C(\vecl{s}, \vecr{s}, \widetilde{s})$ and can now construct our argument similarly to what was done in the previous segment:

\eq{
	\dotp{\vec{l}(\ran{x})}{\vec{G}} &= \dotp{\vecl{a}}{\vec{G}} &&+ \ran{x}\dotp{\vecl{s}}{\vec{G}} &&+ \dotp{-\ran{z}\vec{1}}{\vec{G}} \\
	\dotp{\vec{r}(\ran{x})}{\vec{H'}} &= \dotp{\vecr{a}}{\vec{H}} &&+ \ran{x}\dotp{\vecr{s}}{\vec{H}} &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}}\\
	\widetilde{e}\tB &= \widetilde{a}\tB &&+ \ran{x}\widetilde{s}\tB &&+ 0 \tB \\
	\dotp{\vec{l}(\ran{x})}{\vec{G}} + \dotp{\vec{r}(\ran{x})}{\vec{H'}} + \widetilde{e}\tB &= A &&+ \ran{x}S &&+ (\dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}} - \ran{z}\dotp{\vec{1}}{\vec{G}})
}

From here, using the same argument as above $\P$ sends $\widetilde{e}$ to $\V$. This will be enough to convince $\V$ that $\vec{l}(x)$ and $\vec{r}(x)$ are correctly constructed and that $t(x) = \dotp{\vec{l}(x)}{\vec{r}(x)}$ and together with the previous commitments to $\V$, $\P$ will be able to successfully convince $\V$ that $v$ lies in the range $[0, 2^n)$.

In summary the transcript between $\V$ and $\P$, will end up being the following:

\eq{
	\P \rightarrow \V &: V, n, A, S \\
	\V \rightarrow \P &: \ran{y}, \ran{z} \\
	\P \rightarrow \V &: T_0, T_1 \\
	\V \rightarrow \P &: \ran{x} \\
	\P \rightarrow \V &: t(\ran{x})B, \tt(\ran{x})\tB, \widetilde{e} \hspace*{1cm}\text{($t(x)$ evaluated with challenge $\ran{x}$)}\\
}

Just as with the inner product the Fiat-Shamir Heuristic (\ref{fiat-shamir-heuristic}) can be used to make this process non-interactive.

\subsubsection{Verifier's Algorithm:} \label{verifier-range-proof}

$\V$ has access to the following values:

\eqn{def1-ver}{
	\ran{y}, \ran{z}, \ran{x}, \widetilde{e} &\in \Z_p \\
	\vec{L}, \vec{R} &\in \G_p^{k} \\
	\vec{G}, \vec{H}, \vec{H'} &\in \G^n \\
	V, A, B, T_0, T_1, B, \tB &\in \G \\
	t(\ran{x})B, \tt(\ran{x})\tB &\in \G \hspace*{1cm} \text{(evaluated at x)}
}

$\V$ needs to make a check for the two properties that $\P$ want to prove to it.

To check the first property $\V$ simply checks if the following equation holds:

\eq{
	t(\ran{x})B + \tt(\ran{x})\tB \stackrel{?}{=} \ran{z^2} + \delta(\ran{y},\ran{z})B + \ran{x}T_1 + \ran{x^2}T_2
}

And if it does then that will convince $\V$ that $t(x) = \ran{z^2}v + \delta(\ran{y},\ran{z}) + t_1x + t_2\ran{x^2}$ due to the column-sum argument made in the Prover's Algorithm. (\ref{prover-range-proofs})

Similarly to become convinced of the second property $\V$ checks if this equation holds:

\eq{
	P &= -\widetilde{e}\tB &&+ A &&+ \ran{x}S &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}} &&- \ran{z}\dotp{\vec{1}}{\vec{\vec{G}}} \\
	&= -\widetilde{e}\tB &&+ A &&+ \ran{x}S &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}\circ\vecran{y^{-n}}}{\vec{H}} &&- \ran{z}\dotp{\vec{1}}{\vec{G}}
}

From the column-sum argument we can deduce that $P = \dotp{\vec{l}(x)}{\vec{G}} + \dotp{\vec{r}(x)}{\vec{H'}}$ assuming an honest $\P$. Thus $\V$ can use $t(x)$ and $P$ as inputs into the inner product protocol to prove that $ t(x) = \dotp{\vec{l}(x)}{\vec{r}(x)}$. And if this proof is verified then $\V$ is now convinced that $v\in [0,2^n)$ without knowing any other properties of $v$, thus the range-proof is complete.

Additionally by looking at the trancript one can deduce that that the only way for $\P$ to construct $A$ and $V$ such that they can convince $\V$ that v lies in the range, would be if they can correctly guess \textit{all} three challenge scalars, $\ran{y}, \ran{z}$ and $\ran{x}$ in order to 'cheat'. This happens with a probability so small it is entirely neglegable and thus the proof is also sound.

But the proof being sound does not necessarily mean it is zero-knowledge. The proof also needs to be Zero-knowledge. If we look at all the values availible to $\V$ we can see that it has only two of these values have any direct relation to $v$, these being $V$ and $t(\ran{x})B$. It is trivial to see that $\V$ cannot learn anything about $v$ from $V$ without knowledge of $\tv$ which $\V$ does not know. If we look at the definition of $t(\ran{x})B$ we can see that it DOES use v in the calculation directly. However the definition of $t(x)$ uses $t_2$, which is blinded using $\vecran{s_L}$ and $\vecran{s_R}$ neither of which $\V$ has any way to access. Additionally $t_1$ also uses the definition of $t_2$ and thus is ALSO cannot be infered. Thus there is no amount of calculations that $\V$ can do to figure out anything about $v$ and thus the proof is also zero-knowledge.

\subsection{Aggregation of rangeproofs:}

An aggregated range-proof is a rangeproof which proves that one or most likely more values lie within a certain range: $[0,2^n)$ for some integer n. This is done by aggregating range-proofs for each individual value into a single proof which, if verified, proves that each value given lies within the range while giving away no more information about any of the values. This is done by using a multi-party protocol where a 'party' is created for each value that needs to be proven and each party will converse with the so-called 'dealer' who is responsible for collecting the various commitments from the parties and generating challenges which is used by all parties. At the very end the dealer will then collect all the individual rangeproofs for each value and aggregate them. 

The steps used to do this are the same steps used in the construction of a rangeproof with a few additional steps which aggregate the proofs into a single rangeproof. 

To start with a number of parties are created, one for each value that $\P$ wishes to prove. We call this number m. These parties each create the needed values that $\P$ needs to send to $\V$ in order to convince them that each of these values lie within the range, with one key difference. Namely that the so-called dealer is in charge of generating the needed challenges $\ran{y}, \ran{z}$ and $\ran{x}$ and uses an offset for each party to ensure the order matters. This offset is defined as such for the jth party:

\eq{
	z_{(j)} &= z^j * \vec{y^n_{(j)}}\\
	y^n_{(j)} &= y^{n*m}[j*n:(j+1)*n]
}

the evaluation point $\ran{x}$ is the same for all parties. The choice of $\ran{y}, \ran{z}$ and $\ran{x}$ is done where it would ordinarily be done, but only once $\V$ has received the needed values from ALL parties. Once again, this can also be done using the Fiat-Shamir Heuristic. (\ref{fiat-shamir-heuristic}) 

After each party has created $t_{(j)}(x) = \dotp{\vec{l_{(j)}(x)}}{\vec{r_{(j)}(x)}}$ it would be sufficient to perform the proving steps from \ref{range-proofs} for each party. However sending all of the created values to $\V$ will require $\V$ to make $m$ checks to see if all of these values prove that each $v_j$ lies within the range. However $\P$ can do a few more computations to create a singular proof which requires a single check from $\V$ to see that ALL $v$ lie within the range.

To prove that a single $t_{(j)}(x)$ is correct means proving that $\vec{l_{(j)}(x)}$ and $\vec{r_{(j)}(x)}$ are created correctly and that $t_{(j), 0} = \dotp{\vec{l_{(j)}(x)}}{r_{(j)}(x)}$. But $\P$ can combine some of these statements into one single statement as follows: 

\eq{
	t(x) &= \sum^{m-1}_{j = 0} t_{j}(x)\\
	\widetilde{t}(x) &= \sum^{m-1}_{j = 0} \widetilde{t}_{j}(x)
}

Each party sends their $T_{1,(j)}$ and $T_{2,(j)}$, to $\V$ and $\V$ can then compute:

\eq{
	T_1 &= \sum^{m-1}_{j = 0} T_{1,(j)}\\
	T_2 &= \sum^{m-1}_{j = 0} T_{2,(j)}\\
	\delta(y,z) &= \sum^{m-1}_{j = 0} \delta_{(j)}(y,z)
}

and from here $\V$ can will send back the challenge $\ran{x}$ and $\P$ will return $t(\ran{x})B$ and $\tt(\ran{x})\tB$. From here $\V$ can then convince themselves that for ALL $t_{0,(j)}$ are correct by performing the following check: 

\eq{
	t(\ran{x})B + \widetilde{t}(\ran{x})\widetilde{B} \stackrel{?}{=} \ran{z^2}\sum^{m-1}_{j = 0} \ran{z_{(j)}}*V_{(j)} + \delta(\ran{y},\ran{z})B + \ran{x}T_1 + \ran{x^2}T_2
}

we know that $z_{(j)} = z^j$ we can make a small rewrite:

\eq{
	t(\ran{x})B + \widetilde{t}(\ran{x})\widetilde{B} \stackrel{?}{=} \sum^{m-1}_{j = 0} \ran{z^{j+2}}*V_{(j)} + \delta(\ran{y},\ran{z})B + \ran{x}T_1 + \ran{x^2}T_2
}

And this check will then convince the verifier that all $t_{0,(j)}$ are correct.

Just as with a singular rangeproof $\P$ also wishes to prove that each $\vec{l_{(j)}(x)}$ and $\vec{r_{(j)}(x)}$ are constructed correctly for all parties. This is done in a similar manner as proving $t_{0,(j)}$ are all correct. Proving this property the jth party would be proving the following:

\eq{
	\dotp{l_{(j)}(\ran{x})}{\vec{G_{(j)}}} + \dotp{r_{(j)}(\ran{x})}{\vec{H'_{(j)}}} \stackrel{?}{=} -\widetilde{e}_{(j)}\widetilde{B} + A_{(j)} + \ran{x}S_{(j)} &&+ (\dotp{\ran{z}\vecran{y^n_{(j)}} + \ran{z^2}\ran{z_{(j)}}\vec{2^n}}{H'_{(j)}} - \ran{z}\dotp{\vec{1}}{\vec{G_{(j)}}})
}

$\P$ can then combine the variables in the following way:

\eq{
	\vec{l}(x) &= \vec{l_0}(x) &&\concat \vec{l_1}(x) &&\concat \dots &&\concat \vec{l_{m-1}}(x)\\
	\vec{r}(x) &= \vec{r_0}(x) &&\concat \vec{r_1}(x) &&\concat \dots &&\concat \vec{r_{m-1}}(x)\\
	\vec{G} &= \vec{G_0} &&\concat \vec{G_1} &&\concat \dots &&\concat \vec{G_{m-1}}\\
	\vec{H'} &= \vec{H'_0} &&\concat \vec{H'_1} &&\concat \dots &&\concat \vec{H'_{m-1}}
}

And send $\dotp{\vec{l}(\ran{x})}{\vec{G}}$ and $\dotp{\vec{r}(\ran{x})}{\vec{H'}}$ to $\V$, along with each party's $\widetilde{e}_{(j)}$. From here $\V$ can then construct:

\eq{
	\vecran{y^n_{(j)}} &= \ran{y^{n*m}}[j*n : (j+1)*n]\\
	\ran{z_{(j)}} &= \ran{z^j}\\
	\widetilde{e} &= \sum^{m-1}_{j = 0} \widetilde{e}_{(j)}\\
	A &= \sum^{m-1}_{j = 0} A_{(j)}\\
	S &= \sum^{m-1}_{j = 0} S_{(j)}
}

Note that is has already recieved $A_{(j)}$ and $S_{(j)}$ from the first step of the transcript. $\V$ can then use the following check to convince itself that all $\vec{l_{(j)}}(x)$ and $\vec{r_{(j)}}(x)$ are correctly constructed:

\eq{
	\vec{l_{(j)}}(\ran{x}) + \vec{r_{(j)}}(\ran{x}) \stackrel{?}{=} -\widetilde{e}\widetilde{B} + A + \ran{x}S - \ran{z}\dotp{\vec{1}}{\vec{G}} + \ran{z}\dotp{\vecran{y^{n*m}}}{\vec{H'}} + \sum^{m-1}_{j = 0}\dotp{\ran{z^{j+2}}*\vec{2^n}}{\vec{H'}[j*n: (j+1)*n]}
}

And if this holds then $\V$ is convinced that $\vec{l_{(j)}}(x)$ and $\vec{r_{(j)}}(x)$ are correctly constructed and with the previous part on top it is now convinced that all the given v are within the range $[0,2^n)$ and thus the proof is sound. Additionally, using the same argument as in \ref{verifier-range-proof} for each individual party, this is also zero-knowledge.

\subsubsection{The Algorithm}

\subsection{Fiat-Shamir Heuristic}\label{fiat-shamir-heuristic}

\section{Our Contributions:}

\subsection{Hacspec:} \label{Hacspec}

Hacspec is a subset of the programming language of Rust, designed in a
way that makes it easy to compile into theorem solvers such as Coq or F*.
The cost for having this property is a reduction in certain conventions
available in Rust not being present in Hacspec. This is a double edged
sword, both making the language simple, but also making it harder to
express certain abstract ideas. The language however is still in
development, balancing and adding features to the language.

All code written for this project was written to be Hacspec
compliant.  Our implementation is a specification meant to
be simply understood compared to a more obfuscated, but highly
optimized implementation. We will use property based testing with
QuickCheck\footnote{\url{https://github.com/BurntSushi/quickcheck}}
to check our Bulletproofs implementation against the Dalek-Cryptography
Bulletproofs\footnote{\url{https://github.com/dalek-cryptography/bulletproofs}}.
We have also created a minimal linear algebra library, in hacspec,
since it was needed for the bulletproofs implementation. Property
based testing using QuickCheck is also used for this library, testing
it against the nalgebra\footnote{\url{https://nalgebra.org/}}.

At a later time our implementation could to be compiled to Coq or F* and
proof-checked, leading to better guarantees about our implementation.
This is however not in the scope of this paper and is left to future
work. % mere her

\subsection{Linear Algebra Library Specification:}
\begin{itemize}
	\item Vectors vs Matrices 
	\item Describe formulas, with sources (Dot product) 
	\item Generics :(
	\item Cloning
	\item Double indexing
	\item QuickCheck
\end{itemize} 

It was decided that the specification should of course consist of what we
need, but also of some general linear algebra functions that could be
used for others who might want to use it. The following functions was
decided to be part of the specification:

\begin{itemize}
	\item Instantiate matrix
	\item Instantiate zero filled matrix
	\item (Instantiate one filled matrix)
	\item Instantiate identity matrix
	\item Transposition
	\item Slicing
	\item (Scalar Multiplication)
	\item Addition
	\item Subtraction
	\item Hadamard product
	\item Matrix Multiplication
\end{itemize}

No generalized standard for Linear Algebra Specification.

\subsection{Implementing Ristretto:} \label{implementing-ristretto}

For the Ristretto implementation on Curve25519 we used an IETF-standard specification and the paper 
of exactly this, which was used as a guide for our
implementation.\footnote{\url{https://www.ietf.org/archive/id/draft-irtf-cfrg-ristretto255-00.html}}
This specification was very helpful when implemting, as it clearly
defined the types and functionalities needed to implement Ristretto
securely. However there was one operation that needed to be implemented, which did \textit{not} have a standard forumla for its implementation. This method was scalar multiplication. For this we used the specification from (Insert Reference to literature here). This proved to work perfectly, with the given tests and the IETF-standard methods. 

Ristretto is, in essence, a prime-order subgroup of a non-prime-order
Edwards curve which has a co-factor of 4 or 8. The subgroup formed
under Ristretto is constructed in such a way that it elimates this
co-factor, which is useful to avoid additional checks and also
avoids the risk of leaving in exploitable vulnerabilities which
utilize the co-factor. One such vulnerability was found that affects all CryptoNote based cryptocurrencies\footnote{\url{https://www.getmonero.org/2017/05/17/disclosure-of-a-major-bug-in-cryptonote-based-currencies.html}}. Ristretto elimates all of these checks and
potential vulnerabilities by automatically mapping all points on
the curve directly into the subset of the curve in the operations
themselves. Additionally Ristretto defines equality such that equivalent
representations of the same point are considered equal. In the same
vein the encoding function for points, encodes equivalent points to
the same encoding and by proxy the decoding function decodes those
points to the same point.

For our implementation of bulletproofs, seeing as we needed to test it
against an existing implementation, we made use of the same elliptic
curve, as well as gained an understanding of the basics of elliptic
curve cryptography. The elliptic curve in question is Curve25519, which
is widely used in encryption as is the case here. Curve25519, is defined by the following formula:

$$y^2 = x^3 + 48662x^2 + x$$

This curve is defined over the prime field $p = 2^{255} - 19$ and base
point defined at $x = 9$ with the positive $y$ value. This results in a
sub-group of order $2^{252} + 2774231777737235353585$, which has a
co-factor of $8$, thus meeting the criteria for using Ristretto to eliminate this co-factor.

Each internal point representation is composed of four field elements
$(X : Y : Z : T)$. Field-elements, as defined by the standard are
values modulo $p$, with $p$ being the prime field for Curve25519,
$2^{255} - 19$. This was achieved using the \texttt{public\_nat\_mod!}
macro which defines finite fields over a given modulo. Conveniently it
accepts hex values as modulo values, which allowed us to easily make
the modulo value $2^{255} - 19$. Most of the standard integer operators
like addition, subtraction and multiplication are also implemented for
these field elements. During testing we discovered that the internal
calculations for division is done using integer arithmetic rather than
finite field arithmetic, discussion with the Hacspec team confirmed
that this was an oversight and we raised an issue. This was fortunately
not a problem for us as the few times where division is used directly
it is ensured that the result in finite field arithmetic is equal to
the integer arithmetic solution.

Additionally the standard, specified a series of constants. These
constants were too large to be implemented as integers and hacspec
did not allow for the use \texttt{from\_hex()} method. As such, after
converting each of these constants into their corresponding hex-values,
we built them as byte sequences for which we had an equivalent
\texttt{from\_byte\_seq\_be} which created the correct constants. This
worked as intended but invevitably reduced readability of the code.

While it is impossible for a legal point to be encoded and then have the
decoding on its encoding fail, the decoding method has several checks
that ensure that the input given is legal. The reason for this
is that decoding is a method available to the client and as such there
is no guarantee that the byte-strings they want to decode are legal
points that they first encoded properly, but could be artificially
constructed byte-strings. We want to avoid this and thus the standard
has a list of properties the byte-string input must satisfy for the
decoding to be canonical and thus give a proper point as a result.

\subsection{Implementing Inner Product Proof:}

\subsection{Implementing Bulletproofs:}

The final part of the project was the implementation of the Bulletproofs protocol. The code was heavily based on the code we were testing against as well as the mathematical formulas explained in \ref*{Bulletproofs}. This implementation had X major roadblocks.

The first big roadblock was the immense length it would have. This meant that writing any sort of test for the code would require it to be finished in nearly its entirety. This resulted in many tests being performed before the code was finished through the simple method of printing out every value on the screen and checking if they match what we are testing against, in order to ensure that our code was working correctly. But this in and of itself was an issue as the library we test against, naturally does not print all values on the screen. For this reason we forked their library and inserted the needed print statements. However yet another issue with this was the fact that their library has built-in randomness. This meant that our code had no way to utlize the exact same random values, something that is vital when testing to see if it works in the same way. Therefore we refactored all random values into inputs in their library. This in no way changes the functionality of their code, but instead it allows us to control the random values it uses, which we can then match to our own, and thus allowing us to test against it. 

To keep our implementation as simple as possible we only implemented what would be equivalent to the \texttt{prove\_multiple\_with\_rng} method from the library we test against. We felt this was sufficient as the \texttt{prove\_multiple}, \texttt{prove\_single\_with\_rng} and \texttt{prove\_single} methods would all call \texttt{prove\_multiple\_with\_rng} in some way.

Outside of a few indexing errors and wrong calculation occationally, the implementation of the methods in a way that worked correctly was not much of an issue outside of how long it took to fully implement. The biggest issue with our implementation itself was making it hacspec compliant. Specifically there were many problems involving \texttt{Seq} objects. Most cumbersome of which were the fact that a \texttt{Seq} could not be a part of a tuple, if said tuple was going to be unwrapped. This resulted in \texttt{prove()} being somewhat bloated with sequences, whenever a \texttt{party} method was going to be performed for each party. Ordinarily we would simply make a single \texttt{Seq} and store the tuple in this sequence, however this was not viable as the tuple would need to be unwrapped later, something Hacspec did not allow. Additionally there are many places in which \texttt{Seqs} objects contain other \texttt{Seq} objects where, whenever the outer sequence was indexed into the index needed to be cloned, before it could be used. 

Additionally, while it is not actually much of an issue, Hacspec did not allow us to use iterators, and so we instead have an abundance of for-loops in our code. This results in our code being much slower than the library which we test against, which \textit{did} turn out to be somewhat of a problem when it came to testing correctness. While the code we test against could run in mere seconds, ours took several minutes to finish which made the checks bothersome. 

\subsection{Conclusions on our work:}

\section{Future work:}

\section{Acknowledgements:}

%% TAK TIL FRANZISKUS
%% Lasse?

\section{References}
\printbibliography

\section{Appendix A: Notation:} \label{notation}

A table of notation used throughout this report:

\begin{center}
\begin{tabular}{ c l }
	$a$                         & A scalar \\
	$\vec{a}$                   & A vector \\
	$A$                         & A curve point \\
	$\vec{A}$                   & A vector of curve points \\
	$\ran{a}$                   & A scalar random variable \\
	$\vecran{a}$                & A vector of scalar random variables \\
	$\vecran{A}$                & A vector of random curve points \\
	$\veclo{a}$                 & The first half of vector $\vec{a}$, ($\veclo{a} = [a_{1}, \cdots, a_{n/2}]$) \\
	$\vechi{a}$                 & The second half of vector $\vec{a}$, ($\vechi{a} = [a_{n/2+1}, \cdots, a_{n}]$) \\
	$\vec{a^n}$                 & A vector of scalars which are made up of powers of $a$ i.e. $[1,a,a^2... a^{n-1}]$\\
	$\vec{a} \concat \vec{b}$   & A vector, $\vec{a}$, concatinated with another vector, $\vec{b}$\\
	$\mathbb{A}$                & A set \\
	$\mathbb{A}^n$              & A vector space of dimension $n$ \\ 
	$\mathbb{A}_n$              & A set whose elements are mod $n$ \\ 
	$a(x)$                      & A function mapping integers mod p to integers mod p: $\Z_p \rightarrow \Z_p$ \\
	$\vec{a}(x)$                & A function mapping integers mod p to vectors containing integers mod p: $\Z_p \rightarrow \Z^n_p$ \\
	$\dotp{\vec{a}}{\vec{b}}$   & Dot product of $\vec{a}$ and $\vec{b}$ \\
	$\dotp{\vec{a}}{\vec{A}}$   & The sum of scalar-point products of $\vec{a}$ and $\vec{A}$ ($\dotp{\vec{a}}{\vec{A}} = a_1 A_1 + a_2 A_2 + \cdots + a_n A_n$) \\
\end{tabular}
\end{center}

\section{Appendix B: Ristretto:} \label{ristretto}
Ristretto is a specification of elliptic curve cryptography, created for
the specific purpose of eliminating unwanted co-factors. This was done
in order to circumvent the double-spending vulnerability. This is done
using a so-called quotient group. Also appropriately called a Factor
Group, a Quotient Group is a type of group, which takes elements from a
larger group and, using an equivalence relation, maps elements that are
'similar' to the same element in the quotient group, while preserving
most of the group structure. The remaining elements are factored out,
leaving a group with the same operations, but fewer elements. What
this means for Ristretto is that it takes points on an elliptic curve
and eliminates the co-factor by simply mapping them down to their
'equivalent' elements. This is the reason for Ristretto's somewhat
unorthodox method of doing operations, as it must ensure that each
element is computed to the proper element in the quotient group. This is
also done, in part, to allow the user to use points that are NOT in the
quotient group without worry as any computation done with these points
is automatically mapped to their proper element, thus eliminating the
rather computationally expensive operation of checking if a point is
on the proper curve.

Additionally the internal representation of the Montgomery curve we wish
to eliminate the co-factor for, is its equivalent Twisted Edwards Curve,
which is done to more easily factor points from the original curve into
the quotient group. Additionally Ristretto's equality method ensures
that equivalent representations of points on the curve are equal, even
if they are not factored into the quotient group yet. Additionally
from its encoding and decoding methods, equivalent points are encoded
to the same bitstring, and are therefore decoded into their refactored
quotient group element.

The exposed functions that were implemented are the encoding and decoding
functions, the equality function to compare points, the addition function
over points, point negation, and the derived functions from these,
namely point doubling, point subtraction and scalar multiplication. And
the final function is the One Way Map function. This function is not
entirely necessary for our purposes, but having it allows outside users
to more easily generate points from simple byte sequences, and 
also makes generating random points for testing much easier.

\end{document}
