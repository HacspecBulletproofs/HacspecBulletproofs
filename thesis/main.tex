\documentclass{article}

% === Dependencies === %

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[backend=biber]{biblatex} % Imports biblatex package
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{parskip} % Make space between paragraphs instead of indent
\usepackage[style=iso]{datetime2} % ISO style dates
\usepackage[utf8]{inputenc}

\addbibresource{sample.bib} % Import the bibliography file

% === Paragraphing === %

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
% display heading, like subsubsection
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\makeatother

% === Theorem Styles === %

\newtheorem{definition}{Definition}[section]

% === Custom Commands === %

\newcommand{\eq}[1]{\begin{alignat*}{20}#1\end{alignat*}}
\newcommand{\eqn}[2]{\begin{equation}\label{#1}\begin{split}#2\end{split}\end{equation}}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\ran}[1]{\mathrm{#1}}
\newcommand{\vecran}[1]{\mathbf{#1}}

\renewcommand{\O}{\mathcal{O}}
\newcommand{\V}{\mathcal{V}}
\renewcommand{\P}{\mathcal{P}}

% Sets
\newcommand{\F}{\mathbb{F}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\Z}{\mathbb{Z}}

% Basepoint
\newcommand{\tB}{\widetilde{B}}
\renewcommand{\tt}{\widetilde{t}}
\newcommand{\tv}{\widetilde{v}}

\newcommand\concat{\mathbin{+\mkern-10mu+}} % concat-symbol
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\opn}[1]{\operatorname{#1}}
\newcommand{\veclo}[1]{\vec{#1_{\opn{lo}}}}
\newcommand{\vechi}[1]{\vec{#1_{\opn{hi}}}}
\newcommand{\vecloh}[2]{\vec{#1^{\textit{#2}}_{\opn{lo}}}}
\newcommand{\vechih}[2]{\vec{#1^{\textit{#2}}_{\opn{hi}}}}
\newcommand{\vecl}[1]{\vec{#1_{\opn{L}}}}
\newcommand{\vecr}[1]{\vec{#1_{\opn{R}}}}

% === Header === %

\title{Property based testing of Rust Bulletproofs using Hacspec}
\author{ 
Rasmus Kirk Jakobsen -- 201907084\\
Anders Wibrand Larsen -- 201906147\\
\textbf{Advisor:} Bas Spitters
}

\date{\today}

\begin{document}

\maketitle

\begin{center}
    Bachelor report (15 ECTS) in Computer Science\\
Department of Computer Science, Aarhus University\\
\end{center} 

\begin{center}
	\includegraphics[scale=0.4]{img/bulletproof-hacspec-2.png}
\end{center} 

\subsection*{Abstract:}

\tableofcontents

\newpage

% === Body === %

\section{Introduction:} \label{Introduction}

\section{Prerequisites:}

In this section we will be going over the necessary background knowledge
to understand the results of the work in this project.

\subsection{Finite Field Arithmetic:} \label{Finite Field Arithmetic}

We start with \textit{fields} and the operations defined within
them, as the operations we will later define that work on elliptic
curves are built on fields.

\begin{definition}[Field]
	A field is a set $\F$, along with the \textit{addition} and
	\textit{multiplication} operations. These two operations must
	uphold the so called \textit{field axioms}:

	\begin{itemize}
		\item Associativity of addition and multiplication:
		$\forall a,b,c \in \F : a + (b + c) = (a + b) + c \land a \cdot (b \cdot c) = (a \cdot b) \cdot c$
		\item Commutativity of addition and multiplication:
		$\forall a,b \in \F : a+b=b+a \land a \cdot b = b \cdot a$
		\item Additive and multiplicative identity:
		$\exists 0,1 \in \F : a + 0 = a \land a \cdot 1 = a$
		\item Additive inverses:
		$\forall a \in \F,\ \exists {-a} \in \F : a + ({-a}) = 0$
		\item Multiplicative inverses:
		$\forall a \neq 0 \in \F,\  \exists a^{-1} \in \F : a \cdot a^{-1} = 1$
		\item Distributivity over addition:
		$\forall a,b,c \in \F : a \cdot (b + c) = (a \cdot b) + (a \cdot c)$
	\end{itemize}
\end{definition}

Note that this also means that subtraction and division is defined,
due to the existance of the additive and multiplicative inverses
respectively:

\eq{
	a-b         &= a + (-b) \\
	\frac{a}{b} &= a \cdot b^{-1}
}

Corrolarily, this leads us to \textit{finite fields}:

\begin{definition}[Finite Field]
	A finite field, is a field that contains a finite number of elements.
\end{definition}

One of the most commonly used types of finite fields, and those used
throughout this report are \textit{Prime Fields:} 

\begin{definition}[Prime Field]
	A prime field $\F_p$ is a finite field with elements $[0,p-1]$
	where each operation is performed over integers modulo $p$ with $p$ being a prime number.
\end{definition}

Something important to note about this definition is that inverses in
this kind of field are also positive whole numbers. This will come up
again in section \ref{implementing-ristretto} with regards to the code.

\subsection{Elliptic Curve:}\label{elliptic-curves}

To start our explanation of elliptic curves we add a few definitions:

\begin{definition}[Elliptic Curves]
	An elliptic curve, $E$, is an algebraic curve defined over a prime field, $\F_p$, defined by the formula:
	$$y^2 = x^3 + ax + b$$
\end{definition}

\begin{definition}[EC Additive Identity]
	Let $\O$ be the point where $y = \infty$. For every point, $P$,
	on $E$ the following property holds:
	$P + \O = \O + P = P$.
\end{definition}

\begin{definition}[EC Negation]
	Let $P = (x,y)$ be a point on $E$. The negation of $P$, called $-P$
	is defined as the mirror of P over the x-axis, or $-P = (x,-y)$.
\end{definition}

\begin{definition}[EC Addition]
	Let $\ell$ be a line intersecting $E$ at three points, $P$, $Q$ and
	$-R$. The group operation: Addition defined on two points is defined
	as:
	$$P + Q = {-R}$$
\end{definition}

\begin{definition}[EC Doubling]
	let $\ell$ be the tangent to the point $P$ on $E$, which intersects
	points $P$ and $-R$. The group operation: point doubling on P is
	defined as:
	$$2P = P + P = R$$
\end{definition}

An important thing to note about point addition. In the case where
you add points $P$ and $Q$ where $\ell$ is a tangent to $P$ then $P +
Q = -P$. This is due to the fact that the \textit{only} case in which
this can happen is when $Q = -(P + P)$ and thus you get $P + Q = P +
(-(P + P)) = -P$. Another similar case is using point doubling on any
point $P = (x,0)$ on $E$. The tangent of $P$ do not intersect
a second point. However note that the negation of $P$ here is $-P =
(x,-0) = (x,0) = P$. Thus point doubling on these points is equivalent
to the equation $P + (-P) = \O$.

From these definitions we can extrapolate two more definitions:

\begin{definition}[EC Subtraction]
	Let $P$ and $Q$ be elliptic curve points. The group operation:
	Subtraction is defined by:
	$$P-Q = P + (-Q)$$
\end{definition}

\begin{definition}[EC Scalar Multiplication]
	Let $m$ be a scalar and let $P$ be a point on $E$. The group operation:
	Scalar multiplication is defined by adding $P$ to itself $m$ times and
	denoted a $m\cdot P$.
\end{definition}

Due to the absence of scalar division it is impossible to multiply by
anything other than integers as we cannot have something akin to $2.5
\cdot P = \frac{5\cdot P}{2}$. Therefore scalars are always integers
when doing computations for elliptic curves. Multiplying by $0$ will
naturally yield the identity element $\mathcal{O}$ by definition.
Additionally multiplying by a negative integer, ${-m}$, is defined as
$-m\cdot P = m\cdot ({-P})$.

The particular curve used for our implementation, as well as the
implementation of the Rust bulletproofs, is a special curve from a subset
of curves known as Montgomery curves. These curves are defined by the
formula: $By^2 = x^3 + Ax^2 + x$. Curves of this form have a birational
equivalence with a different set of curves, known as Twisted Edwards
curves. Our implementation, as well as the implementation we test against
utilize this aspect in their internal representations of points on the
curve.

For our implementation of bulletproofs, seeing as we needed to test it
against an existing implementation we made use of the same elliptic
curve, as well as gained an understanding of the basics of elliptic
curve cryptography. The elliptic curve in question is Curve25519, which
is widely used in encryption as is the case here. Curve25519, henceforth
shortened to 25519, is defined by the following formula:

$$y^2 = x^3 + 48662x^2 + x$$

This curve is defined over the prime field $K = 2^{255} - 19$ and base
point defined at $x = 9$ with the positive $y$ value. This results in a
sub-group of order $2^{252} + 2774231777737235353585$, which has a
co-factor of $8$. This co-factor would make cryptocurrencies that use
Curve25519 elliptic curves vulnerable to the so-called 'Double Spending
Vulnerability'. However the implementation known as Ristretto circumvents
this by eliminating the co-factor. For more detail see \ref{ristretto}

\subsection{Bulletproofs:}\label{Bulletproofs}

The ultimate goal of this project has been to implement bulletproofs in
Hacspec, using property-based testing to ensure it is equivalent to the
implementation done in Rust, using 25519. A bulletproof is a
non-interactive aggregation of rangeproofs using Pedersen commitments.
The following section will describe each of these terms in greater detail
on a theoretic level.

\subsubsection{Pedersen Commitment:}

A Pedersen commitment is a commitment scheme defined by a certain
property we will go into a little later. For our purposes we redefine
the scheme to work with commitments to elliptic curve points rather than
a value. First a single point $G$ on the chosen elliptic curve, in this
case 25519, is chosen. This point will be the so-called generator. For
25519 we use the point we previously defined as the base point with x =
9 and positive $y$.

Our adaptation of the Pedersen commitment scheme will involve point
addition on the elliptic curve, rather than multiplication. This will
have the desired effect of perfect hiding for Pedersen commitments. The
points that we will add together to hide our commitment works using
some amount of randomness which ensures the hiding property while
also allowing for a decent binding property. Not perfect, as this is
impossible alongside perfect hiding.

\begin{definition}[Pedersen Commitment]
	A Pedersen commitment, to some message, $a$, with a randomly chosen
	integer, $r$, is defined as:

	$$rH + aG$$

	where $G$ is a canonical generator for the curve and $H$
	is a curve point where no one knows $q$ such that $H = qG$.
\end{definition}

Here $r$ serves as our perfect hiding, $G$ is our generator for the
curve and $H$ is another curve point where no one knows $q$ such that
$H = qG$. This ensures perfect hiding as there is a nearly infinite
amount of possible combinations of two points that can add to any given
point. However while that is true, it does not provide perfect binding by
definition of binding and hiding. From here on we will refer to a
commitment to a message, $a$, with random value, $r$, as $C(a,r)$.

The most important property of Pedersen commitments however is
that pedersen commitments have the property of \textit{Additive
Homomorphism}:

\begin{definition}[Additive Homomorphism]
	For any two Pedersen commitments $C(a_1,r_1), C(a_2,r_2)$ we have:
	$$ C(a_1,r_1) + C(a_2,r_2) = C(a_1 + a_2, r_1 + r_2)$$
\end{definition}

This is simple to show by simply applying the definition of Pedersen
commitments: 

\eq{
	C(a_1,r_1) + C(a_2,r_2) &= r_1H + a_1G  + r_2H + a_2G \\
	                        &= (r_1 + r_2)H + (a_1 + a_2)G \\
	                        &= C(a_1+a_2,r_1+r_2)
}

Which ensures a homomorphism between the sum of commitments and the
commitment to the sum. This property will become vital when proving zero
knowledgeness later. 

Expanding on the \textit{Vector Pedersen Commitment}:
\begin{definition}[Vector Pedersen Commitment]
	A Vector Pedersen Commitment, to some vector of messages,
	$\textbf{v}$, with a randomly chosen integer, $r$,
	is defined as:

	$$rH + \textbf{vG}$$

	where $\textbf{G}$ is a vector of canonical generators for
	the curve and $H$ is still a curve point where no one knows
	q such that $H = qG$.
\end{definition}

\subsubsection{Inner Product Proof:}
A prover, $\P$, wants to prove to a verifier, $\V$, that he has knowledge
of two vectors $\vec{a}$, $\vec{b}$ and know their inner product
$\dotp{\vec{a}}{\vec{b}} = c$. The prover could simply send the verifier
$\vec{a}, \vec{b}, c$ but this would take up $O(n)$ bandwidth and
time to verify, instead we will use a compression technique included
in the bulletproofs paper allowing for just $O(log_2(n))$ bandwidth
and time to verify.

\paragraph{Provers Algorithm:}
$\P$ is given values with the following definitions:

\eqn{def1}{
	\vec{a}, \vec{b} &\in \Z^n_p \\
	\vec{G}, \vec{H} &\in \G^n \\
}
\eqn{def2}{
	P &= \dotp{\vec{a}}{\vec{G}} + \dotp{\vec{b}}{\vec{H}} \\
	c &= \dotp{\vec{a}}{\vec{b}} \\
}

Here $P$ is a Pedersen Commitment but crucially \textit{there is no
blinding}, this blinding will be introduced later on in the range
proofs section (\ref{range-proofs}). We will introduce a combined form
of $P$ and $c$ in the form of $P^{(k)}$, The parenthesised superscript,
denotes that it's a new variable, it does \textit{not} refer to exponentiation:

\eq{
	P^{(k)} &= \dotp{\vec{a}}{\vec{G}} +
	           \dotp{\vec{b}}{\vec{H}} +
	           \dotp{\vec{a}}{\vec{b}}Q \\
} 

This redefinition will be useful, because it will allow us redefine
$\vec{a}, \vec{b}, \vec{G}, \vec{H}$ ($\vec{a}^{(k)}, \vec{b}^{(k)},
\vec{G}^{(k)}, \vec{H}^{(k)}$) into a shorter format, while keeping the
properties in equation \ref{def2} as invariants.

\eq{
	\vec{a}^{(k-1)} &= \veclo{a^{\text{(k)}}} &&\cdot \ran{u}_k      &&+ \ran{u}^{-1}_k &&\cdot \vechi{a^{\text{(k)}}} \\
	\vec{b}^{(k-1)} &= \veclo{b^{\text{(k)}}} &&\cdot \ran{u}^{-1}_k &&+ \ran{u}_k      &&\cdot \vechi{b^{\text{(k)}}} \\
	\vec{G}^{(k-1)} &= \veclo{G^{\text{(k)}}} &&\cdot \ran{u}^{-1}_k &&+ \ran{u}_k      &&\cdot \vechi{G^{\text{(k)}}} \\
	\vec{H}^{(k-1)} &= \veclo{H^{\text{(k)}}} &&\cdot \ran{u}_k      &&+ \ran{u}^{-1}_k &&\cdot \vechi{H^{\text{(k)}}} \\
}

Notice, that these redefined vectors will have a length half of that
of the original vectors, satisfying our requirement for more compact
vectors. The random variable, $\ran{u}$, will be provided by $\V$
to ensure that it is indeed random. From these new vectors we will
define a new compressed $P^{(k-1)}$ and isolate the variable $\ran{u}$:

\eq{
	P^{(k-1)} =
	\dotp{\vec{a}^{(k-1)}}{\vec{G}^{(k-1)}} +
	\dotp{\vec{b}^{(k-1)}}{\vec{H}^{(k-1)}} +
	\dotp{\vec{a}^{(k-1)}}{\vec{b}^{(k-1)}} \cdot Q
}

Notice that $P^{(k-1)}$ is still defined the same way
that we defined $P^{(k)}$, and that the property $c^{(k-1)} =
\dotp{\vec{a}^{(k-1)}}{\vec{b}^{(k-1)}}$ still holds. If we substitute
our defined variables:

\eq{
	P^{(k-1)} = \: \:
	&\dotp
		{        \veclo{a} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{a}}
		{&&\quad \veclo{G} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{G}}
	+ \\
	&\dotp
		{        \veclo{b} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{b}}
		{&&\quad \veclo{H} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{H}}
	+ \\
	&\dotp
		{        \veclo{a} &&\cdot u_k      &&+ u_k^{-1} &&\cdot \vechi{a}}
		{&&\quad \veclo{b} &&\cdot u_k^{-1} &&+ u_k      &&\cdot \vechi{b}}
	\cdot Q \\
}

And group the terms:

\eq{
	P^{(k-1)} = \: \:
	        &\dotp{\veclo{a}}{\veclo{G}}            +
	         \dotp{\vechi{a}}{\vechi{G}}          &&+
	u^2_k    \dotp{\veclo{a}}{\vechi{G}}          &&+
	u^{-2}_k \dotp{\veclo{a}}{\vechi{G}}            +\\
	        &\dotp{\veclo{b}}{\veclo{H}}            +
	         \dotp{\vechi{b}}{\vechi{H}}          &&+
	u^2_k    \dotp{\veclo{b}}{\vechi{H}}          &&+
	u^{-2}_k \dotp{\veclo{b}}{\vechi{H}}            +\\
	       &(\dotp{\veclo{a}}{\veclo{b}}            +
		       \dotp{\vechi{a}}{\vechi{b}}) \cdot Q &&+
	(u^2_k   \dotp{\veclo{a}}{\veclo{b}}          &&+
	u^{-2}_k \dotp{\vechi{a}}{\vechi{b}}) \cdot Q
}

We can see that this compressed $P^{(k-1)}$ contains $c = (\dotp{\veclo{a}}{\veclo{b}} + \dotp{\vechi{a}}{\vechi{b}})$. We can simplify further by defining variables $L_k$ and $R_k$: 

\eq{
	&P^{(k-1)} &&= P_k + u^2_k \cdot L_k + u^{-2}_k \cdot R_k \\
	&\textbf{where:} \\
	&L_k     &&= \dotp{\veclo{a^{\text{(k)}}}}{\vechi{G^{\text{(k)}}}} +
	             \dotp{\vechi{b^{\text{(k)}}}}{\veclo{H^{\text{(k)}}}} + 
	             \dotp{\veclo{a^{\text{(k)}}}}{\vechi{b^{\text{(k)}}}} \cdot Q \\
	&R_k     &&= \dotp{\vechi{a^{\text{(k)}}}}{\veclo{G^{\text{(k)}}}} +
	             \dotp{\veclo{b^{\text{(k)}}}}{\vechi{H^{\text{(k)}}}} +
	             \dotp{\vechi{a^{\text{(k)}}}}{\veclo{b^{\text{(k)}}}} \cdot Q \\
}

So, $\P$ calculates $L_k, R_k$ and sends it to $\V$. Afterwards,
$\V$ responds with $u_k$. This is repeated $k = log_2(n)$ times,
until we get the final $P$, $P^{(0)}$:

\eq{
	P^{(0)} &= a^{(0)}_0 G^{(0)}_0 + b^{(0)}_0 H^{(0)}_0 + a^{(0)}_0 b^{(0)}_0 Q \\
	P^{(0)} &= P^{(k)} + \sum^k_{i=1}(L_i u^2_i + u^{-2}_i R_i)
}

$\P$ finally sends $(a^{(0)}, b^{(0)})$ to $\V$.

To summarize, the entirety of the dialogue between $\V$ and $\P$,
the so called \textit{transcript}, will end up being the following:

\eq{
	\P \rightarrow \V &: L_k, R_k \\
	\V \rightarrow \P &: u_k \\[5pt]
	\P \rightarrow \V &: L_{k-1}, R_{k-1} \\
	\V \rightarrow \P &: u_{k-1} \\[-5pt]
	                  &\vdots \\
	\P \rightarrow \V &: L_{1}, R_{1} \\
	\V \rightarrow \P &: u_{1} \\[5pt]
	\P \rightarrow \V &: a^{(0)}, b^{(0)} \\
}

To make the inner product proof \textit{non-interactive}, the Fiat-Shamir Heuristic (\ref{fiat-shamir-heuristic}) can be used.

\paragraph{Verifiers Algorithm}
$\V$ has knowledge of the following values:
\eqn{def1-ver}{
	a^{(0)}, b^{(0)} &\in \Z_p \\
	\vec{L}, \vec{R} &\in \G_p^{k} \\
	\vec{G}, \vec{H} &\in \G^n \\
	P^{(k)}, Q &\in \G \\
	\vec{u} &\in \Z^{k} \\
}

We need a way to get the final $G^{(0)}$ and $H^{(0)}$, from $\vec{G}$
and $\vec{H}$, let's start with the former. We recall our reduced
version of $\vec{G}$:

\eqn{G0}{
	\vec{G}^{(j-1)} = \veclo{G^\textit{(j)}} u^{-1}_j + \vechi{G^\textit{(j)}} u^{1}_j
}

We want to define a vector $\vec{s}$ such that $\dotp{\vec{s}}{\vec{G}}
= G^{(0)}$. From equation \ref{G0} we conclude that each $s_i$ is
defined as:

\eq{
	&s_i = u^{b(i,k)}_k, \cdots, u^{b(i,1)}_1 \\
	&\textbf{where:} \\
	&b(i,j) = 
	\begin{cases}
		\text{-1} &\quad  g(i,j) = \top \\
		\text{1}  &\quad  g(i,j) = \bot \\
	\end{cases} \\
	&g(i,j) = 
	\begin{cases}
		\top &\quad  \text{if $(i$ mod $2^j) <    2^{j-1}$} \\
		\bot &\quad  \text{if $(i$ mod $2^j) \geq 2^{j-1}$} \\
	\end{cases}
}

We define $b(i,j)$\footnote{Note that in the code we zero-index so $j_{\text{code}} = j+1$.} to be -1 if $G_i$ appears in the left half of $\vec{G}^{(j)}$, and 1 if $G_i$ appears in the right half of $\vec{G}^{(j)}$. Because $G_i$ appears in the $(i \text{ mod } 2^j)$-th entry of $\vec{G}^{j}$....

We make a similar argument for $H^{(0)}$. We want to define a vector
$\vec{s'}$ such that $\dotp{\vec{s'}}{\vec{H}} = H^{(0)}$:
\eqn{H0}{
	\vec{H}^{(k-1)} = \veclo{H^\textit{(k)}} u^{1}_j + \vechi{H^\textit{(j)}} u^{-1}_j
}

To construct $s'$:

\eq{
	&s_i' = u^{b'(i,k)}_k \cdots u^{b'(i,1)}_1 \\
	&\textbf{where:} \\
	&b'(i,j) = 
	\begin{cases}
		\text{-1} &\quad  \lnot g(i,j) = \top \\
		\text{1}  &\quad  \lnot g(i,j) = \bot \\
	\end{cases} \\
}

But this is indeed just:

\eq{
	s' = \frac{1}{s'_1}, \frac{1}{s'_2}, \cdots \frac{1}{s'_n}
}

Leading us to our desired $G^{(0)}, H^{(0)}$:

\eq{
	G^{(0)} &= \dotp{\vec{s}}{\vec{G}} \\
	H^{(0)} &= \dotp{\vec{s'}}{\vec{H}}
}

Now $\V$ can simply check if $P^{(k)} \stackrel{?}{=} P^{\V}$,
if the former statement is true, then the $\V$ concludes that
$\dotp{\vec{a}}{\vec{b}} = c$:

\eq{
	P^{(k)} &\stackrel{?}{=} P^{\V} \\
	        &\stackrel{?}{=} a^{(0)}G^{(0)} + b^{(0)}H^{(0)} + a^{(0)}b^{(0)}Q - \sum^k_{i=1} (L_i u^2_i + u^{-2}_i R_i) \\
	        &\stackrel{?}{=} \dotp{a\vec{s}}{\vec{G}} + \dotp{b\vec{s'}}{\vec{H}} + abQ - \sum^k_{i=1} (L_i u^2_i + u^{-2}_i R_i)
}

\subsubsection{Zero Knowledge Proofs:}\label{zero-knowledge}

\subsubsection{Range Proofs:}\label{range-proofs}

\paragraph{Prover's Algorithm}

A range-proof is a zero-knowledge proof which seeks to prove that for some value $v$, $v$ lies within the range of $[0,2^n)$ without revealing $v$ or any additional information about $v$, thus zero-knowledge. We wish to express this property of $v$ in a single inner product to be able to use an inner product proof as described above. 

To start the prover, $\P$, commits a value $v$ using a Pedersen commitment, $V$, to the verifier, $\V$, as well commiting to the desired range n, which is not blinded.

\eq{
	V = vB + \tv \tB
}

If we let $\vec{a}$ be $v$ expressed in bits then we know the following:

\eq{
	\dotp{\vec{a}}{\vec{2^n}} = v
}

However this is not enough as we also need a guarantee that $\vec{a}$ consists entirely of bits i.e. $\forall a_i\in a: a_i\in \{0,1\}$. This can be proven by the following property:

\eq{\vec{a}\circ (\vec{a} - \vec{1}) = \vec{0}}

This property will only hold if $\vec{a}$ has entries that are either 0 or 1. Additionally due to needing to commit to both $\vec{a}$ and $\vec{a} - \vec{1}$ we will rename these to $\vecl{a}$ and $\vecr{a}$ respectively and add another property to show the relationship between these two which gives us the following final properties:

\eq{
	\dotp{\vecl{a}}{\vec{2^n}} = v \\
	\vecl{a}\circ \vecr{a} = \vec{0} \\
	(\vecl{a} - \vec{1}) - \vecr{a} = \vec{0}
}

We wish to combine these properties into a single inner product which $\P$ will then prove to $\V$. To do this we observe that $\vec{a} = \vec{0} \iff \forall y\in\mathbb{Z}: \dotp{\vec{a}}{\vec{y^n}} = \vec{0}$. So $\P$ lets $\V$ chose a random scalar $\ran{y}$ and using $\ran{y}$ we get a new set of properties:

\eq{
	\dotp{\vecl{a}}{\vec{2^n}} = v \\
	\dotp{\vecl{a} - \vec{1} - \vecr{a}}{\vecran{y^n}} = 0 \\
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} = 0
}

Next $\P$ lets $\V$ chose another random scalar $\ran{z}$, and using this we can combine the three properties above to the following single statement: 

\eq{
	\ran{z^2}v = 
	\ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a} - \vec{1} - \vecr{a}}{\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}}
}

With this we have condensed the properties we wish to describe into a single statement. We wish to rewrite this as a single inner product where $\vecl{a}$ appears only in the first argument of the inner product and $\vecr{a}$ appears only in the second argument and all non-secret terms are factored out.

First we can rewrite the middle term:

\eq{	
	&\ran{z^2}v &&= 
	\ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a}}{\vecran{y^n}} -
	\ran{z}\dotp{\vec{1}}{\vecran{y^n}} -
	\ran{z}\dotp{\vecr{a}}{\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \ran{z^2}\dotp{\vecl{a}}{\vec{2^n}} +
	\ran{z}\dotp{\vecl{a}}{\vecran{y^n}} -
	\ran{z}\dotp{\vec{1}}{\vecr{a}\circ\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n}} +
	\dotp{\vecl{a}}{\ran{z}\vecran{y^n}} +
	\dotp{-\ran{z}\vec{1}}{\vecr{a}\circ\vecran{y^n}} +
	\dotp{\vecl{a}}{\vecr{a}\circ \vecran{y^n}} \\
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} 
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} +
	\dotp{-\ran{z}\vec{1}}{\vecr{a}\circ\vecran{y^n}}
}

For the final step towards combining all the conditions into a single inner product we add $\dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n}}$ to both sides and simplify again:

\eq{
	&\ran{z^2}v + \ran{z}\dotp{\vec{1}}{\vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n}}
	&&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} +
	\dotp{-z\vec{1}}{\vecr{a}\circ\vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n}} \\ 
	&\ran{z^2}v + (\ran{z} - \ran{z^2})\dotp{\vec{1}}{\vecran{y^n}} - \ran{z^3}\dotp{\vec{1}}{\vec{2^n}} &&= \dotp{\vecl{a}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}} + \dotp{-\ran{z}\vec{1}}{\ran{z^2}\vec{2^n}+\ran{z}\vecran{y^n} + \vecr{a}\circ\vecran{y^n}}
	\\
	&\ran{z^2}v + (\ran{z} - \ran{z^2})\dotp{\vec{1}}{\vecran{y^n}} - \ran{z^3}\dotp{\vec{1}}{\vec{2^n}} &&= \dotp{\vecl{a}- \ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + \ran{z}\vecran{y^n} + \vecr{a}\circ \vecran{y^n}}
}

We define a function $\delta(y,z) = (z - z^2)\dotp{\vec{1}}{\vec{y^n}} - z^3\dotp{\vec{1}}{\vec{2^n}}$ and finish simplifying:

\eq{
	\ran{z^2}v + \delta(\ran{y},\ran{z}) = \dotp{\vecl{a}- \ran{z}\vec{1}}{\ran{z^2}\vec{2^n} + (\vecr{a} + \ran{z}\vec{1})\circ\vecran{y^n}}
}

From here on we will refer to the first argument of the inner product as $\vec{l_0}$ and the second argument as $\vec{r_0}$:

\eq{
	\ran{z^2}v + \delta(\ran{y},\ran{z}) = \dotp{\vec{l_0}}{\vec{r_0}}
}

If the goal was simply to construct a single inner product which would prove that $v$ lies in $[0,2^n)$, then $\P$ could simply send these values to $\V$ and be done. However, we wish for the proof to be zero-knowledge and therefore this inner product needs to be blinded. To do this, $\P$ constructs a polynomial $t(x)$. (rewrite to be more clear what the goal is!) This polynomial will be constructed in a way that allows us to change our objective to proving something about $t(x)$ instead of this inner product proof.

First $\P$ chooses random blinding factors $\vecran{s_L}$ and $\vecran{s_R}$ such that $\vecl{s},\vecr{s}\in \Z^n_p$. Using these $\P$ can costruct two vector-function $\vec{l}(x)$ and $\vec{r}(x)$ in the following way:
(Hvad er l1 og r1)
\eq{
	\vec{l}(x) &= \vec{l_0} + \vec{l_1}x &&= (\vecl{a} + \vecl{s}x) - z\vec{1}\\
	\vec{r}(x) &= \vec{r_0} + \vec{r_1}x &&= \ran{z^2}\vec{2^n} + ((\vecr{a} + \vecr{s}x) + \ran{z}\vec{1})\circ\vecran{y^n}
}
(Omskriv dette til at passe med tidligere omskrivinger)
Here $\vec{l_0}$ and $\vec{r_0}$ represent the zero-degree polynomial meaning that they are simply the unblinded versions of $\vec{l}(x)$ and $\vec{r}(x)$. This means that:

\eq{
	\dotp{\vec{l_0}}{\vec{r_0}} = \ran{z^2}v + \delta(\ran{y},\ran{z})
}

We will make use of this property later. Now $\P$ can construct $t(x)$ as follows:

\eq{
	t(x) = \dotp{\vec{l}(x)}{\vec{r}(x)} = t_0 + t_1\ran{x} + t_2\ran{x^2}
}

One way to construct this is using what's known as the Karatsuba method:

\eq{
	t_0 &= \dotp{\vec{l_0}}{\vec{r_0}}\\
	t_2 &= \dotp{\vec{l_1}}{\vec{r_1}}\\
	t_1 &= \dotp{\vec{l_0}+\vec{l_1}}{\vec{r_0} + \vec{r_1}} - t_0 - t_2
}

Now $\P$ wishes to prove to $\V$ that $t_0$ and $t(x)$ are correctly constructed. So, $t_0 = \ran{z^2}v + \delta(\ran{y},\ran{z})$, and $t(x) = \dotp{\vec{l}(x)}{\vec{r}(x)}$ such that, $\vec{l}(x) = \vec{l_0} + \vec{l_1}x$, and, $\vec{r}(x) = \vec{l_0} + \vec{l_1}x$.

First $\P$ wants to show that $t_0$ is correctly constructed. To do
this $\P$ starts by making commitments to each of the coefficients
of $t(x)$. Here we make note of the fact that $\P$ has already
commited $t_0$. Namely, the commitment to $v$, $V$, made at the
very start, is also a commitment to $t_0$ as per the construction
of $t_0$. Additionally $\P$ makes commitments to $T_1 = C(t_1,
\tt_1)$ and $T_2 = C(t_2, \tt_2)$. After commiting
these values to $\V$, $\V$ sends back a challenge scalar $\ran{x}$. Now
with the Pedersen-commitment having base-point $B$ and blinding point
$\tB$ we know from the definitions of $V, T_1, T_2$ that these commitments
relate in the following manner:

\eq{
	t(\ran{x})B                   &= \ran{z^2}vB      &&+ \delta(\ran{y},\ran{z})B &&+ \ran{x}t_1B       &&+ \ran{x^2}t_2B \\
	\tt(\ran{x})\tB               &= \ran{z^2}\tv \tB &&+ 0\tB                     &&+ \ran{x} \tt_1 \tB &&+ \ran{x^2} \tt_2 \tB\\
	t(\ran{x})B + \tt(\ran{x})\tB &= \ran{z^2}V       &&+ \delta(\ran{y},\ran{z})B &&+ \ran{x}T_1        &&+ \ran{x^2}T_2
}

The sum of the first two coefficients in each 'column' is also equal to the last coefficient in their column. To convince $\V$, $\P$ sends $t(x)B$ and $\tt(x)\tB$, evaluated at $\ran{x}$ to $\V$. This is enough to convince $\V$ that $t(x) = \ran{z^2}v + \delta(\ran{y},\ran{z}) + t_1x + t_2\ran{x^2} \Rightarrow t_0 = \ran{z^2}v + \delta(\ran{y},\ran{z})$, which is seen in The Verifier Algorithm further down.

Next $\P$ wishes to convince $\V$ that $\vec{l}(x)$ and $\vec{r}(x)$ are both constructed correctly and that $\vec{t}(x) = \dotp{\vec{l}(x)}{\vec{r}(x)}$. To achieve this $\P$ will use a similar construction as seen above. We want $\P$ to make commitments that allows $\V$ to relate $\vec{l}(x)$ and $\vec{r}(x)$ to $\vecl{a}, \vecl{s}, \vecr{a}$ and $\vecr{s}$. It is not that simple however. As seen here:

\eq{
	\vec{r}(x) = \ran{z^2}\vec{2^n} + ((\vecr{a} + \vecr{s}\ran{x}) + \ran{z}\vec{1})\circ\vecran{y^n}
}

This means we want $\P$ to make commitments to $\vec{y^n}\circ \vecr{a}$ and $\vec{y^n}\circ\vecr{s}$. This is a problem as these commitments need to be made and sent to $\V$ BEFORE $\P$ gets the challenge $\ran{y}$ from $\V$. To fix this issue $\P$ will make a special commitment to $\vecl{a}$ and $\vecr{a}$ as follows:

\eq{
	C(\vecl{a}, \vecr{a}, \widetilde{a}) = \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecl{a}}{\vec{H}} + \widetilde{a}\tB
}

Where G and H are vectors of canonical generator points and hiding points respectively, $\widetilde{a}$ is a randomly chosen blinding scalar and $\tB$ is the same blinding point used in the previous step. This same construction is used for the commitment to $C(\vecl{s}, \vecr{s}, \widetilde{s})$. Here we notice that: 

\eq{
	C(\vecl{a}, \vecr{a}, \widetilde{a}) &= \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecr{a}}{\vec{H}} + \widetilde{a}\tB \\
	&= \dotp{\vecl{a}}{\vec{G}} + \dotp{\vecran{y^n}\circ \vecr{a}}{\vecran{y^{-n}}\circ \vec{H}} + \widetilde{a}\tB
}

We define $\vec{H'} = \vecran{y^{-n}}\circ\vec{H}$, $A = C(\vecl{a},\vecr{a}, \widetilde{a})$ and $S = C(\vecl{s}, \vecr{s}, \widetilde{s})$ and can now construct our argument similarly to what was done in the previous segment:

\eq{
	\dotp{\vec{l}(x)}{\vec{G}} &= \dotp{\vecl{a}}{\vec{G}} &&+ \ran{x}\dotp{\vecl{s}}{\vec{G}} &&+ \dotp{-\ran{z}\vec{1}}{\vec{G}} \\
	\dotp{\vec{r}(x)}{\vec{H'}} &= \dotp{\vecr{a}}{\vec{H}} &&+ \ran{x}\dotp{\vecr{s}}{\vec{H}} &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}}\\
	\widetilde{e}\tB &= \widetilde{a}\tB &&+ \ran{x}\widetilde{s}\tB &&+ 0 \\
	\dotp{\vec{l}(x)}{\vec{G}} + \dotp{\vec{r}(x)}{\vec{H'}} + \widetilde{e}\tB &= A &&+ \ran{x}S &&+ (\dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}} - \ran{z}\dotp{\vec{1}}{\vec{G}})
}

From here, using the same argument as above $\P$ sends $\widetilde{e}$ to $\V$. This will be enough to convince $\V$ that $\vec{l}(x)$ and $\vec{r}(x)$ are correctly constructed and that $t(x) = \dotp{\vec{l}(x)}{\vec{r}(x)}$ and together with the previous commitments to $\V$, $\P$ will be able to successfully convince $\V$ that $v$ lies in the range $[0, 2^n)$.

In summary the \textit{transcript} between $\V$ and $\P$, will end up being the following:

\eq{
	\P \rightarrow \V &: V, n, A, S \\
	\V \rightarrow \P &: y, z \\
	\P \rightarrow \V &: T_0, T_1 \\
	\V \rightarrow \P &: x \\
	\P \rightarrow \V &: t(x)B, \tt(x)\tB \hspace*{1cm}\text{(both evaluated at x)}\\
	\P \rightarrow \V &: \widetilde{e}
}

Just as with the inner product the Fiat-Shamir Heuristic (\ref{fiat-shamir-heuristic}) can be used to make this process \textit{non-interactive}.

\paragraph{Verifier's Algorithm:}

$\V$ has access to the following values:

\eqn{def1-ver}{
	y, z, x, \widetilde{e} &\in \Z_p \\
	\vec{L}, \vec{R} &\in \G_p^{k} \\
	\vec{G}, \vec{H}, \vec{H'} &\in \G^n \\
	V, A, B, T_0, T_1, B, \tB &\in \G \\
	t(x)B, \widetilde{t}(x)\tB &\in \G \hspace*{1cm} \text{(evaluated at x)}
}

$\V$ needs to make a check to see 

who can then check if the following equation holds:

\eq{
	t(x)B + \tt(x)\tB \stackrel{?}{=} \ran{z^2} + \delta(\ran{y},\ran{z})B + \ran{x}T_1 + \ran{x^2}T_2
}

And if it does then that will convince the verifier that $t(x) = \ran{z^2}v + \delta(\ran{y},\ran{z}) + t_1x + t_2\ran{x^2}$ due to the column-sums seen above.

\eq{
	P &= -\widetilde{e}\tB &&+ A &&+ xS &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}}{\vec{H'}} &&- \ran{z}\dotp{\vec{1}}{\vec{\vec{G}}} \\
	&= -\widetilde{e}\tB &&+ A &&+ xS &&+ \dotp{\ran{z}\vecran{y^n} + \ran{z^2}\vec{2^n}\circ\vecran{y^{-n}}}{\vec{H}} &&- \ran{z}\dotp{\vec{1}}{\vec{G}}
}

from the definition of the column-sum argument we can deduce that $P = \dotp{\vec{l}(x)}{\vec{G}} + \dotp{\vec{r}(x)}{\vec{H'}}$ assuming an honest prover. Thus the verifier can use $t(x)$ and $P$ as inputs into the innerproduct protocol to prove that $ t(x) = \dotp{\vec{l}(x)}{\vec{r}(x)}$. And if this proof is verified then the prover has done their range-proof and successfully convinced the verifier that $v\in [0,2^n)$ without revealing any additional information about $v$, thus it has done so in zero-knowledge.

\subsubsection{Aggregation of rangeproofs:}

In the most basic terms a Bulletproof is a zero-knowledge aggregated range-proof which proves that one or most likely more values lie within a certain range: $[0,2^n)$ for some integer n. This is done by aggregating range-proofs for each individual value into a single proof which, if verified proves that each value given lies within the range while giving away no more information about any of the values. This is done by using a multi-party protocol where a 'party' is created for each value that needs to be proven and each party will converse with the so-called 'dealer' who is responsible for collecting the various commitments from the parties and generating challenges which is used by all parties. At the very end the dealer will then collect all the individual rangeproofs for each value and aggregate them. 

The steps used to do this are the same steps used in the construction of a rangeproof with a few additional steps which aggregate the proofs into a single rangeproof. 

To start with a number of parties are created, one for each value that $\P$ wishes to prove. We call this number m. These parties each create the needed values that $\P$ needs to send to $\V$ in order to convince them that each of these values lie within the range, with one key difference. Namely that the so-called dealer is in charge of generating the needed challenges $\ran{y}, \ran{z}$ and $\ran{x}$ and uses an offset for each party to ensure the order matters. This offset is defined as such for the jth party:

\eq{
	z_{(j)} &= z^j * \vec{y^n_{(j)}}\\
	y^n_{(j)} &= y^{n*m}[j*n:(j+1)*n]
}

the evaluation point $\ran{x}$ is the same for all parties. The choice of $\ran{y}, \ran{z}$ and $\ran{x}$ is done where it would ordinarily be done, but only once $\V$ has received the needed values from ALL parties. Once again, this can also be done using the Fiat-Shamir Heuristic. (\ref{fiat-shamir-heuristic}) 

After each party has created $t(x)_{(j)} = \dotp{\vec{l_{(j)}(x)}}{\vec{r_{(j)}(x)}}$ it is sufficient to perform the proving steps from \ref{range-proofs} for each party. However sending all of the created values to $\V$ will require $\V$ to make $m$ checks to see if all of these values prove that each v lies within the range. However $\P$ can do a few more computations to create a singular proof which requires a single check from $\V$ to see that ALL $v$ lie within the range.

To prove that a single $t_{(j)}$ is correct means proving that $\vec{l_{(j)}(x)}$ and $\vec{r_{(j)}(x)}$ are created correctly and that $t_{(j), 0} = \dotp{\vec{l_{(j)}(x)}}{r_{(j)}(x)}$. But $\P$ can combine some of these statements into one single statement as follows: 

\eq{
	t(x) &= \sum^{m-1}_{j = 0} t_{j}(x)\\
	\widetilde{t}(x) &= \sum^{m-1}_{j = 0} \widetilde{t}_{j}(x)
}

Now $\P$ sends $t(x)$ and $\widetilde{t}(x)$ alongside all the values each party would send individually to $\V$ as before and $\V$ can convince themselves that this is correct by computing:

\eq{
	T_1 &= \sum^{m-1}_{j = 0} T_{1,(j)}\\
	T_2 &= \sum^{m-1}_{j = 0} T_{2,(j)}\\
	\delta(y,z) &= \sum^{m-1}_{j = 0} \delta_{(j)}(y,z)
}

and from here the following check is used by $\V$ to convince them that $t_{0, (j)} = \dotp{\vec{l_{(j)}(x)}}{\vec{r_{(j)}(x)}}$ for all j values:

\eq{
	t(x)B + \widetilde{t}(x)\widetilde{B} \stackrel{?}{=} \ran{z^2}\sum^{m-1}_{j = 0} \ran{z_{(j)}}*V_{(j)} + \delta(y,z)B + \ran{x}T_1 + \ran{x^2}T_2
}

we know that $z_{(j)} = z^j$ we can make a small rewrite:

\eq{
	t(x)B + \widetilde{t}(x)\widetilde{B} \stackrel{?}{=} \sum^{m-1}_{j = 0} \ran{z^{j+2}}*V_{(j)} + \delta(y,z)B + \ran{x}T_1 + \ran{x^2}T_2
}

And this check will then convince the verifier that all $t_{0,(j)}$ are correct.

And once again we wish to show that each $\vec{l_{(j)}(x)}$ and $\vec{r_{(j)}(x)}$ are constructed correctly by all parties. This is done in a similar manner as proving $t_{(j)}(x)$ are all correct. Proving this property the jth party would be proving the following:

\eq{
	\dotp{l_{(j)}(x)}{\vec{G_{(j)}}} + \dotp{r_{(j)}(x)}{\vec{H'_{(j)}}} = -\widetilde{e}_{(j)}\widetilde{B} + A_{(j)} + \ran{x}S_{(j)} &&+ (\dotp{\ran{z}\vecran{y^n_{(j)}} + \ran{z^2}\ran{z_{(j)}}\vec{2^n}}{H'_{(j)}} - \ran{z}\dotp{\vec{1}}{\vec{G_{(j)}}})
}

$\P$ can then combine the variables in the following way:

\eq{
	\vec{l}(x) &= \vec{l_0}(x) &&|| \vec{l_1}(x) &&|| \dots &&|| \vec{l_{m-1}}(x)\\
	\vec{r}(x) &= \vec{r_0}(x) &&|| \vec{r_1}(x) &&|| \dots &&|| \vec{r_{m-1}}(x)\\
	\vec{G} &= \vec{G_0} &&|| \vec{G_1} &&|| \dots &&|| \vec{G_{m-1}}\\
	\vec{H'} &= \vec{H'_0} &&|| \vec{H'_1} &&|| \dots &&|| \vec{H'_{m-1}}
}

And send $\dotp{\vec{l}(x)}{\vec{G}}$ and $\dotp{\vec{r}(x)}{\vec{H'}}$ to $\V$, along with all the values from the parties it needs, as per \ref*{range-proofs}. From here $\V$ can then construct:

\eq{
	\vecran{y^n_{(j)}} &= y^{n*m}[j*n : (j+1)*n]\\
	\ran{z_{(j)}} &= z^j\\
	\widetilde{e} &= \sum^{m-1}_{j = 0} \widetilde{e}_{(j)}\\
	A &= \sum^{m-1}_{j = 0} A_{(j)}\\
	S &= \sum^{m-1}_{j = 0} S_{(j)}
}

and then use the following check to convince itself that all $\vec{l_{(j)}}(x)$ and $\vec{r_{(j)}}(x)$ are correctly constructed:

\eq{
	\vec{l_{(j)}}(x) + \vec{r_{(j)}}(x) \stackrel{?}{=} -\widetilde{e}\widetilde{B} + A + \ran{x}S - \ran{z}\dotp{\vec{1}}{\vec{G}} + \ran{z}\dotp{\vecran{y^{n*m}}}{\vec{H'}} + \sum^{m-1}_{j = 0}\dotp{\ran{z^{j+2}}*\vec{2^n}}{\vec{H'}[j*n: (j+1)*n]}
}

And if this holds then $\V$ is convinced that $\vec{l_{(j)}}(x)$ and $\vec{r_{(j)}}(x)$ are correctly constructed and with the previous part on top it is now convinced that all the given v are within the range $[0,2^n)$.


%\section{State of the art:}

\subsubsection{Fiat-Shamir Heuristic}\label{fiat-shamir-heuristic}
% Include the recently found vulnerability

\subsection{Hacspec:} \label{Hacspec}

Hacspec is a subset of the programming language of Rust, designed in a
way that makes it easy to compile into theorem solvers such as Coq or F*.
The cost for having this property is a reduction in certain conventions
available in Rust not being present in Hacspec. This is a double edged
sword, both making the language simple, but also making it harder to
express certain abstract ideas. The language however is still in
development, balancing and adding features to the language.

All code written for this project was written to be Hacspec
compliant.  Our implementation is a specification meant to
be simply understood compared to a more obfuscated, but highly
optimized implementation. We will use property based testing with
QuickCheck\footnote{\url{https://github.com/BurntSushi/quickcheck}}
to check our Bulletproofs implementation against the Dalek-Cryptography
Bulletproofs\footnote{\url{https://github.com/dalek-cryptography/bulletproofs}}.
We have also created a minimal linear algebra library, in hacspec,
since it was needed for the bulletproofs implementation. Property
based testing using QuickCheck is also used for this library, testing
it against the nalgebra\footnote{\url{https://nalgebra.org/}}.

At a later time our implementation could to be compiled to Coq or F* and
proof-checked, leading to better guarantees about our implementation.
This is however not in the scope of this paper and is left to future
work. % mere her

\section{Contributions:}

\subsection{Linear Algebra Library Specification:}
\begin{itemize}
	\item Vectors vs Matrices 
	\item Describe formulas, with sources (Dot product) 
	\item Generics :(
	\item Cloning
	\item Double indexing
	\item QuickCheck
\end{itemize} 

It was decided that the specification should of course consist of what we
need, but also of some general linear algebra functions that could be
used for others who might want to use it. The following functions was
decided to be part of the specification:

\begin{itemize}
	\item Instantiate matrix
	\item Instantiate zero filled matrix
	\item (Instantiate one filled matrix)
	\item Instantiate identity matrix
	\item Transposition
	\item Slicing
	\item (Scalar Multiplication)
	\item Addition
	\item Subtraction
	\item Hadamard product
	\item Matrix Multiplication
\end{itemize}

No generalized standard for Linear Algebra Specification.

\subsection{Implementing Ristretto:} \label{implementing-ristretto}

For our Ristretto implementation on 25519 there
exists an IETF-standard specification of exactly
this, which was used as a guide for our implementation.
\footnote{\url{https://www.ietf.org/archive/id/draft-irtf-cfrg-ristretto255-00.html}}
This specification was very helpful and after inspecting the code we
are testing against closely, it is clear that they used this standard
as well.

The most important things to note about this standard, outside the
explicit formulas used for its various methods are which functions and
internal representations we are allowed to expose to the outside. Namely
that the only functions we may expose were encoding of points, decoding
of encoded points, the map which is used for creating points, point
addition, point negation, point subtraction, scalar multiplication. The
last two were allowed purely from being derivable from repeated
application of point addition and/or negation. Some things that were not
allowed to be exposed under any circumstances, are the internal
representations of either points or its field elements or any functions
that are not the ones mentioned above, which is as expected for something
that is meant to be a thin layer of abstraction beneath an implementation
of 25519.

Each internal point representation is composed of four field elements
$(X : Y : Z :T)$. Field-elements, as defined by the standard are values
modulo p, with p being the prime field for 25519, $2^{255} - 19$. This
was achieved using the \texttt{public\_nat\_mod!} macro which defines
fields over certain values which just so happen to accept hex values as
modulo values, which was very convenient. With this we are now able to
create field elements using various functions and properties that are
native to the type created by \texttt{public\_nat\_mod!}. Most of the
standard integer operators like addition, subtraction and multiplication
are implemented for these field elements for example. An important thing
to note is that the internal calculations for division is done using
integer arithmetic rather than finite field arithmetic, however the
few times where division is used directly it is ensured that the result
in finite field arithmetic is equal to the integer arithmetic solution.

Additionally the standard, specified a series of constants. These
constants were too large to be implemented as integers. Additionally
hacspec did not allow for us to simply use the \texttt{from\_hex()}
method. As such, after converting each of these constants into their
corresponding hex-values, we built them as byte sequences for which
we had an equivalent \texttt{from\_byte\_seq\_be} which created the
correct field elements we wanted.

While it is impossible for a legal point to be encoded and then have the
decoding on its encoding fail, the decoding method has several checks
that ensure that the input given is legal. The primary reason for this
is that decoding is a method available to the client and as such there is
no guarantee that the byte-strings they want to decode are necessarily
legal points that they first encoded properly, but could be artificially
constructed byte-strings. We want to avoid this and thus the standard has
a list of properties the byte-string input must satisfy for the decoding
to be canonical and thus give a proper point as a result. 

\subsection{Implementing Bulletproofs:}

Our bulletproofs implementation was implemented on the foundation of our
implementations of Linear Algebra and Ristretto. The first and simplest
step was implementing Pedersen commitments to single values as well as to
vectors. From our implementation of Ristretto we can easily add points to
each other and multiply points by a scalar. This is essentially the only
thing a Pedersen commitment is. Additionally we needed it to support
vector commitments. From our implementation of Linear Algebra we could
easily adapt this into the code. This implementation was nearly trivial
given the base we built it upon.

The next step in the process towards implementing Bulletproofs is the
inner product proof. The implementation of which does not lean itself
against our Ristretto implementation as our implementation of Pedersen
commitments did, however it does utilize said Pedersen commitments. 


The final part was implementing the bulletproof methods themselves. 

\subsection{Analysis of coding stuff:}

\subsection{Summary of coding stuff:}

\subsection{Conclusions on our work:}

\section{Future work:}

\section{Acknowledgements:}

%% TAK TIL FRANZISKUS
%% Lasse?

\section{References}
\printbibliography

\section{Appendix A: Notation:} \label{notation}

A table of notation used throughout this report:

\begin{center}
\begin{tabular}{ c l }
	$a$                         & A scalar \\
	$\vec{a}$                   & A vector \\
	$A$                         & A curve point \\
	$\vec{A}$                   & A vector of curve points \\
	$\ran{a}$                   & A scalar random variable \\
	$\vecran{a}$                & A vector of scalar random variables \\
	$\vecran{A}$                & A vector of random curve points \\
	$\veclo{a}$                 & The first half of vector $\vec{a}$, ($\veclo{a} = [a_{1}, \cdots, a_{n/2}]$) \\
	$\vechi{a}$                 & The second half of vector $\vec{a}$, ($\vechi{a} = [a_{n/2+1}, \cdots, a_{n}]$) \\
	$\vec{a^n}$                 & A vector of scalars which are made up of powers of $a$ i.e. $[1,a,a^2... a^{n-1}]$\\
	$\vec{a} \concat \vec{b}$   & A vector, $\vec{a}$, concatinated with another vector, $\vec{b}$\\
	$\mathbb{A}$                & A set \\
	$\mathbb{A}^n$              & A vector space of dimension $n$ \\ 
	$\mathbb{A}_n$              & A set whose elements are mod $n$ \\ 
	$a(x)$                      & A function mapping integers mod p to integers mod p: $\Z_p \rightarrow \Z_p$ \\
	$\vec{a}(x)$                & A function mapping integers mod p to vectors containing integers mod p: $\Z_p \rightarrow \Z^n_p$ \\
	$\dotp{\vec{a}}{\vec{b}}$   & Dot product of $\vec{a}$ and $\vec{b}$ \\
	$\dotp{\vec{a}}{\vec{A}}$   & The sum of scalar-point products of $\vec{a}$ and $\vec{A}$ ($\dotp{\vec{a}}{\vec{A}} = a_1 A_1 + a_2 A_2 + \cdots + a_n A_n$) \\
\end{tabular}
\end{center}

\section{Appendix B: Ristretto:} \label{ristretto}
Ristretto is a specification of elliptic curve cryptography, created for
the specific purpose of eliminating unwanted co-factors. This was done
in order to circumvent the double-spending vulnerability. This is done
using a so-called quotient group. Also appropriately called a Factor
Group, a Quotient Group is a type of group, which takes elements from a
larger group and, using an equivalence relation, maps elements that are
'similar' to the same element in the quotient group, while preserving
most of the group structure. The remaining elements are factored out,
leaving a group with the same operations, but fewer elements. What
this means for Ristretto is that it takes points on an elliptic curve
and eliminates the co-factor by simply mapping them down to their
'equivalent' elements. This is the reason for Ristretto's somewhat
unorthodox method of doing operations, as it must ensure that each
element is computed to the proper element in the quotient group. This is
also done, in part, to allow the user to use points that are NOT in the
quotient group without worry as any computation done with these points
is automatically mapped to their proper element, thus eliminating the
rather computationally expensive operation of checking if a point is
on the proper curve.

Additionally the internal representation of the Montgomery curve we wish
to eliminate the co-factor for, is its equivalent Twisted Edwards Curve,
which is done to more easily factor points from the original curve into
the quotient group. Additionally Ristretto's equality method ensures
that equivalent representations of points on the curve are equal, even
if they are not factored into the quotient group yet. Additionally
from its encoding and decoding methods, equivalent points are encoded
to the same bitstring, and are therefore decoded into their refactored
quotient group element.

The exposed functions that were implemented are the encoding and decoding
functions, the equality function to compare points, the addition function
over points, point negation, and the derived functions from these,
namely point doubling, point subtraction and scalar multiplication. And
the final function is the One Way Map function. This function is not
entirely necessary for our purposes, but having it allows outside users
to more easily generate points from simple byte sequences, and 
also makes generating random points for testing much easier.

\end{document}
